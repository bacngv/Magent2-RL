{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GET CODE\n",
        "Our code is in the repo: https://github.com/bacngv/Magent2-RL\n",
        "and while writting the code I set the repo is private, and push zipped code to google drive."
      ],
      "metadata": {
        "id": "AmwuMIL-epD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U5RUgr9VfRrx"
      },
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uemWROPlfWUx",
        "outputId": "22ca4456-bc8a-404f-9d78-4eeb309032c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1BBRMGzwGO3WCNuNQlhlzjr6ncHFHbAFF\n",
            "From (redirected): https://drive.google.com/uc?id=1BBRMGzwGO3WCNuNQlhlzjr6ncHFHbAFF&confirm=t&uuid=f1d72ff1-5a89-4727-868c-625cbbf3f4cc\n",
            "To: /content/Magent2-RL-main.zip\n",
            "100% 43.9M/43.9M [00:01<00:00, 31.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1BBRMGzwGO3WCNuNQlhlzjr6ncHFHbAFF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9guwWTbfbEb",
        "outputId": "7c589706-76e7-4cd0-cdfa-26ec17086fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Magent2-RL-main.zip\n",
            "fb0f5a827dba0e6e47f3fe4f89159d911c2e53b6\n",
            "   creating: Magent2-RL-main/\n",
            " extracting: Magent2-RL-main/.gitignore  \n",
            "  inflating: Magent2-RL-main/README.md  \n",
            "   creating: Magent2-RL-main/algo/\n",
            "  inflating: Magent2-RL-main/algo/__init__.py  \n",
            "  inflating: Magent2-RL-main/algo/ac.py  \n",
            "  inflating: Magent2-RL-main/algo/base.py  \n",
            "  inflating: Magent2-RL-main/algo/q_learning.py  \n",
            "  inflating: Magent2-RL-main/algo/tools.py  \n",
            "   creating: Magent2-RL-main/assets/\n",
            "  inflating: Magent2-RL-main/assets/pretrained.gif  \n",
            "  inflating: Magent2-RL-main/assets/random.gif  \n",
            "  inflating: Magent2-RL-main/battle_vs_dqn.py  \n",
            "  inflating: Magent2-RL-main/battle_vs_final.py  \n",
            "   creating: Magent2-RL-main/data/\n",
            "  inflating: Magent2-RL-main/data/battle_vs_dqn.gif  \n",
            "  inflating: Magent2-RL-main/data/battle_vs_final.gif  \n",
            "   creating: Magent2-RL-main/data/models/\n",
            "   creating: Magent2-RL-main/data/models/ac-0/\n",
            "  inflating: Magent2-RL-main/data/models/ac-0/ac_42  \n",
            "  inflating: Magent2-RL-main/data/models/ac-0/ac_50  \n",
            "   creating: Magent2-RL-main/data/models/ac-1/\n",
            "  inflating: Magent2-RL-main/data/models/ac-1/ac_50  \n",
            "   creating: Magent2-RL-main/data/models/iql-0/\n",
            "  inflating: Magent2-RL-main/data/models/iql-0/dqn_eval_1999  \n",
            "  inflating: Magent2-RL-main/data/models/iql-0/dqn_eval_340  \n",
            "  inflating: Magent2-RL-main/data/models/iql-0/dqn_eval_460  \n",
            "  inflating: Magent2-RL-main/data/models/iql-0/dqn_target_1999  \n",
            "  inflating: Magent2-RL-main/data/models/iql-0/dqn_target_340  \n",
            "  inflating: Magent2-RL-main/data/models/iql-0/dqn_target_460  \n",
            "   creating: Magent2-RL-main/data/models/mfq-0/\n",
            "  inflating: Magent2-RL-main/data/models/mfq-0/mfq_eval_1863   \n",
            "  inflating: Magent2-RL-main/data/models/mfq-0/mfq_target_1863  \n",
            "   creating: Magent2-RL-main/data/render/\n",
            "   creating: Magent2-RL-main/data/render/ac/\n",
            "  inflating: Magent2-RL-main/data/render/ac/replay_100.gif  \n",
            "  inflating: Magent2-RL-main/eval_dqn.py  \n",
            "  inflating: Magent2-RL-main/eval_final.py  \n",
            "  inflating: Magent2-RL-main/final_torch_model.py  \n",
            "  inflating: Magent2-RL-main/main.py  \n",
            "  inflating: Magent2-RL-main/pipeline.ipynb  \n",
            "  inflating: Magent2-RL-main/red.pt  \n",
            "  inflating: Magent2-RL-main/red_final.pt  \n",
            "  inflating: Magent2-RL-main/replay.gif  \n",
            "  inflating: Magent2-RL-main/requirements.txt  \n",
            "   creating: Magent2-RL-main/senarios/\n",
            "  inflating: Magent2-RL-main/senarios/senario_battle.py  \n",
            "  inflating: Magent2-RL-main/torch_model.py  \n",
            "  inflating: Magent2-RL-main/train_battle.py  \n",
            "   creating: Magent2-RL-main/video/\n",
            "  inflating: Magent2-RL-main/video/pretrained.mp4  \n",
            "  inflating: Magent2-RL-main/video/random.mp4  \n"
          ]
        }
      ],
      "source": [
        "!unzip Magent2-RL-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJVQ8IPl2P2z",
        "outputId": "6283c54f-d496-4bf2-bfce-250984975704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Magent2-RL-main\n"
          ]
        }
      ],
      "source": [
        "%cd Magent2-RL-main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-r4MkXjfghe",
        "outputId": "42da7ac8-3f59-401c-cce6-1c805bbe7a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2 (from -r requirements.txt (line 3))\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-43ivrno9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-43ivrno9\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.20.1+cu121)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 1)) (2.36.1)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 1)) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->-r requirements.txt (line 1)) (0.1.10)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3->-r requirements.txt (line 3)) (2.6.1)\n",
            "Collecting pettingzoo>=1.23.1 (from magent2==0.3.3->-r requirements.txt (line 3))\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 6)) (11.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy->-r requirements.txt (line 1)) (75.1.0)\n",
            "Collecting gymnasium>=0.28.0 (from pettingzoo>=1.23.1->magent2==0.3.3->-r requirements.txt (line 3))\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3->-r requirements.txt (line 3)) (3.1.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3->-r requirements.txt (line 3))\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: magent2\n",
            "  Building wheel for magent2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696113 sha256=ebe363a07dbfdbab88b2ba61a04b1625df203555421d116531466543328da80f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1_6xdabl/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\n",
            "Successfully built magent2\n",
            "Installing collected packages: farama-notifications, gymnasium, pettingzoo, magent2\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 magent2-0.3.3 pettingzoo-1.24.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN\n",
        "args:\n",
        "1. --algo: choices={'ac', 'mfac', 'mfq', 'iql'} to choose an algorithm from the preset\n",
        "2. --save_every: #NUMBER decide the self-play update interval\n",
        "3. --update_every: #NUMBER decide the update interval for q-learning [optional]\n",
        "4. --n_round: #NUMBER set the training round\n",
        "5. --render: render or not (if true, will render every save)\n",
        "6. --map_size: #NUMBER (45) set the size of map  # then the amount of agents is 81\n",
        "7. --max_steps: #NUMBER set the max steps\n",
        "8. --cuda: #BOOL use the cuda"
      ],
      "metadata": {
        "id": "GfQn9H5meynD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ZWQjINfn3c",
        "outputId": "bd6f6c32-c664-4f52-faef-11a542949c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4919\n",
            "[*] LOSS: 0.5142221450805664 / Q: {'Eval-Q': 3.204571, 'Target-Q': 3.068615}\n",
            "[*] LOSS: 0.19390840828418732 / Q: {'Eval-Q': 3.456468, 'Target-Q': 3.394471}\n",
            "[*] LOSS: 0.27971959114074707 / Q: {'Eval-Q': 3.740062, 'Target-Q': 3.725944}\n",
            "[*] LOSS: 0.5818513035774231 / Q: {'Eval-Q': 3.928426, 'Target-Q': 3.779252}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1372146246149256, 'total_reward': 388.1699982183054, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1738, EPS: 0.17 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.005306, 0.08875 ]), 'NUM': [49, 48]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4829\n",
            "[*] LOSS: 0.42328453063964844 / Q: {'Eval-Q': 3.835486, 'Target-Q': 3.833198}\n",
            "[*] LOSS: 0.29141098260879517 / Q: {'Eval-Q': 3.87441, 'Target-Q': 3.636282}\n",
            "[*] LOSS: 0.13577400147914886 / Q: {'Eval-Q': 3.406384, 'Target-Q': 3.258004}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11777971547532508, 'total_reward': 383.56499820481986, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1739, EPS: 0.17 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.274342, 0.264821]), 'NUM': [38, 56]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 28]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 28]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4065\n",
            "[*] LOSS: 0.1787029206752777 / Q: {'Eval-Q': 3.733874, 'Target-Q': 3.702626}\n",
            "[*] LOSS: 0.1372193694114685 / Q: {'Eval-Q': 3.182307, 'Target-Q': 3.0418}\n",
            "[*] LOSS: 0.614663302898407 / Q: {'Eval-Q': 3.786942, 'Target-Q': 3.357722}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04629690362428849, 'total_reward': 239.77999898977578, 'kill': 53}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1740.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1740, EPS: 0.17 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.006216, 0.232177]), 'NUM': [37, 62]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3878\n",
            "[*] LOSS: 0.4557693898677826 / Q: {'Eval-Q': 4.281164, 'Target-Q': 4.089752}\n",
            "[*] LOSS: 0.3912620544433594 / Q: {'Eval-Q': 4.58058, 'Target-Q': 4.407027}\n",
            "[*] LOSS: 0.32607966661453247 / Q: {'Eval-Q': 3.699341, 'Target-Q': 3.784157}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08034985136431995, 'total_reward': 164.51499930303544, 'kill': 40}\n",
            "\n",
            "\n",
            "[*] ROUND #1741, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002717,  0.066154]), 'NUM': [46, 52]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.004444,  0.62    ]), 'NUM': [9, 8]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5148\n",
            "[*] LOSS: 0.6126410961151123 / Q: {'Eval-Q': 4.049281, 'Target-Q': 3.810408}\n",
            "[*] LOSS: 0.5100284814834595 / Q: {'Eval-Q': 4.495568, 'Target-Q': 4.182089}\n",
            "[*] LOSS: 0.5472202897071838 / Q: {'Eval-Q': 3.77907, 'Target-Q': 3.709637}\n",
            "[*] LOSS: 0.18604671955108643 / Q: {'Eval-Q': 4.274807, 'Target-Q': 4.204233}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11573177566116773, 'total_reward': 396.2449981290847, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1742, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.512241, -0.001019]), 'NUM': [29, 54]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3729\n",
            "[*] LOSS: 0.23908844590187073 / Q: {'Eval-Q': 2.454841, 'Target-Q': 2.478749}\n",
            "[*] LOSS: 0.36959806084632874 / Q: {'Eval-Q': 3.064723, 'Target-Q': 3.097745}\n",
            "[*] LOSS: 0.4690317213535309 / Q: {'Eval-Q': 3.804909, 'Target-Q': 3.633778}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11791341586772137, 'total_reward': 205.95999909099191, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #1743, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.37525 , 0.097447]), 'NUM': [40, 47]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4556\n",
            "[*] LOSS: 0.20213347673416138 / Q: {'Eval-Q': 3.636411, 'Target-Q': 3.314031}\n",
            "[*] LOSS: 0.3556637167930603 / Q: {'Eval-Q': 4.157166, 'Target-Q': 3.923026}\n",
            "[*] LOSS: 0.2604272961616516 / Q: {'Eval-Q': 3.91505, 'Target-Q': 3.91864}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1262488768158781, 'total_reward': 397.0099981902167, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1744, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.010385, -0.005   ]), 'NUM': [39, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4757\n",
            "[*] LOSS: 0.2956182658672333 / Q: {'Eval-Q': 3.436283, 'Target-Q': 3.351627}\n",
            "[*] LOSS: 0.33477672934532166 / Q: {'Eval-Q': 3.866957, 'Target-Q': 3.695748}\n",
            "[*] LOSS: 0.4096851050853729 / Q: {'Eval-Q': 3.982855, 'Target-Q': 4.031543}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11084731755852972, 'total_reward': 370.5499983401969, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1745, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.088182, 0.48898 ]), 'NUM': [55, 49]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5295\n",
            "[*] LOSS: 0.7616918087005615 / Q: {'Eval-Q': 3.848853, 'Target-Q': 3.833994}\n",
            "[*] LOSS: 0.6407613158226013 / Q: {'Eval-Q': 3.746943, 'Target-Q': 3.56251}\n",
            "[*] LOSS: 0.25992363691329956 / Q: {'Eval-Q': 3.432386, 'Target-Q': 3.256086}\n",
            "[*] LOSS: 0.5655033588409424 / Q: {'Eval-Q': 3.779526, 'Target-Q': 3.624736}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09178200518081805, 'total_reward': 379.67499818746, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1746, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.003421, 0.495   ]), 'NUM': [38, 39]}\n",
            "> step #100, info: {'Ave-Reward': array([0.011667, 0.028333]), 'NUM': [6, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5849\n",
            "[*] LOSS: 1.0472060441970825 / Q: {'Eval-Q': 4.26436, 'Target-Q': 3.931846}\n",
            "[*] LOSS: 0.23323865234851837 / Q: {'Eval-Q': 3.721686, 'Target-Q': 3.732352}\n",
            "[*] LOSS: 0.28278177976608276 / Q: {'Eval-Q': 3.28549, 'Target-Q': 3.116218}\n",
            "[*] LOSS: 0.6695459485054016 / Q: {'Eval-Q': 3.725316, 'Target-Q': 3.813743}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.035820333185263904, 'total_reward': 372.43499831855297, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1747, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.252381, 0.195213]), 'NUM': [42, 47]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.001667, -0.005   ]), 'NUM': [15, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5828\n",
            "[*] LOSS: 0.37253668904304504 / Q: {'Eval-Q': 3.649169, 'Target-Q': 3.56081}\n",
            "[*] LOSS: 0.2875988781452179 / Q: {'Eval-Q': 3.88282, 'Target-Q': 3.739961}\n",
            "[*] LOSS: 0.4877927899360657 / Q: {'Eval-Q': 3.740872, 'Target-Q': 3.684993}\n",
            "[*] LOSS: 0.42696595191955566 / Q: {'Eval-Q': 3.285622, 'Target-Q': 3.303139}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06579744733659892, 'total_reward': 367.28999826777726, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1748, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.222386, 0.095217]), 'NUM': [44, 46]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.028333, -0.025   ]), 'NUM': [3, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4743\n",
            "[*] LOSS: 0.6270345449447632 / Q: {'Eval-Q': 3.294022, 'Target-Q': 3.445593}\n",
            "[*] LOSS: 0.47312816977500916 / Q: {'Eval-Q': 3.693049, 'Target-Q': 3.610905}\n",
            "[*] LOSS: 0.2967641353607178 / Q: {'Eval-Q': 3.532394, 'Target-Q': 3.273477}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05287142440919206, 'total_reward': 354.28499838057905, 'kill': 78}\n",
            "\n",
            "\n",
            "[*] ROUND #1749, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.120375, 0.325349]), 'NUM': [40, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4069\n",
            "[*] LOSS: 0.8656915426254272 / Q: {'Eval-Q': 3.967388, 'Target-Q': 3.813456}\n",
            "[*] LOSS: 0.5245048403739929 / Q: {'Eval-Q': 3.724586, 'Target-Q': 3.450132}\n",
            "[*] LOSS: 0.5540602207183838 / Q: {'Eval-Q': 4.127296, 'Target-Q': 3.963579}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14246004688884378, 'total_reward': 323.95999847352505, 'kill': 68}\n",
            "\n",
            "\n",
            "[*] ROUND #1750, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.007818, 0.087453]), 'NUM': [55, 53]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [24, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5453\n",
            "[*] LOSS: 0.579390287399292 / Q: {'Eval-Q': 3.51515, 'Target-Q': 3.541296}\n",
            "[*] LOSS: 0.5155388116836548 / Q: {'Eval-Q': 3.480836, 'Target-Q': 3.452434}\n",
            "[*] LOSS: 0.3707926273345947 / Q: {'Eval-Q': 3.3597, 'Target-Q': 3.130921}\n",
            "[*] LOSS: 0.40950506925582886 / Q: {'Eval-Q': 3.893402, 'Target-Q': 3.960867}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08997349082976251, 'total_reward': 375.81999807525426, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1751, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.00375 , 0.286939]), 'NUM': [36, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3937\n",
            "[*] LOSS: 0.2869836091995239 / Q: {'Eval-Q': 3.859739, 'Target-Q': 3.377132}\n",
            "[*] LOSS: 0.23013757169246674 / Q: {'Eval-Q': 3.239626, 'Target-Q': 3.178018}\n",
            "[*] LOSS: 0.4372062683105469 / Q: {'Eval-Q': 3.215594, 'Target-Q': 3.075649}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.20459474326695934, 'total_reward': 237.7199989380315, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #1752, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.121889, 0.234146]), 'NUM': [45, 41]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4579\n",
            "[*] LOSS: 0.26196184754371643 / Q: {'Eval-Q': 3.676164, 'Target-Q': 3.589432}\n",
            "[*] LOSS: 0.16773094236850739 / Q: {'Eval-Q': 3.60148, 'Target-Q': 3.579813}\n",
            "[*] LOSS: 0.337057888507843 / Q: {'Eval-Q': 4.191392, 'Target-Q': 4.142242}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12653651892044157, 'total_reward': 371.0299980631098, 'kill': 78}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1753, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.005122, 0.293061]), 'NUM': [41, 49]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4088\n",
            "[*] LOSS: 0.23086731135845184 / Q: {'Eval-Q': 3.470348, 'Target-Q': 3.188876}\n",
            "[*] LOSS: 0.5749854445457458 / Q: {'Eval-Q': 3.346487, 'Target-Q': 3.340414}\n",
            "[*] LOSS: 0.25998765230178833 / Q: {'Eval-Q': 3.858187, 'Target-Q': 3.638302}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08773916957318591, 'total_reward': 228.9649989809841, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #1754, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.203039, 0.229512]), 'NUM': [51, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [32, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15075\n",
            "[*] LOSS: 0.2682875990867615 / Q: {'Eval-Q': 3.94385, 'Target-Q': 3.66663}\n",
            "[*] LOSS: 0.5676771998405457 / Q: {'Eval-Q': 3.706602, 'Target-Q': 3.672826}\n",
            "[*] LOSS: 0.398325651884079 / Q: {'Eval-Q': 3.275997, 'Target-Q': 3.280056}\n",
            "[*] LOSS: 0.466431587934494 / Q: {'Eval-Q': 3.46592, 'Target-Q': 3.507464}\n",
            "[*] LOSS: 0.08898802101612091 / Q: {'Eval-Q': 3.271264, 'Target-Q': 2.867357}\n",
            "[*] LOSS: 0.21358506381511688 / Q: {'Eval-Q': 2.705225, 'Target-Q': 2.547887}\n",
            "[*] LOSS: 0.24599696695804596 / Q: {'Eval-Q': 3.207141, 'Target-Q': 2.949981}\n",
            "[*] LOSS: 0.2774221897125244 / Q: {'Eval-Q': 3.01044, 'Target-Q': 3.022654}\n",
            "[*] LOSS: 0.5403801798820496 / Q: {'Eval-Q': 3.470737, 'Target-Q': 3.382845}\n",
            "[*] LOSS: 0.32340434193611145 / Q: {'Eval-Q': 4.46918, 'Target-Q': 4.192752}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014706104929544219, 'total_reward': 318.86999948415905, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1755, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.002069, 0.083679]), 'NUM': [29, 53]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3793\n",
            "[*] LOSS: 0.3105410933494568 / Q: {'Eval-Q': 2.862847, 'Target-Q': 2.839624}\n",
            "[*] LOSS: 0.2072531282901764 / Q: {'Eval-Q': 3.156938, 'Target-Q': 3.037376}\n",
            "[*] LOSS: 0.24574431777000427 / Q: {'Eval-Q': 3.122686, 'Target-Q': 2.933176}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04530722857800149, 'total_reward': 158.0399993499741, 'kill': 36}\n",
            "\n",
            "\n",
            "[*] ROUND #1756, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.122703, 0.442364]), 'NUM': [37, 55]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4035\n",
            "[*] LOSS: 0.4169542193412781 / Q: {'Eval-Q': 3.499772, 'Target-Q': 3.385778}\n",
            "[*] LOSS: 0.2198535054922104 / Q: {'Eval-Q': 3.648559, 'Target-Q': 3.715578}\n",
            "[*] LOSS: 0.2470889389514923 / Q: {'Eval-Q': 3.399528, 'Target-Q': 3.216777}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07014848091330743, 'total_reward': 176.42999940086156, 'kill': 38}\n",
            "\n",
            "\n",
            "[*] ROUND #1757, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.01375 , 0.009894]), 'NUM': [16, 47]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3372\n",
            "[*] LOSS: 0.8423382639884949 / Q: {'Eval-Q': 4.412722, 'Target-Q': 4.666883}\n",
            "[*] LOSS: 0.326665997505188 / Q: {'Eval-Q': 3.054582, 'Target-Q': 3.063334}\n",
            "[*] LOSS: 0.19666625559329987 / Q: {'Eval-Q': 2.934794, 'Target-Q': 2.891498}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07385430307484873, 'total_reward': 155.44499939773232, 'kill': 36}\n",
            "\n",
            "\n",
            "[*] ROUND #1758, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004242,  0.443148]), 'NUM': [33, 54]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3661\n",
            "[*] LOSS: 0.29535895586013794 / Q: {'Eval-Q': 2.915499, 'Target-Q': 2.903614}\n",
            "[*] LOSS: 0.18570886552333832 / Q: {'Eval-Q': 2.896, 'Target-Q': 2.838902}\n",
            "[*] LOSS: 0.3665074110031128 / Q: {'Eval-Q': 3.287478, 'Target-Q': 3.348992}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11311368537196985, 'total_reward': 180.69999926071614, 'kill': 39}\n",
            "\n",
            "\n",
            "[*] ROUND #1759, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.161667, 0.00163 ]), 'NUM': [33, 46]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3741\n",
            "[*] LOSS: 0.1260930597782135 / Q: {'Eval-Q': 2.670265, 'Target-Q': 2.603851}\n",
            "[*] LOSS: 0.15897808969020844 / Q: {'Eval-Q': 3.104818, 'Target-Q': 2.928716}\n",
            "[*] LOSS: 0.16148194670677185 / Q: {'Eval-Q': 3.616057, 'Target-Q': 3.638758}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09267533131988755, 'total_reward': 197.29999919887632, 'kill': 48}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1760.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1760, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.014516, 0.106628]), 'NUM': [31, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3777\n",
            "[*] LOSS: 0.2220992147922516 / Q: {'Eval-Q': 3.603334, 'Target-Q': 3.627514}\n",
            "[*] LOSS: 0.46523144841194153 / Q: {'Eval-Q': 4.200971, 'Target-Q': 4.062202}\n",
            "[*] LOSS: 0.1271989941596985 / Q: {'Eval-Q': 2.962344, 'Target-Q': 2.620118}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10485332588621069, 'total_reward': 245.21999897249043, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #1761, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.154844, 0.2931  ]), 'NUM': [32, 50]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3740\n",
            "[*] LOSS: 0.45306530594825745 / Q: {'Eval-Q': 3.707095, 'Target-Q': 3.5713}\n",
            "[*] LOSS: 0.532663106918335 / Q: {'Eval-Q': 3.84393, 'Target-Q': 3.738876}\n",
            "[*] LOSS: 0.2106570154428482 / Q: {'Eval-Q': 3.802611, 'Target-Q': 3.601482}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07847393904300683, 'total_reward': 184.90499919839203, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #1762, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.143919, 0.218372]), 'NUM': [37, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3968\n",
            "[*] LOSS: 0.19323207437992096 / Q: {'Eval-Q': 3.666644, 'Target-Q': 3.550707}\n",
            "[*] LOSS: 0.48052915930747986 / Q: {'Eval-Q': 4.656258, 'Target-Q': 4.26539}\n",
            "[*] LOSS: 0.3587292432785034 / Q: {'Eval-Q': 3.15858, 'Target-Q': 3.194533}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09162152851643657, 'total_reward': 240.56499896384776, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #1763, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 1.62647e-01, -1.19000e-04]), 'NUM': [34, 42]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3881\n",
            "[*] LOSS: 0.29445523023605347 / Q: {'Eval-Q': 3.541898, 'Target-Q': 3.350769}\n",
            "[*] LOSS: 0.23782876133918762 / Q: {'Eval-Q': 3.707266, 'Target-Q': 2.782952}\n",
            "[*] LOSS: 0.32556596398353577 / Q: {'Eval-Q': 3.777346, 'Target-Q': 3.791449}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12608950159860607, 'total_reward': 269.79999885801226, 'kill': 61}\n",
            "\n",
            "\n",
            "[*] ROUND #1764, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.007059, 0.195   ]), 'NUM': [34, 51]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3745\n",
            "[*] LOSS: 0.4323625862598419 / Q: {'Eval-Q': 3.796749, 'Target-Q': 3.662151}\n",
            "[*] LOSS: 0.8163898587226868 / Q: {'Eval-Q': 3.714902, 'Target-Q': 3.304026}\n",
            "[*] LOSS: 0.41843003034591675 / Q: {'Eval-Q': 3.842804, 'Target-Q': 3.84066}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07519974415326793, 'total_reward': 184.27999921236187, 'kill': 42}\n",
            "\n",
            "\n",
            "[*] ROUND #1765, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.16375, 0.4832 ]), 'NUM': [28, 50]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3624\n",
            "[*] LOSS: 0.34074777364730835 / Q: {'Eval-Q': 4.120346, 'Target-Q': 4.038998}\n",
            "[*] LOSS: 0.27100518345832825 / Q: {'Eval-Q': 3.067364, 'Target-Q': 3.071173}\n",
            "[*] LOSS: 0.4116997718811035 / Q: {'Eval-Q': 3.782909, 'Target-Q': 3.740463}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05498814693045781, 'total_reward': 153.58499938901514, 'kill': 39}\n",
            "\n",
            "\n",
            "[*] ROUND #1766, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.195192, 0.103265]), 'NUM': [26, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3563\n",
            "[*] LOSS: 0.7590560913085938 / Q: {'Eval-Q': 3.74416, 'Target-Q': 3.723493}\n",
            "[*] LOSS: 0.3480076193809509 / Q: {'Eval-Q': 3.661103, 'Target-Q': 3.68215}\n",
            "[*] LOSS: 0.40304720401763916 / Q: {'Eval-Q': 3.783339, 'Target-Q': 3.715451}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0731208427393724, 'total_reward': 180.78999928291887, 'kill': 43}\n",
            "\n",
            "\n",
            "[*] ROUND #1767, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.134024, -0.00898 ]), 'NUM': [41, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4019\n",
            "[*] LOSS: 0.3035111427307129 / Q: {'Eval-Q': 4.144664, 'Target-Q': 4.013667}\n",
            "[*] LOSS: 0.5202969312667847 / Q: {'Eval-Q': 4.078992, 'Target-Q': 3.725286}\n",
            "[*] LOSS: 0.17771552503108978 / Q: {'Eval-Q': 4.101618, 'Target-Q': 2.94989}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12335436136351022, 'total_reward': 255.8099988847971, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #1768, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.023571, 0.009   ]), 'NUM': [28, 50]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3617\n",
            "[*] LOSS: 0.4134843051433563 / Q: {'Eval-Q': 3.376024, 'Target-Q': 2.712015}\n",
            "[*] LOSS: 0.3495284914970398 / Q: {'Eval-Q': 4.023644, 'Target-Q': 4.10925}\n",
            "[*] LOSS: 0.3949100077152252 / Q: {'Eval-Q': 4.2519, 'Target-Q': 4.11387}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1445378594542347, 'total_reward': 176.91999937500805, 'kill': 41}\n",
            "\n",
            "\n",
            "[*] ROUND #1769, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.013333, 0.087   ]), 'NUM': [33, 50]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3765\n",
            "[*] LOSS: 0.5167882442474365 / Q: {'Eval-Q': 4.444787, 'Target-Q': 4.098859}\n",
            "[*] LOSS: 0.25554159283638 / Q: {'Eval-Q': 3.601382, 'Target-Q': 3.448884}\n",
            "[*] LOSS: 0.3076188564300537 / Q: {'Eval-Q': 3.685058, 'Target-Q': 3.463637}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16071050744469675, 'total_reward': 230.57999899517745, 'kill': 49}\n",
            "\n",
            "\n",
            "[*] ROUND #1770, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.018816, 0.090918]), 'NUM': [38, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3968\n",
            "[*] LOSS: 0.47669532895088196 / Q: {'Eval-Q': 3.593919, 'Target-Q': 3.515711}\n",
            "[*] LOSS: 0.30260807275772095 / Q: {'Eval-Q': 4.338853, 'Target-Q': 4.15987}\n",
            "[*] LOSS: 0.23539979755878448 / Q: {'Eval-Q': 3.793307, 'Target-Q': 3.552613}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1628047392675146, 'total_reward': 242.06499884370714, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #1771, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.010152, -0.016364]), 'NUM': [33, 44]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3866\n",
            "[*] LOSS: 0.37944725155830383 / Q: {'Eval-Q': 3.684796, 'Target-Q': 3.663645}\n",
            "[*] LOSS: 0.3803139328956604 / Q: {'Eval-Q': 3.807177, 'Target-Q': 3.91092}\n",
            "[*] LOSS: 0.227632537484169 / Q: {'Eval-Q': 3.424689, 'Target-Q': 3.182898}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1244827390751274, 'total_reward': 262.17499887850136, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #1772, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.332   , 0.174038]), 'NUM': [30, 52]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3726\n",
            "[*] LOSS: 0.22867451608181 / Q: {'Eval-Q': 4.348232, 'Target-Q': 3.935905}\n",
            "[*] LOSS: 0.21071842312812805 / Q: {'Eval-Q': 3.976872, 'Target-Q': 3.707978}\n",
            "[*] LOSS: 0.3598819971084595 / Q: {'Eval-Q': 3.809612, 'Target-Q': 3.820905}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11024069743433143, 'total_reward': 229.37499910965562, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #1773, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.004   , 0.297041]), 'NUM': [35, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3763\n",
            "[*] LOSS: 0.24953758716583252 / Q: {'Eval-Q': 4.834574, 'Target-Q': 4.513622}\n",
            "[*] LOSS: 0.5068322420120239 / Q: {'Eval-Q': 4.712353, 'Target-Q': 4.715694}\n",
            "[*] LOSS: 0.44934526085853577 / Q: {'Eval-Q': 4.713377, 'Target-Q': 4.699034}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07974848926851431, 'total_reward': 190.5899992035702, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #1774, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.138718, 0.084182]), 'NUM': [39, 55]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.095, -0.005]), 'NUM': [1, 33]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4045\n",
            "[*] LOSS: 0.6024333834648132 / Q: {'Eval-Q': 4.457945, 'Target-Q': 4.024496}\n",
            "[*] LOSS: 0.3147069215774536 / Q: {'Eval-Q': 4.712986, 'Target-Q': 4.363834}\n",
            "[*] LOSS: 0.49005210399627686 / Q: {'Eval-Q': 4.300007, 'Target-Q': 4.36802}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07367596992928983, 'total_reward': 215.97999907564372, 'kill': 48}\n",
            "\n",
            "\n",
            "[*] ROUND #1775, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.281  , 0.23025]), 'NUM': [35, 40]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.000882, -0.005   ]), 'NUM': [17, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4659\n",
            "[*] LOSS: 0.3173510730266571 / Q: {'Eval-Q': 3.813628, 'Target-Q': 3.682938}\n",
            "[*] LOSS: 0.35434597730636597 / Q: {'Eval-Q': 5.001818, 'Target-Q': 4.569722}\n",
            "[*] LOSS: 0.5666403770446777 / Q: {'Eval-Q': 4.680354, 'Target-Q': 4.619788}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11698679483387525, 'total_reward': 377.1299981381744, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1776, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.004205, 0.093148]), 'NUM': [44, 54]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4187\n",
            "[*] LOSS: 0.4566020965576172 / Q: {'Eval-Q': 4.626313, 'Target-Q': 4.538019}\n",
            "[*] LOSS: 0.28342604637145996 / Q: {'Eval-Q': 4.545205, 'Target-Q': 4.468673}\n",
            "[*] LOSS: 0.32633092999458313 / Q: {'Eval-Q': 4.226653, 'Target-Q': 4.004773}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11277144105918593, 'total_reward': 235.06999900657684, 'kill': 52}\n",
            "\n",
            "\n",
            "[*] ROUND #1777, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.268108, 0.089082]), 'NUM': [37, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3965\n",
            "[*] LOSS: 0.3242475986480713 / Q: {'Eval-Q': 5.025276, 'Target-Q': 4.64787}\n",
            "[*] LOSS: 0.481781005859375 / Q: {'Eval-Q': 4.553462, 'Target-Q': 4.11662}\n",
            "[*] LOSS: 0.3948589563369751 / Q: {'Eval-Q': 4.683274, 'Target-Q': 4.676492}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08440047109171195, 'total_reward': 212.57999915536493, 'kill': 50}\n",
            "\n",
            "\n",
            "[*] ROUND #1778, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.2011  , 0.108864]), 'NUM': [50, 44]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4903\n",
            "[*] LOSS: 0.4331284463405609 / Q: {'Eval-Q': 3.952841, 'Target-Q': 3.674931}\n",
            "[*] LOSS: 0.4768945872783661 / Q: {'Eval-Q': 3.94531, 'Target-Q': 3.645288}\n",
            "[*] LOSS: 0.24951106309890747 / Q: {'Eval-Q': 3.835025, 'Target-Q': 3.745779}\n",
            "[*] LOSS: 0.5104985237121582 / Q: {'Eval-Q': 4.561988, 'Target-Q': 4.490298}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10782214795416899, 'total_reward': 363.9099983330816, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1779, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.010667, 0.106111]), 'NUM': [45, 45]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4955\n",
            "[*] LOSS: 0.5357490181922913 / Q: {'Eval-Q': 4.039429, 'Target-Q': 3.951165}\n",
            "[*] LOSS: 0.36202067136764526 / Q: {'Eval-Q': 4.579355, 'Target-Q': 4.206272}\n",
            "[*] LOSS: 0.4498324990272522 / Q: {'Eval-Q': 4.957409, 'Target-Q': 5.02692}\n",
            "[*] LOSS: 0.49333786964416504 / Q: {'Eval-Q': 4.531851, 'Target-Q': 4.466018}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10606409762058039, 'total_reward': 399.9099982632324, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1780.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1780, EPS: 0.16 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.139861, 0.358537]), 'NUM': [36, 41]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3937\n",
            "[*] LOSS: 0.18513859808444977 / Q: {'Eval-Q': 4.712874, 'Target-Q': 4.473306}\n",
            "[*] LOSS: 0.27229827642440796 / Q: {'Eval-Q': 3.471383, 'Target-Q': 3.464834}\n",
            "[*] LOSS: 0.41044825315475464 / Q: {'Eval-Q': 4.093239, 'Target-Q': 4.050223}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10949861971369917, 'total_reward': 275.41999884322286, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #1781, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.172167, 0.24641 ]), 'NUM': [30, 39]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 16]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3939\n",
            "[*] LOSS: 0.5639343857765198 / Q: {'Eval-Q': 4.468025, 'Target-Q': 4.455374}\n",
            "[*] LOSS: 0.4249395728111267 / Q: {'Eval-Q': 4.755788, 'Target-Q': 4.379726}\n",
            "[*] LOSS: 0.3043237626552582 / Q: {'Eval-Q': 4.369556, 'Target-Q': 4.089799}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07953677171587373, 'total_reward': 296.80999874323606, 'kill': 65}\n",
            "\n",
            "\n",
            "[*] ROUND #1782, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.114149, -0.007317]), 'NUM': [47, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.009286, -0.005   ]), 'NUM': [7, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.009286, -0.005   ]), 'NUM': [7, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5662\n",
            "[*] LOSS: 0.3667899966239929 / Q: {'Eval-Q': 4.049565, 'Target-Q': 3.95914}\n",
            "[*] LOSS: 0.5149473547935486 / Q: {'Eval-Q': 4.578098, 'Target-Q': 4.070774}\n",
            "[*] LOSS: 0.510699987411499 / Q: {'Eval-Q': 3.916854, 'Target-Q': 3.701562}\n",
            "[*] LOSS: 0.3542657196521759 / Q: {'Eval-Q': 4.164621, 'Target-Q': 4.30486}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.051802611331173365, 'total_reward': 376.65999840572476, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1783, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.39    , 0.109302]), 'NUM': [39, 43]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6362\n",
            "[*] LOSS: 0.27358320355415344 / Q: {'Eval-Q': 3.919549, 'Target-Q': 3.514316}\n",
            "[*] LOSS: 0.41927793622016907 / Q: {'Eval-Q': 4.52295, 'Target-Q': 4.388717}\n",
            "[*] LOSS: 0.2645326256752014 / Q: {'Eval-Q': 3.938404, 'Target-Q': 3.932698}\n",
            "[*] LOSS: 0.5913489460945129 / Q: {'Eval-Q': 4.322992, 'Target-Q': 4.078146}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.050060110852169715, 'total_reward': 363.12499826028943, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1784, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.317187, 0.205625]), 'NUM': [32, 48]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3742\n",
            "[*] LOSS: 0.5387015342712402 / Q: {'Eval-Q': 4.572056, 'Target-Q': 4.091545}\n",
            "[*] LOSS: 0.6813365817070007 / Q: {'Eval-Q': 5.664003, 'Target-Q': 5.707928}\n",
            "[*] LOSS: 0.36328378319740295 / Q: {'Eval-Q': 4.324482, 'Target-Q': 4.317098}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09086038473924253, 'total_reward': 217.19499908480793, 'kill': 48}\n",
            "\n",
            "\n",
            "[*] ROUND #1785, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004872,  0.092872]), 'NUM': [39, 47]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4090\n",
            "[*] LOSS: 0.3292056918144226 / Q: {'Eval-Q': 4.237266, 'Target-Q': 4.17485}\n",
            "[*] LOSS: 0.3187010884284973 / Q: {'Eval-Q': 4.645764, 'Target-Q': 4.45313}\n",
            "[*] LOSS: 0.7701505422592163 / Q: {'Eval-Q': 4.12915, 'Target-Q': 3.917962}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11433071355218971, 'total_reward': 234.7549989502877, 'kill': 51}\n",
            "\n",
            "\n",
            "[*] ROUND #1786, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.006081, 0.175392]), 'NUM': [37, 51]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3880\n",
            "[*] LOSS: 0.1439177244901657 / Q: {'Eval-Q': 4.19961, 'Target-Q': 4.069158}\n",
            "[*] LOSS: 0.2685508131980896 / Q: {'Eval-Q': 4.151848, 'Target-Q': 4.026048}\n",
            "[*] LOSS: 0.479397714138031 / Q: {'Eval-Q': 4.420269, 'Target-Q': 4.505408}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07727205590207455, 'total_reward': 187.00499918870628, 'kill': 44}\n",
            "\n",
            "\n",
            "[*] ROUND #1787, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.128333, -0.007317]), 'NUM': [42, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4973\n",
            "[*] LOSS: 0.33837294578552246 / Q: {'Eval-Q': 4.23679, 'Target-Q': 4.170546}\n",
            "[*] LOSS: 0.2964142858982086 / Q: {'Eval-Q': 4.330024, 'Target-Q': 4.129008}\n",
            "[*] LOSS: 0.47933244705200195 / Q: {'Eval-Q': 4.611136, 'Target-Q': 4.615078}\n",
            "[*] LOSS: 0.3539975881576538 / Q: {'Eval-Q': 4.716668, 'Target-Q': 4.488518}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09635931658117856, 'total_reward': 389.5649982728064, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1788, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.125952, 0.001489]), 'NUM': [42, 47]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4911\n",
            "[*] LOSS: 0.5397396683692932 / Q: {'Eval-Q': 4.003379, 'Target-Q': 3.917734}\n",
            "[*] LOSS: 0.500998854637146 / Q: {'Eval-Q': 3.755104, 'Target-Q': 3.720738}\n",
            "[*] LOSS: 1.3557246923446655 / Q: {'Eval-Q': 4.406792, 'Target-Q': 4.455065}\n",
            "[*] LOSS: 0.5071449875831604 / Q: {'Eval-Q': 4.194704, 'Target-Q': 3.952002}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11020577140993665, 'total_reward': 399.434998203069, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1789, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.136154, 0.099556]), 'NUM': [39, 45]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [14, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [13, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8409\n",
            "[*] LOSS: 0.4535118341445923 / Q: {'Eval-Q': 4.476843, 'Target-Q': 4.262898}\n",
            "[*] LOSS: 0.24876920878887177 / Q: {'Eval-Q': 3.989517, 'Target-Q': 3.902286}\n",
            "[*] LOSS: 0.5904573202133179 / Q: {'Eval-Q': 3.746674, 'Target-Q': 3.899972}\n",
            "[*] LOSS: 0.42326655983924866 / Q: {'Eval-Q': 3.984284, 'Target-Q': 3.526907}\n",
            "[*] LOSS: 0.7234856486320496 / Q: {'Eval-Q': 4.06461, 'Target-Q': 3.903076}\n",
            "[*] LOSS: 0.7124966382980347 / Q: {'Eval-Q': 4.621844, 'Target-Q': 4.187481}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02666913586163606, 'total_reward': 352.49999872315675, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1790, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.118929, 0.08898 ]), 'NUM': [42, 49]}\n",
            "> step #100, info: {'Ave-Reward': array([ 1.22    , -0.011786]), 'NUM': [4, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 9]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 8]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005 ,  0.0075]), 'NUM': [2, 8]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5042\n",
            "[*] LOSS: 0.5150229930877686 / Q: {'Eval-Q': 3.767589, 'Target-Q': 3.74539}\n",
            "[*] LOSS: 0.42831951379776 / Q: {'Eval-Q': 3.752808, 'Target-Q': 3.660349}\n",
            "[*] LOSS: 0.3249991238117218 / Q: {'Eval-Q': 3.258925, 'Target-Q': 3.173504}\n",
            "[*] LOSS: 0.378322571516037 / Q: {'Eval-Q': 3.845004, 'Target-Q': 3.986458}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05353691591581395, 'total_reward': 339.0899983746931, 'kill': 73}\n",
            "\n",
            "\n",
            "[*] ROUND #1791, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.118021, 0.12369 ]), 'NUM': [48, 42]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4717\n",
            "[*] LOSS: 0.6222466230392456 / Q: {'Eval-Q': 3.743987, 'Target-Q': 3.786446}\n",
            "[*] LOSS: 0.4783000648021698 / Q: {'Eval-Q': 4.925038, 'Target-Q': 4.682574}\n",
            "[*] LOSS: 0.42873749136924744 / Q: {'Eval-Q': 3.396406, 'Target-Q': 3.164335}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1045979308687673, 'total_reward': 380.8449982693419, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1792, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.140395, 0.527021]), 'NUM': [38, 47]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 12]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4147\n",
            "[*] LOSS: 0.5331540107727051 / Q: {'Eval-Q': 3.092291, 'Target-Q': 3.094342}\n",
            "[*] LOSS: 0.280047208070755 / Q: {'Eval-Q': 4.180995, 'Target-Q': 3.765422}\n",
            "[*] LOSS: 0.328640341758728 / Q: {'Eval-Q': 3.596726, 'Target-Q': 3.648244}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1383165117895566, 'total_reward': 293.2699985699728, 'kill': 69}\n",
            "\n",
            "\n",
            "[*] ROUND #1793, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.013088, 0.352143]), 'NUM': [34, 42]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4666\n",
            "[*] LOSS: 0.4947575330734253 / Q: {'Eval-Q': 4.499318, 'Target-Q': 4.182694}\n",
            "[*] LOSS: 0.3058373034000397 / Q: {'Eval-Q': 3.921484, 'Target-Q': 3.931102}\n",
            "[*] LOSS: 0.18664878606796265 / Q: {'Eval-Q': 3.240852, 'Target-Q': 3.039136}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0948938937749614, 'total_reward': 384.3499982142821, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1794, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.004756, 0.003163]), 'NUM': [41, 49]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.038333,  0.00125 ]), 'NUM': [3, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4412\n",
            "[*] LOSS: 0.4552445709705353 / Q: {'Eval-Q': 3.414735, 'Target-Q': 3.264183}\n",
            "[*] LOSS: 0.5231872200965881 / Q: {'Eval-Q': 4.170874, 'Target-Q': 3.817244}\n",
            "[*] LOSS: 0.3998425006866455 / Q: {'Eval-Q': 3.779863, 'Target-Q': 3.565697}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14112485993792231, 'total_reward': 302.74499862641096, 'kill': 67}\n",
            "\n",
            "\n",
            "[*] ROUND #1795, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.006481, 0.2475  ]), 'NUM': [27, 40]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3744\n",
            "[*] LOSS: 0.23830284178256989 / Q: {'Eval-Q': 3.732606, 'Target-Q': 3.482152}\n",
            "[*] LOSS: 0.169830322265625 / Q: {'Eval-Q': 3.643138, 'Target-Q': 3.566606}\n",
            "[*] LOSS: 0.5809611678123474 / Q: {'Eval-Q': 3.38669, 'Target-Q': 3.207811}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.17553917846324055, 'total_reward': 300.7849986096844, 'kill': 66}\n",
            "\n",
            "\n",
            "[*] ROUND #1796, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.003889, -0.008571]), 'NUM': [45, 56]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.62    , -0.004583]), 'NUM': [8, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4979\n",
            "[*] LOSS: 0.22450461983680725 / Q: {'Eval-Q': 3.232748, 'Target-Q': 2.961105}\n",
            "[*] LOSS: 0.4250970184803009 / Q: {'Eval-Q': 3.501798, 'Target-Q': 3.422616}\n",
            "[*] LOSS: 0.08392568677663803 / Q: {'Eval-Q': 3.566948, 'Target-Q': 3.354113}\n",
            "[*] LOSS: 0.4796975553035736 / Q: {'Eval-Q': 3.761135, 'Target-Q': 3.647604}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06888547302729588, 'total_reward': 332.7099983235821, 'kill': 71}\n",
            "\n",
            "\n",
            "[*] ROUND #1797, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.010513, 0.097   ]), 'NUM': [39, 50]}\n",
            "> step #100, info: {'Ave-Reward': array([ 1.083889, -0.022273]), 'NUM': [9, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 6]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 6]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 6]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6622\n",
            "[*] LOSS: 0.21606872975826263 / Q: {'Eval-Q': 3.285999, 'Target-Q': 3.13689}\n",
            "[*] LOSS: 0.26391878724098206 / Q: {'Eval-Q': 3.517508, 'Target-Q': 3.111615}\n",
            "[*] LOSS: 0.31971627473831177 / Q: {'Eval-Q': 3.177389, 'Target-Q': 3.179616}\n",
            "[*] LOSS: 0.3389885723590851 / Q: {'Eval-Q': 3.304664, 'Target-Q': 2.978174}\n",
            "[*] LOSS: 0.3152957558631897 / Q: {'Eval-Q': 3.816842, 'Target-Q': 3.651258}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03344581712323795, 'total_reward': 348.6599986311048, 'kill': 75}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1798, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.200882, -0.004783]), 'NUM': [51, 46]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.002143,  0.045   ]), 'NUM': [35, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6105\n",
            "[*] LOSS: 0.4138501286506653 / Q: {'Eval-Q': 3.341994, 'Target-Q': 3.43992}\n",
            "[*] LOSS: 0.249437615275383 / Q: {'Eval-Q': 3.021705, 'Target-Q': 2.956667}\n",
            "[*] LOSS: 0.38049542903900146 / Q: {'Eval-Q': 3.067472, 'Target-Q': 3.163719}\n",
            "[*] LOSS: 0.44481295347213745 / Q: {'Eval-Q': 3.65108, 'Target-Q': 3.427002}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06797008653661399, 'total_reward': 374.11499820370227, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1799, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.004918,  0.092917]), 'NUM': [61, 48]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.004868,  0.1325  ]), 'NUM': [38, 32]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005   , -0.019286]), 'NUM': [21, 28]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.000263]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8174\n",
            "[*] LOSS: 0.13119614124298096 / Q: {'Eval-Q': 2.805926, 'Target-Q': 2.55475}\n",
            "[*] LOSS: 0.5167139768600464 / Q: {'Eval-Q': 3.682292, 'Target-Q': 3.130405}\n",
            "[*] LOSS: 0.22499261796474457 / Q: {'Eval-Q': 3.847654, 'Target-Q': 3.707897}\n",
            "[*] LOSS: 0.35596364736557007 / Q: {'Eval-Q': 3.045585, 'Target-Q': 3.017947}\n",
            "[*] LOSS: 0.3534703850746155 / Q: {'Eval-Q': 3.183523, 'Target-Q': 2.93801}\n",
            "[*] LOSS: 0.22614365816116333 / Q: {'Eval-Q': 3.33302, 'Target-Q': 3.186509}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06525151925243776, 'total_reward': 267.4349984852597, 'kill': 63}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1800.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1800, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.004528, 0.105204]), 'NUM': [53, 49]}\n",
            "> step #100, info: {'Ave-Reward': array([0.002321, 0.505   ]), 'NUM': [28, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13492\n",
            "[*] LOSS: 0.19762516021728516 / Q: {'Eval-Q': 2.847841, 'Target-Q': 2.776698}\n",
            "[*] LOSS: 0.5782012343406677 / Q: {'Eval-Q': 3.135886, 'Target-Q': 3.14759}\n",
            "[*] LOSS: 0.21477071940898895 / Q: {'Eval-Q': 2.957634, 'Target-Q': 2.950197}\n",
            "[*] LOSS: 0.11121471226215363 / Q: {'Eval-Q': 2.670745, 'Target-Q': 2.458992}\n",
            "[*] LOSS: 0.3403403162956238 / Q: {'Eval-Q': 3.077747, 'Target-Q': 2.92847}\n",
            "[*] LOSS: 0.22711627185344696 / Q: {'Eval-Q': 2.616355, 'Target-Q': 2.399547}\n",
            "[*] LOSS: 0.3427478075027466 / Q: {'Eval-Q': 2.581108, 'Target-Q': 2.571887}\n",
            "[*] LOSS: 0.16728796064853668 / Q: {'Eval-Q': 2.982391, 'Target-Q': 2.894016}\n",
            "[*] LOSS: 0.3888727128505707 / Q: {'Eval-Q': 3.324083, 'Target-Q': 3.369704}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018051790781073055, 'total_reward': 313.9099993323907, 'kill': 76}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1801, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002667,  0.101818]), 'NUM': [45, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([0.0075  , 0.028333]), 'NUM': [8, 9]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.015]), 'NUM': [2, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5313\n",
            "[*] LOSS: 0.4550391435623169 / Q: {'Eval-Q': 2.85434, 'Target-Q': 2.881812}\n",
            "[*] LOSS: 0.2737651467323303 / Q: {'Eval-Q': 2.635331, 'Target-Q': 2.709529}\n",
            "[*] LOSS: 0.39294108748435974 / Q: {'Eval-Q': 3.750186, 'Target-Q': 3.68432}\n",
            "[*] LOSS: 0.20248763263225555 / Q: {'Eval-Q': 3.374228, 'Target-Q': 3.332996}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.051392393486093174, 'total_reward': 365.83499809447676, 'kill': 78}\n",
            "\n",
            "\n",
            "[*] ROUND #1802, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.014189, 0.248846]), 'NUM': [37, 39]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4070\n",
            "[*] LOSS: 0.4147963225841522 / Q: {'Eval-Q': 3.540041, 'Target-Q': 3.360368}\n",
            "[*] LOSS: 0.29507139325141907 / Q: {'Eval-Q': 2.65161, 'Target-Q': 2.614066}\n",
            "[*] LOSS: 0.18621018528938293 / Q: {'Eval-Q': 3.444471, 'Target-Q': 3.151755}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.141878589032338, 'total_reward': 313.0549985039979, 'kill': 69}\n",
            "\n",
            "\n",
            "[*] ROUND #1803, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.356909, 0.076667]), 'NUM': [55, 48]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.016471,  0.550556]), 'NUM': [17, 9]}\n",
            "> step #150, info: {'Ave-Reward': array([0.028333, 0.011667]), 'NUM': [3, 6]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.105, -0.005]), 'NUM': [1, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6032\n",
            "[*] LOSS: 0.2793666422367096 / Q: {'Eval-Q': 2.62295, 'Target-Q': 2.547711}\n",
            "[*] LOSS: 0.30723586678504944 / Q: {'Eval-Q': 3.050856, 'Target-Q': 2.969215}\n",
            "[*] LOSS: 0.15388727188110352 / Q: {'Eval-Q': 2.754905, 'Target-Q': 2.745199}\n",
            "[*] LOSS: 0.6630696654319763 / Q: {'Eval-Q': 3.794257, 'Target-Q': 3.726409}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04674874065208089, 'total_reward': 361.6399979926646, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1804, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.015   , 0.005638]), 'NUM': [45, 47]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [10, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5357\n",
            "[*] LOSS: 0.21677468717098236 / Q: {'Eval-Q': 3.206637, 'Target-Q': 3.24937}\n",
            "[*] LOSS: 0.5360636711120605 / Q: {'Eval-Q': 3.310734, 'Target-Q': 3.269467}\n",
            "[*] LOSS: 0.1026264876127243 / Q: {'Eval-Q': 2.005953, 'Target-Q': 1.999528}\n",
            "[*] LOSS: 0.32224443554878235 / Q: {'Eval-Q': 3.395174, 'Target-Q': 3.288159}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0719061982348081, 'total_reward': 360.369998103939, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1805, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.139737, -0.007317]), 'NUM': [38, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5051\n",
            "[*] LOSS: 1.5027118921279907 / Q: {'Eval-Q': 2.677632, 'Target-Q': 2.749658}\n",
            "[*] LOSS: 0.8075078129768372 / Q: {'Eval-Q': 2.988606, 'Target-Q': 2.877762}\n",
            "[*] LOSS: 0.4086533188819885 / Q: {'Eval-Q': 2.796288, 'Target-Q': 2.715024}\n",
            "[*] LOSS: 0.2060251086950302 / Q: {'Eval-Q': 3.285147, 'Target-Q': 3.06511}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08591042901187587, 'total_reward': 352.46499833557755, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1806, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.114674, 0.112195]), 'NUM': [46, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5296\n",
            "[*] LOSS: 0.5331884026527405 / Q: {'Eval-Q': 3.913922, 'Target-Q': 3.834743}\n",
            "[*] LOSS: 0.139358252286911 / Q: {'Eval-Q': 2.726957, 'Target-Q': 2.714054}\n",
            "[*] LOSS: 0.07979763299226761 / Q: {'Eval-Q': 2.457705, 'Target-Q': 2.39246}\n",
            "[*] LOSS: 0.25173962116241455 / Q: {'Eval-Q': 3.327726, 'Target-Q': 3.241468}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07593163046103572, 'total_reward': 379.6499977633357, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1807, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.801613, 0.121163]), 'NUM': [31, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3769\n",
            "[*] LOSS: 0.3439587652683258 / Q: {'Eval-Q': 2.992022, 'Target-Q': 2.970725}\n",
            "[*] LOSS: 0.4160402715206146 / Q: {'Eval-Q': 3.56171, 'Target-Q': 3.168745}\n",
            "[*] LOSS: 0.31128162145614624 / Q: {'Eval-Q': 2.812524, 'Target-Q': 2.222234}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14644251052435964, 'total_reward': 295.6599986497313, 'kill': 65}\n",
            "\n",
            "\n",
            "[*] ROUND #1808, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.505   , -0.011477]), 'NUM': [30, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3875\n",
            "[*] LOSS: 0.3169342279434204 / Q: {'Eval-Q': 2.75609, 'Target-Q': 2.604468}\n",
            "[*] LOSS: 0.4584873914718628 / Q: {'Eval-Q': 2.637395, 'Target-Q': 2.696389}\n",
            "[*] LOSS: 0.5818499326705933 / Q: {'Eval-Q': 3.151291, 'Target-Q': 3.173115}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08654611251592763, 'total_reward': 292.02999860141426, 'kill': 62}\n",
            "\n",
            "\n",
            "[*] ROUND #1809, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.111   , 0.445341]), 'NUM': [45, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005,  0.015]), 'NUM': [20, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [19, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10641\n",
            "[*] LOSS: 0.5086108446121216 / Q: {'Eval-Q': 2.312573, 'Target-Q': 2.385474}\n",
            "[*] LOSS: 0.2465832233428955 / Q: {'Eval-Q': 2.661915, 'Target-Q': 2.38825}\n",
            "[*] LOSS: 0.21798580884933472 / Q: {'Eval-Q': 2.523114, 'Target-Q': 2.400584}\n",
            "[*] LOSS: 0.13885901868343353 / Q: {'Eval-Q': 2.666619, 'Target-Q': 2.529541}\n",
            "[*] LOSS: 0.4602307081222534 / Q: {'Eval-Q': 2.512842, 'Target-Q': 2.508913}\n",
            "[*] LOSS: 0.9446359872817993 / Q: {'Eval-Q': 3.616782, 'Target-Q': 3.780511}\n",
            "[*] LOSS: 0.193820059299469 / Q: {'Eval-Q': 3.056624, 'Target-Q': 3.073559}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.018776307983397886, 'total_reward': 311.5099990097806, 'kill': 76}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1810, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.373148, 0.257432]), 'NUM': [27, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3991\n",
            "[*] LOSS: 0.1876591444015503 / Q: {'Eval-Q': 2.95398, 'Target-Q': 2.966317}\n",
            "[*] LOSS: 0.10463041812181473 / Q: {'Eval-Q': 2.255532, 'Target-Q': 2.28331}\n",
            "[*] LOSS: 0.6386922001838684 / Q: {'Eval-Q': 2.576813, 'Target-Q': 2.577014}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.023184623212072406, 'total_reward': 281.54499876871705, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #1811, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.445   , -0.010968]), 'NUM': [34, 31]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.012692,  0.015   ]), 'NUM': [13, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([ 0.0075, -0.005 ]), 'NUM': [8, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [8, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6906\n",
            "[*] LOSS: 0.5237022042274475 / Q: {'Eval-Q': 2.95724, 'Target-Q': 3.030369}\n",
            "[*] LOSS: 0.17555251717567444 / Q: {'Eval-Q': 3.042024, 'Target-Q': 3.07457}\n",
            "[*] LOSS: 0.6921082735061646 / Q: {'Eval-Q': 3.131122, 'Target-Q': 3.034694}\n",
            "[*] LOSS: 0.35286426544189453 / Q: {'Eval-Q': 2.528769, 'Target-Q': 2.367761}\n",
            "[*] LOSS: 0.24599121510982513 / Q: {'Eval-Q': 3.774326, 'Target-Q': 3.451773}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024472458868701787, 'total_reward': 361.73499847669154, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1812, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.235476, -0.00913 ]), 'NUM': [42, 46]}\n",
            "> step #100, info: {'Ave-Reward': array([0.009286, 0.045   ]), 'NUM': [7, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.009286, -0.005   ]), 'NUM': [7, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6655\n",
            "[*] LOSS: 0.2759771943092346 / Q: {'Eval-Q': 3.412558, 'Target-Q': 3.098923}\n",
            "[*] LOSS: 0.0636962503194809 / Q: {'Eval-Q': 2.819008, 'Target-Q': 2.250867}\n",
            "[*] LOSS: 0.16177862882614136 / Q: {'Eval-Q': 3.118899, 'Target-Q': 3.063519}\n",
            "[*] LOSS: 0.5913978219032288 / Q: {'Eval-Q': 3.207183, 'Target-Q': 3.161716}\n",
            "[*] LOSS: 0.3880053460597992 / Q: {'Eval-Q': 3.15738, 'Target-Q': 3.055628}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03259859152273229, 'total_reward': 363.69499847572297, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1813, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.124659, 0.114634]), 'NUM': [44, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0175, -0.045 ]), 'NUM': [8, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.033571, -0.005   ]), 'NUM': [7, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4951\n",
            "[*] LOSS: 0.47518643736839294 / Q: {'Eval-Q': 2.897619, 'Target-Q': 2.968}\n",
            "[*] LOSS: 0.8136996030807495 / Q: {'Eval-Q': 2.987148, 'Target-Q': 2.944732}\n",
            "[*] LOSS: 0.6952922940254211 / Q: {'Eval-Q': 2.743712, 'Target-Q': 2.308348}\n",
            "[*] LOSS: 0.35187360644340515 / Q: {'Eval-Q': 2.442728, 'Target-Q': 2.567737}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07299015978928289, 'total_reward': 364.91999817080796, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1814, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.303824, -0.004744]), 'NUM': [34, 39]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4033\n",
            "[*] LOSS: 0.19273097813129425 / Q: {'Eval-Q': 2.556926, 'Target-Q': 2.558566}\n",
            "[*] LOSS: 0.34983351826667786 / Q: {'Eval-Q': 2.91288, 'Target-Q': 2.917228}\n",
            "[*] LOSS: 0.45673465728759766 / Q: {'Eval-Q': 3.01409, 'Target-Q': 3.108247}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07091867475226145, 'total_reward': 347.0399983888492, 'kill': 74}\n",
            "\n",
            "\n",
            "[*] ROUND #1815, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.32   , -0.00475]), 'NUM': [32, 40]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3874\n",
            "[*] LOSS: 0.46739184856414795 / Q: {'Eval-Q': 3.09684, 'Target-Q': 3.116804}\n",
            "[*] LOSS: 0.19316823780536652 / Q: {'Eval-Q': 2.94714, 'Target-Q': 2.766423}\n",
            "[*] LOSS: 0.21002569794654846 / Q: {'Eval-Q': 3.342789, 'Target-Q': 3.287577}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13691271594320217, 'total_reward': 301.1349984379485, 'kill': 65}\n",
            "\n",
            "\n",
            "[*] ROUND #1816, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.01569, -0.005  ]), 'NUM': [29, 39]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3793\n",
            "[*] LOSS: 0.5497032999992371 / Q: {'Eval-Q': 2.850993, 'Target-Q': 2.913308}\n",
            "[*] LOSS: 0.23804420232772827 / Q: {'Eval-Q': 3.24249, 'Target-Q': 3.251075}\n",
            "[*] LOSS: 0.526722252368927 / Q: {'Eval-Q': 3.017298, 'Target-Q': 2.947155}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11426576693281502, 'total_reward': 306.23999857436866, 'kill': 65}\n",
            "\n",
            "\n",
            "[*] ROUND #1817, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.424375, 0.114756]), 'NUM': [24, 41]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3625\n",
            "[*] LOSS: 0.6313245296478271 / Q: {'Eval-Q': 3.782493, 'Target-Q': 3.721895}\n",
            "[*] LOSS: 0.5091214776039124 / Q: {'Eval-Q': 3.216864, 'Target-Q': 3.116289}\n",
            "[*] LOSS: 0.35664427280426025 / Q: {'Eval-Q': 2.940371, 'Target-Q': 2.941958}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12486423547565165, 'total_reward': 280.4799986789003, 'kill': 64}\n",
            "\n",
            "\n",
            "[*] ROUND #1818, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.188333, 0.000857]), 'NUM': [30, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5826\n",
            "[*] LOSS: 0.14418017864227295 / Q: {'Eval-Q': 2.626735, 'Target-Q': 2.461131}\n",
            "[*] LOSS: 0.44399914145469666 / Q: {'Eval-Q': 3.255911, 'Target-Q': 3.100434}\n",
            "[*] LOSS: 0.49576884508132935 / Q: {'Eval-Q': 2.826128, 'Target-Q': 2.870684}\n",
            "[*] LOSS: 0.23206821084022522 / Q: {'Eval-Q': 3.118224, 'Target-Q': 2.698779}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03262878364162088, 'total_reward': 351.2449983730912, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1819, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002674, -0.02593 ]), 'NUM': [43, 43]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4450\n",
            "[*] LOSS: 0.27049896121025085 / Q: {'Eval-Q': 3.08383, 'Target-Q': 2.97832}\n",
            "[*] LOSS: 0.2559184730052948 / Q: {'Eval-Q': 2.974527, 'Target-Q': 2.665636}\n",
            "[*] LOSS: 0.2615833282470703 / Q: {'Eval-Q': 3.403795, 'Target-Q': 3.345978}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07937635724599257, 'total_reward': 258.95499864034355, 'kill': 58}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1820.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1820, EPS: 0.15 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.118462, 0.365125]), 'NUM': [39, 40]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.02 , -0.005]), 'NUM': [4, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4325\n",
            "[*] LOSS: 0.185503751039505 / Q: {'Eval-Q': 3.735992, 'Target-Q': 3.735554}\n",
            "[*] LOSS: 0.2976193428039551 / Q: {'Eval-Q': 3.850808, 'Target-Q': 3.396479}\n",
            "[*] LOSS: 0.19286003708839417 / Q: {'Eval-Q': 3.54975, 'Target-Q': 3.615926}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07358823696778294, 'total_reward': 314.679998293519, 'kill': 70}\n",
            "\n",
            "\n",
            "[*] ROUND #1821, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.254359, 0.22939 ]), 'NUM': [39, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 13]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.005]), 'NUM': [2, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 9]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4513\n",
            "[*] LOSS: 0.2093544900417328 / Q: {'Eval-Q': 4.251319, 'Target-Q': 4.21394}\n",
            "[*] LOSS: 0.9660109877586365 / Q: {'Eval-Q': 4.488819, 'Target-Q': 4.4169}\n",
            "[*] LOSS: 0.6526138186454773 / Q: {'Eval-Q': 3.309168, 'Target-Q': 3.477078}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08044413873149002, 'total_reward': 331.33499823883176, 'kill': 74}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1822, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.3075  , -0.001406]), 'NUM': [48, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [16, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5812\n",
            "[*] LOSS: 0.40899857878685 / Q: {'Eval-Q': 3.506582, 'Target-Q': 3.447673}\n",
            "[*] LOSS: 0.3964022696018219 / Q: {'Eval-Q': 3.101443, 'Target-Q': 3.001516}\n",
            "[*] LOSS: 0.14215804636478424 / Q: {'Eval-Q': 2.837016, 'Target-Q': 2.651744}\n",
            "[*] LOSS: 0.7130062580108643 / Q: {'Eval-Q': 4.324787, 'Target-Q': 3.975834}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0422529108527178, 'total_reward': 350.6199980759993, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1823, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.127791, 0.272917]), 'NUM': [43, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [15, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5868\n",
            "[*] LOSS: 0.2380480319261551 / Q: {'Eval-Q': 3.230539, 'Target-Q': 3.06025}\n",
            "[*] LOSS: 0.4057767987251282 / Q: {'Eval-Q': 3.831154, 'Target-Q': 3.889513}\n",
            "[*] LOSS: 0.21982251107692719 / Q: {'Eval-Q': 3.037038, 'Target-Q': 2.883629}\n",
            "[*] LOSS: 0.34578219056129456 / Q: {'Eval-Q': 3.996706, 'Target-Q': 3.818629}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05581200472744118, 'total_reward': 371.68999826256186, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1824, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.129268, 0.152286]), 'NUM': [41, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.028333]), 'NUM': [13, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.009286, -0.055   ]), 'NUM': [7, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.021667, -0.005   ]), 'NUM': [6, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5332\n",
            "[*] LOSS: 0.4453831911087036 / Q: {'Eval-Q': 4.300553, 'Target-Q': 3.932254}\n",
            "[*] LOSS: 0.2383577674627304 / Q: {'Eval-Q': 3.302617, 'Target-Q': 3.347893}\n",
            "[*] LOSS: 0.538951575756073 / Q: {'Eval-Q': 4.604885, 'Target-Q': 4.669514}\n",
            "[*] LOSS: 0.333946168422699 / Q: {'Eval-Q': 3.624769, 'Target-Q': 3.688151}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.059596125414029505, 'total_reward': 381.91499808989465, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1825, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.001111,  0.255526]), 'NUM': [54, 38]}\n",
            "> step #100, info: {'Ave-Reward': array([0.010385, 0.015   ]), 'NUM': [13, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [12, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6144\n",
            "[*] LOSS: 0.33395397663116455 / Q: {'Eval-Q': 3.727408, 'Target-Q': 3.495532}\n",
            "[*] LOSS: 0.31838124990463257 / Q: {'Eval-Q': 4.122523, 'Target-Q': 3.997886}\n",
            "[*] LOSS: 0.6923102140426636 / Q: {'Eval-Q': 3.736069, 'Target-Q': 3.517206}\n",
            "[*] LOSS: 0.3128608465194702 / Q: {'Eval-Q': 3.620205, 'Target-Q': 3.525111}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.050399133907409195, 'total_reward': 376.42499812692404, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1826, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.423587, 0.322111]), 'NUM': [46, 45]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.012143, -0.005   ]), 'NUM': [14, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.055]), 'NUM': [14, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [13, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.03 , -0.005]), 'NUM': [12, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8188\n",
            "[*] LOSS: 0.5821685791015625 / Q: {'Eval-Q': 3.984568, 'Target-Q': 3.756892}\n",
            "[*] LOSS: 0.16543497145175934 / Q: {'Eval-Q': 2.996766, 'Target-Q': 2.889108}\n",
            "[*] LOSS: 0.3879333734512329 / Q: {'Eval-Q': 3.130595, 'Target-Q': 3.108002}\n",
            "[*] LOSS: 0.173404261469841 / Q: {'Eval-Q': 2.922921, 'Target-Q': 2.682759}\n",
            "[*] LOSS: 0.7153208255767822 / Q: {'Eval-Q': 3.435398, 'Target-Q': 3.297628}\n",
            "[*] LOSS: 1.053972840309143 / Q: {'Eval-Q': 4.112466, 'Target-Q': 3.713918}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02339384564286638, 'total_reward': 359.1099978303537, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1827, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.112128, 0.141081]), 'NUM': [47, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007969,  2.445   ]), 'NUM': [32, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [28, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.055]), 'NUM': [23, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [20, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005,  0.045]), 'NUM': [16, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11795\n",
            "[*] LOSS: 0.6347815990447998 / Q: {'Eval-Q': 3.810898, 'Target-Q': 3.59235}\n",
            "[*] LOSS: 0.37593528628349304 / Q: {'Eval-Q': 4.183459, 'Target-Q': 3.925215}\n",
            "[*] LOSS: 0.25434935092926025 / Q: {'Eval-Q': 2.899283, 'Target-Q': 2.904095}\n",
            "[*] LOSS: 0.5152140259742737 / Q: {'Eval-Q': 3.488643, 'Target-Q': 3.094756}\n",
            "[*] LOSS: 0.39011266827583313 / Q: {'Eval-Q': 3.254968, 'Target-Q': 3.125527}\n",
            "[*] LOSS: 0.4283013343811035 / Q: {'Eval-Q': 2.899697, 'Target-Q': 2.831236}\n",
            "[*] LOSS: 0.38866665959358215 / Q: {'Eval-Q': 2.961829, 'Target-Q': 2.665279}\n",
            "[*] LOSS: 0.24627187848091125 / Q: {'Eval-Q': 3.758976, 'Target-Q': 3.51649}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016311727244418744, 'total_reward': 336.34999850485474, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1828, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.095179, 0.267361]), 'NUM': [56, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [45, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [41, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [33, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [26, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [23, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [23, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [23, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 15065\n",
            "[*] LOSS: 0.18798649311065674 / Q: {'Eval-Q': 2.613948, 'Target-Q': 2.593046}\n",
            "[*] LOSS: 0.1959950029850006 / Q: {'Eval-Q': 2.467275, 'Target-Q': 2.36791}\n",
            "[*] LOSS: 0.2973903715610504 / Q: {'Eval-Q': 2.608959, 'Target-Q': 2.59374}\n",
            "[*] LOSS: 0.5021718144416809 / Q: {'Eval-Q': 2.639138, 'Target-Q': 2.721662}\n",
            "[*] LOSS: 0.2755504548549652 / Q: {'Eval-Q': 2.53669, 'Target-Q': 2.443954}\n",
            "[*] LOSS: 0.5862619280815125 / Q: {'Eval-Q': 2.994758, 'Target-Q': 3.001546}\n",
            "[*] LOSS: 0.0972122922539711 / Q: {'Eval-Q': 2.821486, 'Target-Q': 2.69837}\n",
            "[*] LOSS: 0.4515901505947113 / Q: {'Eval-Q': 2.833213, 'Target-Q': 2.889593}\n",
            "[*] LOSS: 0.4541146159172058 / Q: {'Eval-Q': 2.462214, 'Target-Q': 2.504155}\n",
            "[*] LOSS: 0.3128073215484619 / Q: {'Eval-Q': 2.503794, 'Target-Q': 2.332661}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012420600629808478, 'total_reward': 321.16499912086874, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1829, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.242542, 0.088936]), 'NUM': [59, 47]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.095]), 'NUM': [47, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.007375,  4.895   ]), 'NUM': [40, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007879,  4.895   ]), 'NUM': [33, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.001, -0.005]), 'NUM': [25, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13051\n",
            "[*] LOSS: 0.1143810898065567 / Q: {'Eval-Q': 2.088195, 'Target-Q': 2.059718}\n",
            "[*] LOSS: 0.11733012646436691 / Q: {'Eval-Q': 1.921222, 'Target-Q': 1.874575}\n",
            "[*] LOSS: 0.15916483104228973 / Q: {'Eval-Q': 2.11483, 'Target-Q': 2.091612}\n",
            "[*] LOSS: 0.28125840425491333 / Q: {'Eval-Q': 2.320924, 'Target-Q': 2.25158}\n",
            "[*] LOSS: 0.20278680324554443 / Q: {'Eval-Q': 2.956973, 'Target-Q': 2.848347}\n",
            "[*] LOSS: 0.18354453146457672 / Q: {'Eval-Q': 2.038226, 'Target-Q': 1.926642}\n",
            "[*] LOSS: 0.2716868221759796 / Q: {'Eval-Q': 2.053176, 'Target-Q': 1.913252}\n",
            "[*] LOSS: 0.1301283985376358 / Q: {'Eval-Q': 1.920188, 'Target-Q': 1.968579}\n",
            "[*] LOSS: 0.16159281134605408 / Q: {'Eval-Q': 2.30638, 'Target-Q': 2.246998}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01938185981307597, 'total_reward': 354.4249987555668, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1830, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.369167, 0.114524]), 'NUM': [54, 42]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5124\n",
            "[*] LOSS: 0.23262543976306915 / Q: {'Eval-Q': 2.529028, 'Target-Q': 2.59015}\n",
            "[*] LOSS: 0.5037220120429993 / Q: {'Eval-Q': 2.356388, 'Target-Q': 2.450243}\n",
            "[*] LOSS: 0.21451511979103088 / Q: {'Eval-Q': 2.230244, 'Target-Q': 1.942344}\n",
            "[*] LOSS: 0.15060263872146606 / Q: {'Eval-Q': 2.106207, 'Target-Q': 2.170042}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08283217526588675, 'total_reward': 364.36999788321555, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1831, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.09    , 0.469516]), 'NUM': [57, 31]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 21583\n",
            "[*] LOSS: 0.12787358462810516 / Q: {'Eval-Q': 1.896112, 'Target-Q': 1.858378}\n",
            "[*] LOSS: 0.08521825820207596 / Q: {'Eval-Q': 1.950236, 'Target-Q': 1.934726}\n",
            "[*] LOSS: 0.21993298828601837 / Q: {'Eval-Q': 1.167675, 'Target-Q': 1.369572}\n",
            "[*] LOSS: 0.19744019210338593 / Q: {'Eval-Q': 1.514676, 'Target-Q': 1.329169}\n",
            "[*] LOSS: 0.1802382916212082 / Q: {'Eval-Q': 1.494457, 'Target-Q': 1.571453}\n",
            "[*] LOSS: 0.1566946804523468 / Q: {'Eval-Q': 1.805118, 'Target-Q': 1.77006}\n",
            "[*] LOSS: 0.20189569890499115 / Q: {'Eval-Q': 1.238596, 'Target-Q': 1.306823}\n",
            "[*] LOSS: 0.2182541936635971 / Q: {'Eval-Q': 1.466645, 'Target-Q': 1.454883}\n",
            "[*] LOSS: 0.21882203221321106 / Q: {'Eval-Q': 1.652156, 'Target-Q': 1.733805}\n",
            "[*] LOSS: 0.07584031671285629 / Q: {'Eval-Q': 1.42985, 'Target-Q': 1.182008}\n",
            "[*] LOSS: 0.09930144995450974 / Q: {'Eval-Q': 1.205593, 'Target-Q': 1.24156}\n",
            "[*] LOSS: 0.057862021028995514 / Q: {'Eval-Q': 1.783066, 'Target-Q': 1.739536}\n",
            "[*] LOSS: 0.31846827268600464 / Q: {'Eval-Q': 1.877289, 'Target-Q': 1.696566}\n",
            "[*] LOSS: 0.42652827501296997 / Q: {'Eval-Q': 1.326632, 'Target-Q': 1.21391}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.011806544281878918, 'total_reward': 299.5349998306483, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1832, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.002105, 0.105638]), 'NUM': [57, 47]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5226\n",
            "[*] LOSS: 0.3170039653778076 / Q: {'Eval-Q': 1.821035, 'Target-Q': 1.950222}\n",
            "[*] LOSS: 0.0306813046336174 / Q: {'Eval-Q': 1.187604, 'Target-Q': 1.174539}\n",
            "[*] LOSS: 0.16557395458221436 / Q: {'Eval-Q': 1.591022, 'Target-Q': 1.540209}\n",
            "[*] LOSS: 0.17741093039512634 / Q: {'Eval-Q': 1.632473, 'Target-Q': 1.700963}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09041196386701802, 'total_reward': 383.21999785955995, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1833, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.011512, 0.24378 ]), 'NUM': [43, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 10]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 10]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.008571, -0.005   ]), 'NUM': [28, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13496\n",
            "[*] LOSS: 0.1346263587474823 / Q: {'Eval-Q': 1.591074, 'Target-Q': 1.534604}\n",
            "[*] LOSS: 0.09119600057601929 / Q: {'Eval-Q': 1.235838, 'Target-Q': 1.26369}\n",
            "[*] LOSS: 0.13907834887504578 / Q: {'Eval-Q': 1.538049, 'Target-Q': 1.462472}\n",
            "[*] LOSS: 0.3403630256652832 / Q: {'Eval-Q': 1.56094, 'Target-Q': 1.58124}\n",
            "[*] LOSS: 0.6898453235626221 / Q: {'Eval-Q': 1.518072, 'Target-Q': 1.521043}\n",
            "[*] LOSS: 0.5619087219238281 / Q: {'Eval-Q': 1.360391, 'Target-Q': 1.418064}\n",
            "[*] LOSS: 0.1696295589208603 / Q: {'Eval-Q': 1.551435, 'Target-Q': 1.584128}\n",
            "[*] LOSS: 0.2526480257511139 / Q: {'Eval-Q': 1.523142, 'Target-Q': 1.46296}\n",
            "[*] LOSS: 0.07810105383396149 / Q: {'Eval-Q': 1.418157, 'Target-Q': 1.398222}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.011067509358293492, 'total_reward': 222.28499806579202, 'kill': 71}\n",
            "\n",
            "\n",
            "[*] ROUND #1834, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.481364, 0.010778]), 'NUM': [22, 45]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3562\n",
            "[*] LOSS: 0.10187838226556778 / Q: {'Eval-Q': 1.543003, 'Target-Q': 1.414703}\n",
            "[*] LOSS: 0.24268250167369843 / Q: {'Eval-Q': 1.783618, 'Target-Q': 1.868842}\n",
            "[*] LOSS: 0.09317649155855179 / Q: {'Eval-Q': 1.815732, 'Target-Q': 1.757968}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.17309648844941625, 'total_reward': 238.99499896541238, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #1835, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.087818, 0.123333]), 'NUM': [55, 39]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [47, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.007065,  1.628333]), 'NUM': [46, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.028333]), 'NUM': [39, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005   ,  0.028333]), 'NUM': [32, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.008654,  1.628333]), 'NUM': [26, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 16284\n",
            "[*] LOSS: 0.12187208980321884 / Q: {'Eval-Q': 1.525812, 'Target-Q': 1.496903}\n",
            "[*] LOSS: 0.4265698492527008 / Q: {'Eval-Q': 1.407602, 'Target-Q': 1.413079}\n",
            "[*] LOSS: 0.2812499403953552 / Q: {'Eval-Q': 1.533898, 'Target-Q': 1.574443}\n",
            "[*] LOSS: 0.11568722128868103 / Q: {'Eval-Q': 1.649397, 'Target-Q': 1.597789}\n",
            "[*] LOSS: 0.07878996431827545 / Q: {'Eval-Q': 1.74049, 'Target-Q': 1.692269}\n",
            "[*] LOSS: 0.2208741307258606 / Q: {'Eval-Q': 1.666404, 'Target-Q': 1.720185}\n",
            "[*] LOSS: 0.06887295842170715 / Q: {'Eval-Q': 1.511544, 'Target-Q': 1.513713}\n",
            "[*] LOSS: 0.19593846797943115 / Q: {'Eval-Q': 1.636061, 'Target-Q': 1.752694}\n",
            "[*] LOSS: 0.04338429868221283 / Q: {'Eval-Q': 1.338935, 'Target-Q': 1.367607}\n",
            "[*] LOSS: 0.08913156390190125 / Q: {'Eval-Q': 1.818465, 'Target-Q': 1.631045}\n",
            "[*] LOSS: 0.0668126791715622 / Q: {'Eval-Q': 1.734285, 'Target-Q': 1.527312}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01198072403209654, 'total_reward': 306.2599991597235, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1836, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.103511, -0.004857]), 'NUM': [47, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 19632\n",
            "[*] LOSS: 0.26203304529190063 / Q: {'Eval-Q': 1.348217, 'Target-Q': 1.220552}\n",
            "[*] LOSS: 0.1737373322248459 / Q: {'Eval-Q': 1.527204, 'Target-Q': 1.371124}\n",
            "[*] LOSS: 0.10390621423721313 / Q: {'Eval-Q': 1.029754, 'Target-Q': 0.970473}\n",
            "[*] LOSS: 0.07217558473348618 / Q: {'Eval-Q': 1.237249, 'Target-Q': 0.98786}\n",
            "[*] LOSS: 0.15271469950675964 / Q: {'Eval-Q': 1.346828, 'Target-Q': 1.278119}\n",
            "[*] LOSS: 0.1987430304288864 / Q: {'Eval-Q': 1.512553, 'Target-Q': 1.310121}\n",
            "[*] LOSS: 0.06523401290178299 / Q: {'Eval-Q': 1.542961, 'Target-Q': 1.556567}\n",
            "[*] LOSS: 0.07433641701936722 / Q: {'Eval-Q': 1.356288, 'Target-Q': 1.316199}\n",
            "[*] LOSS: 0.08251480013132095 / Q: {'Eval-Q': 0.990012, 'Target-Q': 0.917347}\n",
            "[*] LOSS: 0.064101442694664 / Q: {'Eval-Q': 1.197729, 'Target-Q': 1.190326}\n",
            "[*] LOSS: 0.02540038898587227 / Q: {'Eval-Q': 1.010646, 'Target-Q': 1.015857}\n",
            "[*] LOSS: 0.025387074798345566 / Q: {'Eval-Q': 0.955983, 'Target-Q': 0.958871}\n",
            "[*] LOSS: 0.058397963643074036 / Q: {'Eval-Q': 1.304637, 'Target-Q': 1.332295}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014082518404761248, 'total_reward': 308.01499979384243, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1837, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.199184, 0.126842]), 'NUM': [49, 38]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6239\n",
            "[*] LOSS: 0.08807373046875 / Q: {'Eval-Q': 1.042068, 'Target-Q': 1.053527}\n",
            "[*] LOSS: 0.11399286985397339 / Q: {'Eval-Q': 1.629806, 'Target-Q': 1.413942}\n",
            "[*] LOSS: 0.05732960253953934 / Q: {'Eval-Q': 1.078522, 'Target-Q': 1.023933}\n",
            "[*] LOSS: 0.11944840103387833 / Q: {'Eval-Q': 1.304256, 'Target-Q': 1.30883}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05944780978006878, 'total_reward': 373.5749982446432, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1838, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.314891, 0.424853]), 'NUM': [46, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.162857, -0.004   ]), 'NUM': [28, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [28, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13538\n",
            "[*] LOSS: 0.12057305127382278 / Q: {'Eval-Q': 1.265615, 'Target-Q': 1.253228}\n",
            "[*] LOSS: 0.08986169099807739 / Q: {'Eval-Q': 1.331324, 'Target-Q': 1.267966}\n",
            "[*] LOSS: 0.18344548344612122 / Q: {'Eval-Q': 1.0303, 'Target-Q': 1.114522}\n",
            "[*] LOSS: 0.2833908200263977 / Q: {'Eval-Q': 0.872607, 'Target-Q': 0.804406}\n",
            "[*] LOSS: 0.4140978455543518 / Q: {'Eval-Q': 1.041054, 'Target-Q': 1.052167}\n",
            "[*] LOSS: 0.263933926820755 / Q: {'Eval-Q': 1.218841, 'Target-Q': 1.144002}\n",
            "[*] LOSS: 0.27349618077278137 / Q: {'Eval-Q': 1.505245, 'Target-Q': 1.465329}\n",
            "[*] LOSS: 0.11933183670043945 / Q: {'Eval-Q': 1.332953, 'Target-Q': 1.326776}\n",
            "[*] LOSS: 0.13771705329418182 / Q: {'Eval-Q': 1.032538, 'Target-Q': 1.078342}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.016234072662901024, 'total_reward': 336.2749990792945, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1839, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.100755, 0.116548]), 'NUM': [53, 42]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [39, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 17479\n",
            "[*] LOSS: 0.08203490078449249 / Q: {'Eval-Q': 1.341441, 'Target-Q': 1.222715}\n",
            "[*] LOSS: 0.09178853780031204 / Q: {'Eval-Q': 1.518308, 'Target-Q': 1.480092}\n",
            "[*] LOSS: 0.05174852907657623 / Q: {'Eval-Q': 1.031057, 'Target-Q': 1.020646}\n",
            "[*] LOSS: 0.13337591290473938 / Q: {'Eval-Q': 1.684093, 'Target-Q': 1.713898}\n",
            "[*] LOSS: 0.050016868859529495 / Q: {'Eval-Q': 1.330165, 'Target-Q': 1.330008}\n",
            "[*] LOSS: 0.13724559545516968 / Q: {'Eval-Q': 0.901559, 'Target-Q': 0.96442}\n",
            "[*] LOSS: 0.20638960599899292 / Q: {'Eval-Q': 1.36907, 'Target-Q': 1.332203}\n",
            "[*] LOSS: 0.14475221931934357 / Q: {'Eval-Q': 1.111677, 'Target-Q': 1.130027}\n",
            "[*] LOSS: 0.040844544768333435 / Q: {'Eval-Q': 0.756043, 'Target-Q': 0.737605}\n",
            "[*] LOSS: 0.12119386345148087 / Q: {'Eval-Q': 0.913268, 'Target-Q': 0.951562}\n",
            "[*] LOSS: 0.06521175056695938 / Q: {'Eval-Q': 1.167676, 'Target-Q': 1.16386}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014004377931768347, 'total_reward': 303.714999852702, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1840.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1840, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.09161 , -0.002766]), 'NUM': [59, 47]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.003039,  0.004091]), 'NUM': [51, 22]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.007222,  0.015   ]), 'NUM': [45, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [45, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11292\n",
            "[*] LOSS: 0.062071867287158966 / Q: {'Eval-Q': 1.204498, 'Target-Q': 1.257863}\n",
            "[*] LOSS: 0.09132838249206543 / Q: {'Eval-Q': 0.912498, 'Target-Q': 0.828892}\n",
            "[*] LOSS: 0.25227445363998413 / Q: {'Eval-Q': 1.123318, 'Target-Q': 1.120041}\n",
            "[*] LOSS: 0.13524608314037323 / Q: {'Eval-Q': 1.07004, 'Target-Q': 1.004562}\n",
            "[*] LOSS: 0.09010085463523865 / Q: {'Eval-Q': 1.035271, 'Target-Q': 1.05585}\n",
            "[*] LOSS: 0.09705234318971634 / Q: {'Eval-Q': 1.235838, 'Target-Q': 1.279108}\n",
            "[*] LOSS: 0.05465647578239441 / Q: {'Eval-Q': 1.103369, 'Target-Q': 1.032455}\n",
            "[*] LOSS: 0.1405169814825058 / Q: {'Eval-Q': 1.070696, 'Target-Q': 0.91964}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.029828648322263767, 'total_reward': 351.3199988603592, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1841, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.170089, 0.128611]), 'NUM': [56, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007,  0.005]), 'NUM': [50, 10]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10803\n",
            "[*] LOSS: 0.1630192995071411 / Q: {'Eval-Q': 1.003928, 'Target-Q': 0.952562}\n",
            "[*] LOSS: 0.28393039107322693 / Q: {'Eval-Q': 1.753444, 'Target-Q': 1.70481}\n",
            "[*] LOSS: 0.4073808491230011 / Q: {'Eval-Q': 1.465374, 'Target-Q': 1.582594}\n",
            "[*] LOSS: 0.09208759665489197 / Q: {'Eval-Q': 0.963005, 'Target-Q': 0.863245}\n",
            "[*] LOSS: 0.12350937724113464 / Q: {'Eval-Q': 1.648894, 'Target-Q': 1.658342}\n",
            "[*] LOSS: 0.19413702189922333 / Q: {'Eval-Q': 1.140431, 'Target-Q': 1.17566}\n",
            "[*] LOSS: 0.1215314194560051 / Q: {'Eval-Q': 0.731992, 'Target-Q': 0.690738}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03138804235800945, 'total_reward': 359.46499879844487, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1842, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.193148, -0.004773]), 'NUM': [54, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001154, -0.005   ]), 'NUM': [52, 23]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.003039, -0.005   ]), 'NUM': [51, 13]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [51, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 21625\n",
            "[*] LOSS: 0.06500624120235443 / Q: {'Eval-Q': 1.030513, 'Target-Q': 0.906799}\n",
            "[*] LOSS: 0.18035052716732025 / Q: {'Eval-Q': 1.314754, 'Target-Q': 1.263697}\n",
            "[*] LOSS: 0.25195062160491943 / Q: {'Eval-Q': 1.281089, 'Target-Q': 1.297898}\n",
            "[*] LOSS: 0.06401720643043518 / Q: {'Eval-Q': 1.286739, 'Target-Q': 1.271363}\n",
            "[*] LOSS: 0.47524428367614746 / Q: {'Eval-Q': 1.256247, 'Target-Q': 1.280803}\n",
            "[*] LOSS: 0.18796993792057037 / Q: {'Eval-Q': 1.34688, 'Target-Q': 1.43043}\n",
            "[*] LOSS: 0.10770125687122345 / Q: {'Eval-Q': 1.654414, 'Target-Q': 1.64293}\n",
            "[*] LOSS: 0.1614721715450287 / Q: {'Eval-Q': 1.311896, 'Target-Q': 1.393694}\n",
            "[*] LOSS: 0.11625280976295471 / Q: {'Eval-Q': 1.223904, 'Target-Q': 1.263222}\n",
            "[*] LOSS: 0.047391194850206375 / Q: {'Eval-Q': 1.352055, 'Target-Q': 1.193941}\n",
            "[*] LOSS: 0.11497768759727478 / Q: {'Eval-Q': 1.228622, 'Target-Q': 1.143879}\n",
            "[*] LOSS: 0.07010164856910706 / Q: {'Eval-Q': 1.208123, 'Target-Q': 1.1213}\n",
            "[*] LOSS: 0.14971524477005005 / Q: {'Eval-Q': 1.09752, 'Target-Q': 1.172919}\n",
            "[*] LOSS: 0.165156289935112 / Q: {'Eval-Q': 1.498206, 'Target-Q': 1.242191}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012822974036589664, 'total_reward': 302.82500014733523, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1843, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.078871, -0.000341]), 'NUM': [62, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [59, 32]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.079746, -0.014286]), 'NUM': [59, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.006667,  0.328333]), 'NUM': [57, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [54, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [54, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.003148, -0.005   ]), 'NUM': [54, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 21124\n",
            "[*] LOSS: 0.23647645115852356 / Q: {'Eval-Q': 1.318143, 'Target-Q': 1.223033}\n",
            "[*] LOSS: 0.05829063057899475 / Q: {'Eval-Q': 1.123182, 'Target-Q': 1.105012}\n",
            "[*] LOSS: 0.12207022309303284 / Q: {'Eval-Q': 1.209072, 'Target-Q': 1.15129}\n",
            "[*] LOSS: 0.05515701323747635 / Q: {'Eval-Q': 0.93871, 'Target-Q': 0.966403}\n",
            "[*] LOSS: 0.08353296667337418 / Q: {'Eval-Q': 0.967724, 'Target-Q': 0.803045}\n",
            "[*] LOSS: 0.027046505361795425 / Q: {'Eval-Q': 1.034248, 'Target-Q': 1.030562}\n",
            "[*] LOSS: 0.07659967243671417 / Q: {'Eval-Q': 0.977319, 'Target-Q': 0.987993}\n",
            "[*] LOSS: 0.08591965585947037 / Q: {'Eval-Q': 1.32562, 'Target-Q': 1.304179}\n",
            "[*] LOSS: 0.06331473588943481 / Q: {'Eval-Q': 0.95597, 'Target-Q': 0.820339}\n",
            "[*] LOSS: 0.15228843688964844 / Q: {'Eval-Q': 0.945402, 'Target-Q': 0.986277}\n",
            "[*] LOSS: 0.13889554142951965 / Q: {'Eval-Q': 1.247836, 'Target-Q': 1.251517}\n",
            "[*] LOSS: 0.19419871270656586 / Q: {'Eval-Q': 1.071342, 'Target-Q': 1.072621}\n",
            "[*] LOSS: 0.17553499341011047 / Q: {'Eval-Q': 1.022658, 'Target-Q': 0.982239}\n",
            "[*] LOSS: 0.046815600246191025 / Q: {'Eval-Q': 1.1517, 'Target-Q': 1.177467}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.013721668678718846, 'total_reward': 305.81499982532114, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1844, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.079746, -0.01027 ]), 'NUM': [59, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [55, 24]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.093039, -0.010278]), 'NUM': [51, 18]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.011667]), 'NUM': [48, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.007083, -0.005   ]), 'NUM': [48, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [47, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 21142\n",
            "[*] LOSS: 0.13634580373764038 / Q: {'Eval-Q': 0.846386, 'Target-Q': 0.970392}\n",
            "[*] LOSS: 0.031532738357782364 / Q: {'Eval-Q': 0.698459, 'Target-Q': 0.685283}\n",
            "[*] LOSS: 0.09991252422332764 / Q: {'Eval-Q': 0.872653, 'Target-Q': 0.831531}\n",
            "[*] LOSS: 0.13150396943092346 / Q: {'Eval-Q': 0.701411, 'Target-Q': 0.785752}\n",
            "[*] LOSS: 0.08774492889642715 / Q: {'Eval-Q': 0.989197, 'Target-Q': 0.944947}\n",
            "[*] LOSS: 0.06610359251499176 / Q: {'Eval-Q': 0.817336, 'Target-Q': 0.792569}\n",
            "[*] LOSS: 0.21999216079711914 / Q: {'Eval-Q': 0.704044, 'Target-Q': 0.791031}\n",
            "[*] LOSS: 0.04417729377746582 / Q: {'Eval-Q': 0.80879, 'Target-Q': 0.793727}\n",
            "[*] LOSS: 0.025679798796772957 / Q: {'Eval-Q': 0.719909, 'Target-Q': 0.691053}\n",
            "[*] LOSS: 0.12296025454998016 / Q: {'Eval-Q': 0.955465, 'Target-Q': 1.045431}\n",
            "[*] LOSS: 0.14693164825439453 / Q: {'Eval-Q': 1.019982, 'Target-Q': 1.07069}\n",
            "[*] LOSS: 0.22564709186553955 / Q: {'Eval-Q': 0.875202, 'Target-Q': 0.973446}\n",
            "[*] LOSS: 0.10140502452850342 / Q: {'Eval-Q': 1.043286, 'Target-Q': 1.102289}\n",
            "[*] LOSS: 0.009794875048100948 / Q: {'Eval-Q': 0.668047, 'Target-Q': 0.693263}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012594483578105591, 'total_reward': 308.3599999686703, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1845, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.148203, 0.131389]), 'NUM': [64, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [57, 23]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.000745,  0.005   ]), 'NUM': [47, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13553\n",
            "[*] LOSS: 0.24568870663642883 / Q: {'Eval-Q': 1.08301, 'Target-Q': 1.102761}\n",
            "[*] LOSS: 0.06611891090869904 / Q: {'Eval-Q': 0.800015, 'Target-Q': 0.776983}\n",
            "[*] LOSS: 0.0585453137755394 / Q: {'Eval-Q': 0.686609, 'Target-Q': 0.68062}\n",
            "[*] LOSS: 0.2574942409992218 / Q: {'Eval-Q': 0.958483, 'Target-Q': 0.967618}\n",
            "[*] LOSS: 0.02062813565135002 / Q: {'Eval-Q': 0.66366, 'Target-Q': 0.594356}\n",
            "[*] LOSS: 0.1386684775352478 / Q: {'Eval-Q': 0.756141, 'Target-Q': 0.721844}\n",
            "[*] LOSS: 0.09141169488430023 / Q: {'Eval-Q': 0.797571, 'Target-Q': 0.783044}\n",
            "[*] LOSS: 0.028347456827759743 / Q: {'Eval-Q': 1.185781, 'Target-Q': 1.120313}\n",
            "[*] LOSS: 0.03605801984667778 / Q: {'Eval-Q': 0.608661, 'Target-Q': 0.6078}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021709691800892494, 'total_reward': 340.61999822501093, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1846, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.078871, -0.004857]), 'NUM': [62, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.009286]), 'NUM': [62, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.003305, -0.005   ]), 'NUM': [59, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10121\n",
            "[*] LOSS: 0.28037065267562866 / Q: {'Eval-Q': 0.838815, 'Target-Q': 0.945483}\n",
            "[*] LOSS: 0.02449321374297142 / Q: {'Eval-Q': 0.655009, 'Target-Q': 0.696259}\n",
            "[*] LOSS: 0.03736919164657593 / Q: {'Eval-Q': 0.851398, 'Target-Q': 0.833608}\n",
            "[*] LOSS: 0.01611291617155075 / Q: {'Eval-Q': 0.796256, 'Target-Q': 0.812967}\n",
            "[*] LOSS: 0.05495447292923927 / Q: {'Eval-Q': 0.772932, 'Target-Q': 0.828907}\n",
            "[*] LOSS: 0.11602417379617691 / Q: {'Eval-Q': 0.974197, 'Target-Q': 1.052202}\n",
            "[*] LOSS: 0.03177526593208313 / Q: {'Eval-Q': 0.866754, 'Target-Q': 0.799339}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03557035545909051, 'total_reward': 361.2049984484911, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1847, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.212778, -0.006875]), 'NUM': [45, 48]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 43]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 43]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 41]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 41]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 41]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 41]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 41]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 17860\n",
            "[*] LOSS: 0.09871164709329605 / Q: {'Eval-Q': 1.147516, 'Target-Q': 1.126331}\n",
            "[*] LOSS: 0.18343639373779297 / Q: {'Eval-Q': 0.690956, 'Target-Q': 0.74268}\n",
            "[*] LOSS: 0.018776703625917435 / Q: {'Eval-Q': 0.620972, 'Target-Q': 0.64641}\n",
            "[*] LOSS: 0.06713765859603882 / Q: {'Eval-Q': 0.701181, 'Target-Q': 0.724291}\n",
            "[*] LOSS: 0.26838791370391846 / Q: {'Eval-Q': 0.744173, 'Target-Q': 0.813834}\n",
            "[*] LOSS: 0.02036314085125923 / Q: {'Eval-Q': 0.629203, 'Target-Q': 0.618284}\n",
            "[*] LOSS: 0.180504709482193 / Q: {'Eval-Q': 0.719986, 'Target-Q': 0.788942}\n",
            "[*] LOSS: 0.09431757032871246 / Q: {'Eval-Q': 0.919477, 'Target-Q': 0.85835}\n",
            "[*] LOSS: 0.02423826977610588 / Q: {'Eval-Q': 1.155197, 'Target-Q': 1.133318}\n",
            "[*] LOSS: 0.08969105780124664 / Q: {'Eval-Q': 0.93396, 'Target-Q': 0.913869}\n",
            "[*] LOSS: 0.10749001055955887 / Q: {'Eval-Q': 0.858203, 'Target-Q': 0.850623}\n",
            "[*] LOSS: 0.022321544587612152 / Q: {'Eval-Q': 0.527144, 'Target-Q': 0.502625}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0034949194798688543, 'total_reward': 107.50000061281025, 'kill': 40}\n",
            "\n",
            "\n",
            "[*] ROUND #1848, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.081207, 0.000694]), 'NUM': [58, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.006887, -0.001296]), 'NUM': [53, 27]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.002826, -0.005   ]), 'NUM': [46, 8]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 19883\n",
            "[*] LOSS: 0.019627943634986877 / Q: {'Eval-Q': 0.718671, 'Target-Q': 0.69683}\n",
            "[*] LOSS: 0.037145357578992844 / Q: {'Eval-Q': 0.896889, 'Target-Q': 0.910531}\n",
            "[*] LOSS: 0.12128475308418274 / Q: {'Eval-Q': 0.807685, 'Target-Q': 0.797503}\n",
            "[*] LOSS: 0.044432707130908966 / Q: {'Eval-Q': 0.787457, 'Target-Q': 0.822115}\n",
            "[*] LOSS: 0.08989004045724869 / Q: {'Eval-Q': 0.936241, 'Target-Q': 0.918617}\n",
            "[*] LOSS: 0.34696707129478455 / Q: {'Eval-Q': 0.866111, 'Target-Q': 0.904694}\n",
            "[*] LOSS: 0.01231461949646473 / Q: {'Eval-Q': 0.500251, 'Target-Q': 0.512573}\n",
            "[*] LOSS: 0.021623333916068077 / Q: {'Eval-Q': 0.574208, 'Target-Q': 0.49807}\n",
            "[*] LOSS: 0.05086344853043556 / Q: {'Eval-Q': 0.910414, 'Target-Q': 0.902393}\n",
            "[*] LOSS: 0.028310034424066544 / Q: {'Eval-Q': 0.648641, 'Target-Q': 0.617678}\n",
            "[*] LOSS: 0.5263870358467102 / Q: {'Eval-Q': 0.631152, 'Target-Q': 0.765071}\n",
            "[*] LOSS: 0.20759205520153046 / Q: {'Eval-Q': 0.728454, 'Target-Q': 0.810757}\n",
            "[*] LOSS: 0.08130468428134918 / Q: {'Eval-Q': 0.618127, 'Target-Q': 0.633586}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012582460783963833, 'total_reward': 306.9699997520074, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1849, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.004615, -0.007273]), 'NUM': [52, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007439, -0.009545]), 'NUM': [41, 22]}\n",
            "> step #150, info: {'Ave-Reward': array([ 0.000263, -0.005   ]), 'NUM': [38, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7904\n",
            "[*] LOSS: 0.06293877214193344 / Q: {'Eval-Q': 0.696835, 'Target-Q': 0.645339}\n",
            "[*] LOSS: 0.20199166238307953 / Q: {'Eval-Q': 0.936776, 'Target-Q': 0.948336}\n",
            "[*] LOSS: 0.1436714380979538 / Q: {'Eval-Q': 0.96415, 'Target-Q': 1.005615}\n",
            "[*] LOSS: 0.07633113116025925 / Q: {'Eval-Q': 0.75275, 'Target-Q': 0.687187}\n",
            "[*] LOSS: 0.023683123290538788 / Q: {'Eval-Q': 0.907781, 'Target-Q': 0.918544}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04937827244773949, 'total_reward': 376.794997937046, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1850, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.005169, -0.000556]), 'NUM': [59, 45]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001226, -0.019286]), 'NUM': [53, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.003077,  0.095   ]), 'NUM': [52, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9134\n",
            "[*] LOSS: 0.17991286516189575 / Q: {'Eval-Q': 0.955827, 'Target-Q': 0.797862}\n",
            "[*] LOSS: 0.14277973771095276 / Q: {'Eval-Q': 0.737781, 'Target-Q': 0.662785}\n",
            "[*] LOSS: 0.10161620378494263 / Q: {'Eval-Q': 0.692957, 'Target-Q': 0.689458}\n",
            "[*] LOSS: 0.03767593577504158 / Q: {'Eval-Q': 0.812489, 'Target-Q': 0.805131}\n",
            "[*] LOSS: 0.3369595408439636 / Q: {'Eval-Q': 0.531883, 'Target-Q': 0.598702}\n",
            "[*] LOSS: 0.3490777909755707 / Q: {'Eval-Q': 0.928865, 'Target-Q': 1.000763}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.041503304229783926, 'total_reward': 376.07499856874347, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1851, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.157984, 0.099565]), 'NUM': [62, 46]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001296, -0.005   ]), 'NUM': [54, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9186\n",
            "[*] LOSS: 0.24599768221378326 / Q: {'Eval-Q': 1.170487, 'Target-Q': 1.187499}\n",
            "[*] LOSS: 0.07286369800567627 / Q: {'Eval-Q': 0.919677, 'Target-Q': 0.823111}\n",
            "[*] LOSS: 0.13091863691806793 / Q: {'Eval-Q': 1.056154, 'Target-Q': 1.134794}\n",
            "[*] LOSS: 0.05554112792015076 / Q: {'Eval-Q': 0.671737, 'Target-Q': 0.718674}\n",
            "[*] LOSS: 0.19835877418518066 / Q: {'Eval-Q': 0.854392, 'Target-Q': 0.952565}\n",
            "[*] LOSS: 0.15589025616645813 / Q: {'Eval-Q': 0.873872, 'Target-Q': 0.878615}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04110674155962233, 'total_reward': 374.10499872546643, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1852, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.167213, 0.107979]), 'NUM': [61, 47]}\n",
            "> step #100, info: {'Ave-Reward': array([0.001122, 0.011667]), 'NUM': [49, 6]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.002917, -0.005   ]), 'NUM': [48, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 11269\n",
            "[*] LOSS: 0.030721331015229225 / Q: {'Eval-Q': 0.79241, 'Target-Q': 0.781068}\n",
            "[*] LOSS: 0.07089132815599442 / Q: {'Eval-Q': 0.858134, 'Target-Q': 0.845043}\n",
            "[*] LOSS: 0.019049322232604027 / Q: {'Eval-Q': 0.810725, 'Target-Q': 0.770333}\n",
            "[*] LOSS: 0.10552796721458435 / Q: {'Eval-Q': 1.014882, 'Target-Q': 1.065982}\n",
            "[*] LOSS: 0.09483291953802109 / Q: {'Eval-Q': 0.512714, 'Target-Q': 0.531864}\n",
            "[*] LOSS: 0.026685232296586037 / Q: {'Eval-Q': 0.686869, 'Target-Q': 0.657833}\n",
            "[*] LOSS: 0.1218767762184143 / Q: {'Eval-Q': 1.021884, 'Target-Q': 1.032848}\n",
            "[*] LOSS: 0.12351468205451965 / Q: {'Eval-Q': 1.036523, 'Target-Q': 1.008853}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02980875004983645, 'total_reward': 348.5199990104884, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1853, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.180804, 0.112317]), 'NUM': [56, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.095   , -0.004643]), 'NUM': [50, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8115\n",
            "[*] LOSS: 0.1630067378282547 / Q: {'Eval-Q': 0.819246, 'Target-Q': 0.873089}\n",
            "[*] LOSS: 0.09968461096286774 / Q: {'Eval-Q': 1.176854, 'Target-Q': 1.158321}\n",
            "[*] LOSS: 0.11047856509685516 / Q: {'Eval-Q': 1.073289, 'Target-Q': 1.059902}\n",
            "[*] LOSS: 0.14442339539527893 / Q: {'Eval-Q': 1.285347, 'Target-Q': 1.306526}\n",
            "[*] LOSS: 0.15125074982643127 / Q: {'Eval-Q': 0.991914, 'Target-Q': 1.007302}\n",
            "[*] LOSS: 0.10374245047569275 / Q: {'Eval-Q': 0.741081, 'Target-Q': 0.797417}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.046920172561008985, 'total_reward': 377.39499846752733, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1854, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.187255, 0.109878]), 'NUM': [51, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [45, 24]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [44, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [43, 20]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [43, 18]}\n",
            "> step #300, info: {'Ave-Reward': array([0.001977, 0.000556]), 'NUM': [43, 18]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [37, 14]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 18332\n",
            "[*] LOSS: 0.03518727049231529 / Q: {'Eval-Q': 0.98568, 'Target-Q': 0.965629}\n",
            "[*] LOSS: 0.22739799320697784 / Q: {'Eval-Q': 1.145813, 'Target-Q': 1.065697}\n",
            "[*] LOSS: 0.2248663306236267 / Q: {'Eval-Q': 0.779417, 'Target-Q': 0.861352}\n",
            "[*] LOSS: 0.10743696242570877 / Q: {'Eval-Q': 1.338492, 'Target-Q': 1.367926}\n",
            "[*] LOSS: 0.06298507750034332 / Q: {'Eval-Q': 1.000299, 'Target-Q': 1.031885}\n",
            "[*] LOSS: 0.17503325641155243 / Q: {'Eval-Q': 0.78838, 'Target-Q': 0.877074}\n",
            "[*] LOSS: 0.4100326895713806 / Q: {'Eval-Q': 1.071453, 'Target-Q': 1.087349}\n",
            "[*] LOSS: 0.19846685230731964 / Q: {'Eval-Q': 0.885566, 'Target-Q': 0.936193}\n",
            "[*] LOSS: 0.10378057509660721 / Q: {'Eval-Q': 0.64259, 'Target-Q': 0.59899}\n",
            "[*] LOSS: 0.08650784194469452 / Q: {'Eval-Q': 0.989354, 'Target-Q': 0.970438}\n",
            "[*] LOSS: 0.036788277328014374 / Q: {'Eval-Q': 0.915964, 'Target-Q': 0.726499}\n",
            "[*] LOSS: 0.08163361251354218 / Q: {'Eval-Q': 0.878649, 'Target-Q': 0.862379}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.009473903567403342, 'total_reward': 226.36500031128526, 'kill': 67}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1855, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.004804, -0.003113]), 'NUM': [51, 53]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.002959, -0.005   ]), 'NUM': [49, 27]}\n",
            "> step #150, info: {'Ave-Reward': array([0.001122, 0.028333]), 'NUM': [49, 9]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005   ,  0.011667]), 'NUM': [49, 6]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 20577\n",
            "[*] LOSS: 0.07421831041574478 / Q: {'Eval-Q': 1.049583, 'Target-Q': 0.995436}\n",
            "[*] LOSS: 0.08339795470237732 / Q: {'Eval-Q': 1.087538, 'Target-Q': 1.015538}\n",
            "[*] LOSS: 0.04482046887278557 / Q: {'Eval-Q': 1.167836, 'Target-Q': 0.970192}\n",
            "[*] LOSS: 0.06461462378501892 / Q: {'Eval-Q': 0.731386, 'Target-Q': 0.737768}\n",
            "[*] LOSS: 0.060110609978437424 / Q: {'Eval-Q': 0.967391, 'Target-Q': 0.967433}\n",
            "[*] LOSS: 0.04975054785609245 / Q: {'Eval-Q': 0.777497, 'Target-Q': 0.719185}\n",
            "[*] LOSS: 0.1511823683977127 / Q: {'Eval-Q': 1.010372, 'Target-Q': 0.90905}\n",
            "[*] LOSS: 0.07606834173202515 / Q: {'Eval-Q': 1.11701, 'Target-Q': 1.100792}\n",
            "[*] LOSS: 0.03467146307229996 / Q: {'Eval-Q': 0.498547, 'Target-Q': 0.466205}\n",
            "[*] LOSS: 0.06071058660745621 / Q: {'Eval-Q': 0.892082, 'Target-Q': 0.943664}\n",
            "[*] LOSS: 0.26293855905532837 / Q: {'Eval-Q': 0.92824, 'Target-Q': 0.936401}\n",
            "[*] LOSS: 0.04817520081996918 / Q: {'Eval-Q': 1.144303, 'Target-Q': 1.148415}\n",
            "[*] LOSS: 0.2618733048439026 / Q: {'Eval-Q': 1.49975, 'Target-Q': 1.409005}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.013066856548258067, 'total_reward': 280.4800001466647, 'kill': 76}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1856, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.16569 , -0.016143]), 'NUM': [58, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12765\n",
            "[*] LOSS: 0.025597751140594482 / Q: {'Eval-Q': 0.536232, 'Target-Q': 0.558347}\n",
            "[*] LOSS: 0.054650332778692245 / Q: {'Eval-Q': 0.800062, 'Target-Q': 0.808387}\n",
            "[*] LOSS: 0.026270637288689613 / Q: {'Eval-Q': 0.570061, 'Target-Q': 0.5754}\n",
            "[*] LOSS: 0.0689334124326706 / Q: {'Eval-Q': 1.133394, 'Target-Q': 0.912875}\n",
            "[*] LOSS: 0.18279071152210236 / Q: {'Eval-Q': 0.921178, 'Target-Q': 0.825513}\n",
            "[*] LOSS: 0.6123032569885254 / Q: {'Eval-Q': 1.085086, 'Target-Q': 1.053345}\n",
            "[*] LOSS: 0.03874427080154419 / Q: {'Eval-Q': 0.753615, 'Target-Q': 0.786417}\n",
            "[*] LOSS: 0.04974735900759697 / Q: {'Eval-Q': 0.993295, 'Target-Q': 0.992106}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02426736946992101, 'total_reward': 339.0299988295883, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1857, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.000172, 0.007821]), 'NUM': [58, 39]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007083, -0.005   ]), 'NUM': [48, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9227\n",
            "[*] LOSS: 0.036670077592134476 / Q: {'Eval-Q': 0.89345, 'Target-Q': 0.888934}\n",
            "[*] LOSS: 0.12848111987113953 / Q: {'Eval-Q': 0.904612, 'Target-Q': 0.950265}\n",
            "[*] LOSS: 0.06100309267640114 / Q: {'Eval-Q': 0.624877, 'Target-Q': 0.624002}\n",
            "[*] LOSS: 0.17859235405921936 / Q: {'Eval-Q': 0.814491, 'Target-Q': 0.822938}\n",
            "[*] LOSS: 0.1251184493303299 / Q: {'Eval-Q': 0.762485, 'Target-Q': 0.735805}\n",
            "[*] LOSS: 0.043392352759838104 / Q: {'Eval-Q': 0.646061, 'Target-Q': 0.658068}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03800075872143984, 'total_reward': 362.734998492524, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1858, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.136026, 0.007625]), 'NUM': [39, 40]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001875, -0.005   ]), 'NUM': [32, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8377\n",
            "[*] LOSS: 0.08984576165676117 / Q: {'Eval-Q': 0.987357, 'Target-Q': 0.941348}\n",
            "[*] LOSS: 0.07712490856647491 / Q: {'Eval-Q': 0.698304, 'Target-Q': 0.722571}\n",
            "[*] LOSS: 0.1172981709241867 / Q: {'Eval-Q': 0.93572, 'Target-Q': 0.912928}\n",
            "[*] LOSS: 0.26575183868408203 / Q: {'Eval-Q': 0.885926, 'Target-Q': 0.78127}\n",
            "[*] LOSS: 0.2763301730155945 / Q: {'Eval-Q': 0.936182, 'Target-Q': 0.883196}\n",
            "[*] LOSS: 0.06773858517408371 / Q: {'Eval-Q': 0.846967, 'Target-Q': 0.809524}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04329607931633009, 'total_reward': 373.46999821439385, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1859, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.087857, -0.002614]), 'NUM': [56, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007222, -0.0175  ]), 'NUM': [45, 16]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.009444, -0.035   ]), 'NUM': [45, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007273, -0.105   ]), 'NUM': [44, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.009545, -0.105   ]), 'NUM': [44, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007273, -0.105   ]), 'NUM': [44, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.009545, -0.105   ]), 'NUM': [44, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.007273, -0.105   ]), 'NUM': [44, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 19455\n",
            "[*] LOSS: 0.18709324300289154 / Q: {'Eval-Q': 1.033628, 'Target-Q': 0.854105}\n",
            "[*] LOSS: 0.22041332721710205 / Q: {'Eval-Q': 1.13904, 'Target-Q': 0.929009}\n",
            "[*] LOSS: 0.033664241433143616 / Q: {'Eval-Q': 0.593978, 'Target-Q': 0.594218}\n",
            "[*] LOSS: 0.15341943502426147 / Q: {'Eval-Q': 0.731016, 'Target-Q': 0.755251}\n",
            "[*] LOSS: 0.1112419068813324 / Q: {'Eval-Q': 0.833892, 'Target-Q': 0.860578}\n",
            "[*] LOSS: 0.32396581768989563 / Q: {'Eval-Q': 0.909674, 'Target-Q': 0.977973}\n",
            "[*] LOSS: 0.13581478595733643 / Q: {'Eval-Q': 0.843859, 'Target-Q': 0.751685}\n",
            "[*] LOSS: 0.049675047397613525 / Q: {'Eval-Q': 0.709923, 'Target-Q': 0.666191}\n",
            "[*] LOSS: 0.04820118471980095 / Q: {'Eval-Q': 0.840406, 'Target-Q': 0.80221}\n",
            "[*] LOSS: 0.06788984686136246 / Q: {'Eval-Q': 0.744037, 'Target-Q': 0.756591}\n",
            "[*] LOSS: 0.1836855709552765 / Q: {'Eval-Q': 0.830144, 'Target-Q': 0.854498}\n",
            "[*] LOSS: 0.13914640247821808 / Q: {'Eval-Q': 0.609355, 'Target-Q': 0.519005}\n",
            "[*] LOSS: 0.09185048192739487 / Q: {'Eval-Q': 0.942004, 'Target-Q': 0.794643}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.00966020842681301, 'total_reward': 246.5099977767095, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1860.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1860, EPS: 0.14 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.078929, -0.01027 ]), 'NUM': [56, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.001667]), 'NUM': [49, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007128, -0.005   ]), 'NUM': [47, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12453\n",
            "[*] LOSS: 0.10343124717473984 / Q: {'Eval-Q': 0.962303, 'Target-Q': 0.880674}\n",
            "[*] LOSS: 0.330423504114151 / Q: {'Eval-Q': 0.786058, 'Target-Q': 0.835537}\n",
            "[*] LOSS: 0.5525870323181152 / Q: {'Eval-Q': 1.238949, 'Target-Q': 1.180571}\n",
            "[*] LOSS: 0.016135426238179207 / Q: {'Eval-Q': 0.532963, 'Target-Q': 0.515306}\n",
            "[*] LOSS: 0.07619141042232513 / Q: {'Eval-Q': 0.768918, 'Target-Q': 0.790277}\n",
            "[*] LOSS: 0.06117602810263634 / Q: {'Eval-Q': 0.728297, 'Target-Q': 0.675856}\n",
            "[*] LOSS: 0.05010949820280075 / Q: {'Eval-Q': 0.678793, 'Target-Q': 0.64687}\n",
            "[*] LOSS: 0.1057668924331665 / Q: {'Eval-Q': 0.731171, 'Target-Q': 0.776583}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02165262043576669, 'total_reward': 305.2049965932965, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1861, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.222273, 0.001327]), 'NUM': [44, 49]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001912,  0.187308]), 'NUM': [34, 26]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.001667,  0.002143]), 'NUM': [30, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [25, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9032\n",
            "[*] LOSS: 0.20579670369625092 / Q: {'Eval-Q': 1.118834, 'Target-Q': 1.135802}\n",
            "[*] LOSS: 0.11103411763906479 / Q: {'Eval-Q': 1.267798, 'Target-Q': 1.178482}\n",
            "[*] LOSS: 0.049655377864837646 / Q: {'Eval-Q': 0.954853, 'Target-Q': 0.962121}\n",
            "[*] LOSS: 0.1760217547416687 / Q: {'Eval-Q': 0.87963, 'Target-Q': 0.948733}\n",
            "[*] LOSS: 0.07791425287723541 / Q: {'Eval-Q': 0.947688, 'Target-Q': 0.921043}\n",
            "[*] LOSS: 0.2683795094490051 / Q: {'Eval-Q': 1.193809, 'Target-Q': 1.123323}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04209788739392789, 'total_reward': 370.8249985696748, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1862, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.006338,  0.097222]), 'NUM': [71, 45]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.065149, -0.004792]), 'NUM': [67, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10084\n",
            "[*] LOSS: 0.0653499960899353 / Q: {'Eval-Q': 1.206241, 'Target-Q': 1.043031}\n",
            "[*] LOSS: 0.40684545040130615 / Q: {'Eval-Q': 0.724561, 'Target-Q': 0.775432}\n",
            "[*] LOSS: 0.07496346533298492 / Q: {'Eval-Q': 0.932741, 'Target-Q': 0.697403}\n",
            "[*] LOSS: 0.186677485704422 / Q: {'Eval-Q': 1.35331, 'Target-Q': 1.290751}\n",
            "[*] LOSS: 0.005073842592537403 / Q: {'Eval-Q': 0.505332, 'Target-Q': 0.378992}\n",
            "[*] LOSS: 0.1215784102678299 / Q: {'Eval-Q': 0.97376, 'Target-Q': 0.887809}\n",
            "[*] LOSS: 0.3716590404510498 / Q: {'Eval-Q': 1.055525, 'Target-Q': 1.06872}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03314556465524432, 'total_reward': 332.564996750094, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1863, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.205417, -0.001944]), 'NUM': [48, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.228333, -0.016176]), 'NUM': [42, 17]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7761\n",
            "[*] LOSS: 0.2136181741952896 / Q: {'Eval-Q': 1.102791, 'Target-Q': 1.083944}\n",
            "[*] LOSS: 0.2141999751329422 / Q: {'Eval-Q': 1.224931, 'Target-Q': 1.115385}\n",
            "[*] LOSS: 0.2880822420120239 / Q: {'Eval-Q': 1.250768, 'Target-Q': 1.207196}\n",
            "[*] LOSS: 0.19502167403697968 / Q: {'Eval-Q': 1.091125, 'Target-Q': 1.092864}\n",
            "[*] LOSS: 0.34490305185317993 / Q: {'Eval-Q': 1.466071, 'Target-Q': 1.557042}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04672360260723731, 'total_reward': 364.20999832917005, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1864, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.080714, -0.004891]), 'NUM': [63, 46]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.083136, -0.015263]), 'NUM': [59, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [57, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [57, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.006754, -0.005   ]), 'NUM': [57, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 16846\n",
            "[*] LOSS: 0.04762060195207596 / Q: {'Eval-Q': 0.836164, 'Target-Q': 0.856872}\n",
            "[*] LOSS: 0.10095499455928802 / Q: {'Eval-Q': 1.078489, 'Target-Q': 1.026182}\n",
            "[*] LOSS: 0.08616762608289719 / Q: {'Eval-Q': 0.810115, 'Target-Q': 0.740935}\n",
            "[*] LOSS: 0.09309469908475876 / Q: {'Eval-Q': 1.063086, 'Target-Q': 1.066238}\n",
            "[*] LOSS: 0.142995685338974 / Q: {'Eval-Q': 0.99754, 'Target-Q': 1.045432}\n",
            "[*] LOSS: 0.0435311496257782 / Q: {'Eval-Q': 0.81614, 'Target-Q': 0.807458}\n",
            "[*] LOSS: 0.08581654727458954 / Q: {'Eval-Q': 0.635696, 'Target-Q': 0.615391}\n",
            "[*] LOSS: 0.06844943016767502 / Q: {'Eval-Q': 1.136064, 'Target-Q': 1.006805}\n",
            "[*] LOSS: 0.24109473824501038 / Q: {'Eval-Q': 1.219124, 'Target-Q': 1.1739}\n",
            "[*] LOSS: 0.10336212813854218 / Q: {'Eval-Q': 1.240868, 'Target-Q': 0.951597}\n",
            "[*] LOSS: 0.10416392982006073 / Q: {'Eval-Q': 1.022275, 'Target-Q': 1.087884}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019048349923188886, 'total_reward': 337.98999938275665, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1865, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.000172, 0.004091]), 'NUM': [58, 44]}\n",
            "> step #100, info: {'Ave-Reward': array([0.009388, 0.345   ]), 'NUM': [49, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7382\n",
            "[*] LOSS: 0.20353290438652039 / Q: {'Eval-Q': 1.005036, 'Target-Q': 1.000828}\n",
            "[*] LOSS: 0.15014608204364777 / Q: {'Eval-Q': 1.433748, 'Target-Q': 1.167311}\n",
            "[*] LOSS: 0.06283818185329437 / Q: {'Eval-Q': 1.006593, 'Target-Q': 0.768567}\n",
            "[*] LOSS: 0.03555687516927719 / Q: {'Eval-Q': 1.293254, 'Target-Q': 1.159551}\n",
            "[*] LOSS: 0.2470780909061432 / Q: {'Eval-Q': 0.971356, 'Target-Q': 0.953286}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05464100455294664, 'total_reward': 377.76999831106514, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1866, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.076481, -0.010735]), 'NUM': [54, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   , -0.016111]), 'NUM': [50, 18]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [48, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9993\n",
            "[*] LOSS: 0.09222661703824997 / Q: {'Eval-Q': 1.009534, 'Target-Q': 1.02809}\n",
            "[*] LOSS: 0.08639482408761978 / Q: {'Eval-Q': 1.115923, 'Target-Q': 1.172255}\n",
            "[*] LOSS: 0.30775749683380127 / Q: {'Eval-Q': 0.98945, 'Target-Q': 0.899904}\n",
            "[*] LOSS: 0.4774102568626404 / Q: {'Eval-Q': 1.214811, 'Target-Q': 0.923724}\n",
            "[*] LOSS: 0.25044137239456177 / Q: {'Eval-Q': 0.637924, 'Target-Q': 0.54289}\n",
            "[*] LOSS: 0.07737048715353012 / Q: {'Eval-Q': 1.260823, 'Target-Q': 1.233961}\n",
            "[*] LOSS: 0.09737058728933334 / Q: {'Eval-Q': 1.286496, 'Target-Q': 1.270608}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03473367027229455, 'total_reward': 358.30999832600355, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1867, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.197143, 0.002241]), 'NUM': [49, 29]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 8]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6460\n",
            "[*] LOSS: 0.3021208643913269 / Q: {'Eval-Q': 1.506657, 'Target-Q': 1.313241}\n",
            "[*] LOSS: 0.21601159870624542 / Q: {'Eval-Q': 1.374562, 'Target-Q': 1.430767}\n",
            "[*] LOSS: 0.02401767298579216 / Q: {'Eval-Q': 0.706319, 'Target-Q': 0.727495}\n",
            "[*] LOSS: 0.05603698268532753 / Q: {'Eval-Q': 0.782699, 'Target-Q': 0.834194}\n",
            "[*] LOSS: 0.20127961039543152 / Q: {'Eval-Q': 1.438707, 'Target-Q': 1.43394}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05976174897914227, 'total_reward': 364.6999983601272, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1868, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.2075  , -0.004722]), 'NUM': [48, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.133889, -0.00375 ]), 'NUM': [36, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5913\n",
            "[*] LOSS: 0.025967754423618317 / Q: {'Eval-Q': 1.158482, 'Target-Q': 0.922668}\n",
            "[*] LOSS: 0.1185566782951355 / Q: {'Eval-Q': 0.989611, 'Target-Q': 0.99552}\n",
            "[*] LOSS: 0.29835963249206543 / Q: {'Eval-Q': 1.323752, 'Target-Q': 1.324207}\n",
            "[*] LOSS: 0.07989886403083801 / Q: {'Eval-Q': 1.060668, 'Target-Q': 0.993991}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07159347710742502, 'total_reward': 391.2649981835857, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1869, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.406111, 0.003289]), 'NUM': [36, 38]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.008704, -0.012692]), 'NUM': [27, 13]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.015]), 'NUM': [23, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6677\n",
            "[*] LOSS: 0.18695636093616486 / Q: {'Eval-Q': 1.3236, 'Target-Q': 1.250854}\n",
            "[*] LOSS: 0.21458272635936737 / Q: {'Eval-Q': 1.463833, 'Target-Q': 1.440993}\n",
            "[*] LOSS: 0.4133860766887665 / Q: {'Eval-Q': 1.5726, 'Target-Q': 1.447942}\n",
            "[*] LOSS: 0.27308717370033264 / Q: {'Eval-Q': 1.952139, 'Target-Q': 1.613175}\n",
            "[*] LOSS: 0.17163261771202087 / Q: {'Eval-Q': 1.404214, 'Target-Q': 1.314198}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05843771964931625, 'total_reward': 370.1099980054423, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1870, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.343953, 0.130735]), 'NUM': [43, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.002059, -0.005   ]), 'NUM': [34, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5946\n",
            "[*] LOSS: 0.3076128661632538 / Q: {'Eval-Q': 1.281895, 'Target-Q': 1.321426}\n",
            "[*] LOSS: 0.3764885663986206 / Q: {'Eval-Q': 1.442596, 'Target-Q': 1.568084}\n",
            "[*] LOSS: 0.2923547029495239 / Q: {'Eval-Q': 1.408757, 'Target-Q': 1.433636}\n",
            "[*] LOSS: 0.12228953838348389 / Q: {'Eval-Q': 1.339607, 'Target-Q': 1.294943}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06901385588324993, 'total_reward': 373.3149981312454, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1871, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.1953, 0.5275]), 'NUM': [50, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.      , -0.021667]), 'NUM': [40, 6]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7080\n",
            "[*] LOSS: 0.2896118760108948 / Q: {'Eval-Q': 1.330645, 'Target-Q': 1.283094}\n",
            "[*] LOSS: 0.1693999171257019 / Q: {'Eval-Q': 1.351778, 'Target-Q': 1.412746}\n",
            "[*] LOSS: 0.2055252194404602 / Q: {'Eval-Q': 1.241397, 'Target-Q': 1.343176}\n",
            "[*] LOSS: 0.5605214238166809 / Q: {'Eval-Q': 1.499042, 'Target-Q': 1.46402}\n",
            "[*] LOSS: 0.3464376628398895 / Q: {'Eval-Q': 2.000837, 'Target-Q': 2.177822}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05171433525081581, 'total_reward': 371.614998402074, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1872, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.191481, 0.176406]), 'NUM': [27, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4119\n",
            "[*] LOSS: 0.2099064737558365 / Q: {'Eval-Q': 1.810608, 'Target-Q': 1.768989}\n",
            "[*] LOSS: 0.22795350849628448 / Q: {'Eval-Q': 1.472165, 'Target-Q': 1.52708}\n",
            "[*] LOSS: 0.5266035795211792 / Q: {'Eval-Q': 1.393122, 'Target-Q': 1.410514}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04912531851519472, 'total_reward': 302.40999849978834, 'kill': 67}\n",
            "\n",
            "\n",
            "[*] ROUND #1873, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002917, -0.001154]), 'NUM': [48, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.002561, -0.005   ]), 'NUM': [41, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5919\n",
            "[*] LOSS: 0.3636031746864319 / Q: {'Eval-Q': 1.778652, 'Target-Q': 1.751352}\n",
            "[*] LOSS: 0.1470026820898056 / Q: {'Eval-Q': 1.472103, 'Target-Q': 1.414983}\n",
            "[*] LOSS: 0.4835613965988159 / Q: {'Eval-Q': 1.583728, 'Target-Q': 1.510851}\n",
            "[*] LOSS: 0.23266202211380005 / Q: {'Eval-Q': 1.756578, 'Target-Q': 1.607126}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06850708517500602, 'total_reward': 385.0049976455048, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1874, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.1998  , 0.401224]), 'NUM': [25, 49]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3653\n",
            "[*] LOSS: 0.1692303717136383 / Q: {'Eval-Q': 1.633651, 'Target-Q': 1.616379}\n",
            "[*] LOSS: 0.3683926463127136 / Q: {'Eval-Q': 1.888343, 'Target-Q': 1.860342}\n",
            "[*] LOSS: 0.4716990292072296 / Q: {'Eval-Q': 1.86993, 'Target-Q': 1.895548}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07487490850949581, 'total_reward': 177.53999915067106, 'kill': 41}\n",
            "\n",
            "\n",
            "[*] ROUND #1875, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.560769, 0.301515]), 'NUM': [26, 33]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [7, 14]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4055\n",
            "[*] LOSS: 0.20869992673397064 / Q: {'Eval-Q': 1.412775, 'Target-Q': 1.433982}\n",
            "[*] LOSS: 0.3300471603870392 / Q: {'Eval-Q': 1.908094, 'Target-Q': 2.018866}\n",
            "[*] LOSS: 0.21917138993740082 / Q: {'Eval-Q': 2.299066, 'Target-Q': 2.191916}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10166777539920441, 'total_reward': 341.32999838050455, 'kill': 71}\n",
            "\n",
            "\n",
            "[*] ROUND #1876, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.079127, -0.004865]), 'NUM': [63, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([0.077759, 0.00375 ]), 'NUM': [58, 12]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7880\n",
            "[*] LOSS: 0.5889747142791748 / Q: {'Eval-Q': 1.964098, 'Target-Q': 1.762444}\n",
            "[*] LOSS: 0.14102639257907867 / Q: {'Eval-Q': 2.222996, 'Target-Q': 2.01499}\n",
            "[*] LOSS: 0.1663505882024765 / Q: {'Eval-Q': 1.272775, 'Target-Q': 1.278876}\n",
            "[*] LOSS: 0.1457047462463379 / Q: {'Eval-Q': 1.40981, 'Target-Q': 1.3204}\n",
            "[*] LOSS: 0.28006434440612793 / Q: {'Eval-Q': 2.144634, 'Target-Q': 2.074248}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04823301157503638, 'total_reward': 367.6199982268736, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1877, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.000541, 0.16375 ]), 'NUM': [37, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.008846,  0.005   ]), 'NUM': [26, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5795\n",
            "[*] LOSS: 0.36510390043258667 / Q: {'Eval-Q': 1.58162, 'Target-Q': 1.633606}\n",
            "[*] LOSS: 0.2798754572868347 / Q: {'Eval-Q': 1.446776, 'Target-Q': 1.50349}\n",
            "[*] LOSS: 0.38378846645355225 / Q: {'Eval-Q': 1.85734, 'Target-Q': 1.856739}\n",
            "[*] LOSS: 0.15210914611816406 / Q: {'Eval-Q': 1.543264, 'Target-Q': 1.458851}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07011658185540036, 'total_reward': 383.2199978493154, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1878, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.099082, -0.004865]), 'NUM': [49, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.104302, -0.0045  ]), 'NUM': [43, 10]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6524\n",
            "[*] LOSS: 0.26330816745758057 / Q: {'Eval-Q': 2.292045, 'Target-Q': 2.277365}\n",
            "[*] LOSS: 1.0181363821029663 / Q: {'Eval-Q': 2.137514, 'Target-Q': 2.334066}\n",
            "[*] LOSS: 0.16993984580039978 / Q: {'Eval-Q': 2.01943, 'Target-Q': 1.949643}\n",
            "[*] LOSS: 0.255374014377594 / Q: {'Eval-Q': 2.154384, 'Target-Q': 2.163146}\n",
            "[*] LOSS: 0.21897174417972565 / Q: {'Eval-Q': 2.60516, 'Target-Q': 2.407288}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06137471420132384, 'total_reward': 375.07999811600894, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1879, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.099457, 0.145147]), 'NUM': [46, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.280714, -0.003571]), 'NUM': [35, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5764\n",
            "[*] LOSS: 0.5417032241821289 / Q: {'Eval-Q': 1.924322, 'Target-Q': 1.92945}\n",
            "[*] LOSS: 0.16980795562267303 / Q: {'Eval-Q': 1.996349, 'Target-Q': 1.859409}\n",
            "[*] LOSS: 0.30220016837120056 / Q: {'Eval-Q': 2.343376, 'Target-Q': 2.415959}\n",
            "[*] LOSS: 0.21677938103675842 / Q: {'Eval-Q': 1.900588, 'Target-Q': 1.761964}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07223622209627903, 'total_reward': 368.40999781526625, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1880.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1880, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.170439, 0.001176]), 'NUM': [57, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.02 ]), 'NUM': [50, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.105]), 'NUM': [50, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12874\n",
            "[*] LOSS: 0.5071229338645935 / Q: {'Eval-Q': 2.513658, 'Target-Q': 2.34874}\n",
            "[*] LOSS: 0.19950878620147705 / Q: {'Eval-Q': 2.136571, 'Target-Q': 2.139909}\n",
            "[*] LOSS: 0.18813031911849976 / Q: {'Eval-Q': 2.449347, 'Target-Q': 2.246302}\n",
            "[*] LOSS: 0.4500277638435364 / Q: {'Eval-Q': 2.388916, 'Target-Q': 2.349758}\n",
            "[*] LOSS: 0.6035864949226379 / Q: {'Eval-Q': 2.014796, 'Target-Q': 1.821386}\n",
            "[*] LOSS: 0.5234657526016235 / Q: {'Eval-Q': 2.291482, 'Target-Q': 2.180214}\n",
            "[*] LOSS: 0.30741751194000244 / Q: {'Eval-Q': 2.280607, 'Target-Q': 2.151401}\n",
            "[*] LOSS: 0.1782226711511612 / Q: {'Eval-Q': 1.674539, 'Target-Q': 1.589006}\n",
            "[*] LOSS: 0.1473972648382187 / Q: {'Eval-Q': 2.002607, 'Target-Q': 1.813072}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.023764020036694606, 'total_reward': 336.08499875012785, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1881, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.286571, 0.2154  ]), 'NUM': [35, 25]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5241\n",
            "[*] LOSS: 0.17268964648246765 / Q: {'Eval-Q': 1.915002, 'Target-Q': 1.721687}\n",
            "[*] LOSS: 0.21538132429122925 / Q: {'Eval-Q': 2.344634, 'Target-Q': 2.380281}\n",
            "[*] LOSS: 0.34628742933273315 / Q: {'Eval-Q': 1.886028, 'Target-Q': 1.871237}\n",
            "[*] LOSS: 0.2316364347934723 / Q: {'Eval-Q': 1.848773, 'Target-Q': 1.792786}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07204325037571369, 'total_reward': 356.16499785520136, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1882, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.374487, -0.008542]), 'NUM': [39, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.008065,  1.628333]), 'NUM': [31, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5382\n",
            "[*] LOSS: 0.310990571975708 / Q: {'Eval-Q': 1.628618, 'Target-Q': 1.541797}\n",
            "[*] LOSS: 0.10871782898902893 / Q: {'Eval-Q': 2.534419, 'Target-Q': 2.488876}\n",
            "[*] LOSS: 0.25142961740493774 / Q: {'Eval-Q': 2.195528, 'Target-Q': 2.120642}\n",
            "[*] LOSS: 0.4353134036064148 / Q: {'Eval-Q': 1.817028, 'Target-Q': 1.849951}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07390984983866092, 'total_reward': 379.6449977653101, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1883, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.008646,  0.671034]), 'NUM': [48, 29]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.011061,  0.02    ]), 'NUM': [33, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5710\n",
            "[*] LOSS: 0.3862685561180115 / Q: {'Eval-Q': 1.777768, 'Target-Q': 1.664603}\n",
            "[*] LOSS: 0.26934272050857544 / Q: {'Eval-Q': 2.182532, 'Target-Q': 2.17816}\n",
            "[*] LOSS: 0.22767502069473267 / Q: {'Eval-Q': 1.386632, 'Target-Q': 1.404814}\n",
            "[*] LOSS: 0.447244256734848 / Q: {'Eval-Q': 1.909934, 'Target-Q': 1.83133}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06638223056195933, 'total_reward': 367.4949975647032, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1884, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.211875, 0.17375 ]), 'NUM': [24, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3605\n",
            "[*] LOSS: 0.1631735861301422 / Q: {'Eval-Q': 1.953832, 'Target-Q': 2.019936}\n",
            "[*] LOSS: 0.22063963115215302 / Q: {'Eval-Q': 1.584931, 'Target-Q': 1.52228}\n",
            "[*] LOSS: 0.5304031372070312 / Q: {'Eval-Q': 2.058672, 'Target-Q': 1.929563}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12416305725871281, 'total_reward': 323.0799982622266, 'kill': 68}\n",
            "\n",
            "\n",
            "[*] ROUND #1885, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.29875 , 0.604687]), 'NUM': [32, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.002692, -0.005   ]), 'NUM': [13, 12]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [11, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7665\n",
            "[*] LOSS: 0.21436084806919098 / Q: {'Eval-Q': 1.686606, 'Target-Q': 1.662553}\n",
            "[*] LOSS: 0.2093827724456787 / Q: {'Eval-Q': 1.83533, 'Target-Q': 1.699631}\n",
            "[*] LOSS: 0.12169517576694489 / Q: {'Eval-Q': 1.525897, 'Target-Q': 1.500579}\n",
            "[*] LOSS: 0.3386978209018707 / Q: {'Eval-Q': 1.670797, 'Target-Q': 1.584108}\n",
            "[*] LOSS: 0.1645403653383255 / Q: {'Eval-Q': 1.800811, 'Target-Q': 1.740138}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02853058542870247, 'total_reward': 353.52499785088, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1886, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.001122, -0.012895]), 'NUM': [49, 38]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.002222, -0.020385]), 'NUM': [36, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6984\n",
            "[*] LOSS: 0.3314332365989685 / Q: {'Eval-Q': 2.065188, 'Target-Q': 1.839051}\n",
            "[*] LOSS: 0.2670944929122925 / Q: {'Eval-Q': 2.435924, 'Target-Q': 2.394761}\n",
            "[*] LOSS: 0.9132725596427917 / Q: {'Eval-Q': 2.249338, 'Target-Q': 2.405135}\n",
            "[*] LOSS: 0.15194347500801086 / Q: {'Eval-Q': 2.205402, 'Target-Q': 2.202421}\n",
            "[*] LOSS: 0.26917725801467896 / Q: {'Eval-Q': 2.004708, 'Target-Q': 1.963803}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05446407604429522, 'total_reward': 376.90499785915017, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1887, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.011129, -0.001875]), 'NUM': [31, 32]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   ,  0.023571]), 'NUM': [18, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9269\n",
            "[*] LOSS: 0.370059072971344 / Q: {'Eval-Q': 2.256835, 'Target-Q': 2.216468}\n",
            "[*] LOSS: 0.37268492579460144 / Q: {'Eval-Q': 2.030652, 'Target-Q': 1.884728}\n",
            "[*] LOSS: 0.2577800154685974 / Q: {'Eval-Q': 1.468733, 'Target-Q': 1.284683}\n",
            "[*] LOSS: 0.2729211151599884 / Q: {'Eval-Q': 2.233371, 'Target-Q': 2.233189}\n",
            "[*] LOSS: 0.2605746388435364 / Q: {'Eval-Q': 1.925952, 'Target-Q': 2.0352}\n",
            "[*] LOSS: 0.21642370522022247 / Q: {'Eval-Q': 1.8984, 'Target-Q': 1.928174}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02400242228129836, 'total_reward': 348.97999841254205, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1888, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.000405, -0.018333]), 'NUM': [37, 15]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [31, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5657\n",
            "[*] LOSS: 0.2608354091644287 / Q: {'Eval-Q': 1.751152, 'Target-Q': 1.765711}\n",
            "[*] LOSS: 0.21564719080924988 / Q: {'Eval-Q': 1.847485, 'Target-Q': 1.785628}\n",
            "[*] LOSS: 0.2838127017021179 / Q: {'Eval-Q': 1.812436, 'Target-Q': 1.838036}\n",
            "[*] LOSS: 0.21335527300834656 / Q: {'Eval-Q': 1.974493, 'Target-Q': 2.022945}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06719563777222497, 'total_reward': 386.6649981364608, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1889, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.1952  , 0.170179]), 'NUM': [25, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005,  0.005]), 'NUM': [3, 10]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 10]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 9]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.055, -0.005]), 'NUM': [2, 9]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 9]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4599\n",
            "[*] LOSS: 0.4794996380805969 / Q: {'Eval-Q': 2.225659, 'Target-Q': 2.041497}\n",
            "[*] LOSS: 0.35028520226478577 / Q: {'Eval-Q': 1.889835, 'Target-Q': 1.792604}\n",
            "[*] LOSS: 0.1849193423986435 / Q: {'Eval-Q': 1.84393, 'Target-Q': 1.942064}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.025214921408444325, 'total_reward': 331.999998354353, 'kill': 72}\n",
            "\n",
            "\n",
            "[*] ROUND #1890, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.000238,  0.0075  ]), 'NUM': [21, 32]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3561\n",
            "[*] LOSS: 0.5357223749160767 / Q: {'Eval-Q': 2.107234, 'Target-Q': 2.184899}\n",
            "[*] LOSS: 0.21978315711021423 / Q: {'Eval-Q': 2.223273, 'Target-Q': 2.188924}\n",
            "[*] LOSS: 0.2840592563152313 / Q: {'Eval-Q': 2.321348, 'Target-Q': 2.195738}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09289587765270105, 'total_reward': 263.0999987628311, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #1891, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002766,  0.18037 ]), 'NUM': [47, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7226\n",
            "[*] LOSS: 0.32718297839164734 / Q: {'Eval-Q': 2.333583, 'Target-Q': 2.012873}\n",
            "[*] LOSS: 0.43880629539489746 / Q: {'Eval-Q': 2.976174, 'Target-Q': 2.984147}\n",
            "[*] LOSS: 0.18194636702537537 / Q: {'Eval-Q': 2.148624, 'Target-Q': 1.967994}\n",
            "[*] LOSS: 0.37287798523902893 / Q: {'Eval-Q': 2.759497, 'Target-Q': 2.395976}\n",
            "[*] LOSS: 0.4575995206832886 / Q: {'Eval-Q': 1.803713, 'Target-Q': 1.623997}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.047594918954558085, 'total_reward': 361.8849980439991, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1892, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.002143, 0.010385]), 'NUM': [42, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5381\n",
            "[*] LOSS: 0.569597601890564 / Q: {'Eval-Q': 2.147017, 'Target-Q': 2.169108}\n",
            "[*] LOSS: 0.23921754956245422 / Q: {'Eval-Q': 2.607587, 'Target-Q': 2.511138}\n",
            "[*] LOSS: 0.46368277072906494 / Q: {'Eval-Q': 1.637334, 'Target-Q': 1.588695}\n",
            "[*] LOSS: 0.28487473726272583 / Q: {'Eval-Q': 1.869738, 'Target-Q': 1.862313}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07346526000866059, 'total_reward': 367.4049979224801, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1893, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.18934 , -0.012308]), 'NUM': [53, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5380\n",
            "[*] LOSS: 0.1472322791814804 / Q: {'Eval-Q': 2.04863, 'Target-Q': 2.171258}\n",
            "[*] LOSS: 0.2945387661457062 / Q: {'Eval-Q': 2.101114, 'Target-Q': 1.830846}\n",
            "[*] LOSS: 0.3395015299320221 / Q: {'Eval-Q': 2.494469, 'Target-Q': 2.421378}\n",
            "[*] LOSS: 0.17120864987373352 / Q: {'Eval-Q': 2.050734, 'Target-Q': 2.104884}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07707577370344285, 'total_reward': 387.7549978448078, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1894, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.152143, -0.00125 ]), 'NUM': [35, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([0.000882, 0.02    ]), 'NUM': [17, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5305\n",
            "[*] LOSS: 0.6654704809188843 / Q: {'Eval-Q': 2.176982, 'Target-Q': 1.95474}\n",
            "[*] LOSS: 0.7890653014183044 / Q: {'Eval-Q': 2.363294, 'Target-Q': 2.367593}\n",
            "[*] LOSS: 0.6416666507720947 / Q: {'Eval-Q': 2.685907, 'Target-Q': 2.547608}\n",
            "[*] LOSS: 0.1758653223514557 / Q: {'Eval-Q': 2.316684, 'Target-Q': 2.202997}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07080861323054721, 'total_reward': 373.29999815672636, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1895, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.110556, -0.011724]), 'NUM': [45, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5571\n",
            "[*] LOSS: 0.3524616062641144 / Q: {'Eval-Q': 2.654256, 'Target-Q': 2.306368}\n",
            "[*] LOSS: 0.6211857199668884 / Q: {'Eval-Q': 2.515189, 'Target-Q': 2.543019}\n",
            "[*] LOSS: 0.3106122314929962 / Q: {'Eval-Q': 2.843235, 'Target-Q': 2.751634}\n",
            "[*] LOSS: 0.2780587673187256 / Q: {'Eval-Q': 2.778412, 'Target-Q': 2.900773}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07391835772323728, 'total_reward': 372.4399977233261, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1896, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.099184, 0.127568]), 'NUM': [49, 37]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4766\n",
            "[*] LOSS: 0.4414055347442627 / Q: {'Eval-Q': 2.318076, 'Target-Q': 2.245274}\n",
            "[*] LOSS: 0.6754390597343445 / Q: {'Eval-Q': 2.229838, 'Target-Q': 2.140202}\n",
            "[*] LOSS: 0.2632209062576294 / Q: {'Eval-Q': 2.611224, 'Target-Q': 2.341927}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09333637113928048, 'total_reward': 374.6249978020787, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1897, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.113721, 0.24525 ]), 'NUM': [43, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4494\n",
            "[*] LOSS: 0.3808930516242981 / Q: {'Eval-Q': 2.339133, 'Target-Q': 2.273036}\n",
            "[*] LOSS: 0.37852999567985535 / Q: {'Eval-Q': 2.828874, 'Target-Q': 2.628668}\n",
            "[*] LOSS: 0.6421684622764587 / Q: {'Eval-Q': 3.221912, 'Target-Q': 3.052517}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09693332684109833, 'total_reward': 385.57499795407057, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1898, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.225349, 0.308125]), 'NUM': [43, 16]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5058\n",
            "[*] LOSS: 0.3360695242881775 / Q: {'Eval-Q': 2.299132, 'Target-Q': 2.327505}\n",
            "[*] LOSS: 0.4990402162075043 / Q: {'Eval-Q': 2.130792, 'Target-Q': 2.171228}\n",
            "[*] LOSS: 0.1459020972251892 / Q: {'Eval-Q': 1.695192, 'Target-Q': 1.685494}\n",
            "[*] LOSS: 0.369331032037735 / Q: {'Eval-Q': 2.568301, 'Target-Q': 2.369069}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08037902504504318, 'total_reward': 376.61499798018485, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1899, EPS: 0.13 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.147941, 0.005893]), 'NUM': [34, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [20, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.01 , -0.005]), 'NUM': [20, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6741\n",
            "[*] LOSS: 0.3752994239330292 / Q: {'Eval-Q': 2.511936, 'Target-Q': 2.443747}\n",
            "[*] LOSS: 0.21800555288791656 / Q: {'Eval-Q': 2.339578, 'Target-Q': 2.209667}\n",
            "[*] LOSS: 0.5702599883079529 / Q: {'Eval-Q': 2.360183, 'Target-Q': 2.197928}\n",
            "[*] LOSS: 0.5579653978347778 / Q: {'Eval-Q': 2.667455, 'Target-Q': 2.720142}\n",
            "[*] LOSS: 0.5546687245368958 / Q: {'Eval-Q': 2.855986, 'Target-Q': 2.748806}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04384650995565282, 'total_reward': 357.6999981813133, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1900.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1900, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.002885, 0.191154]), 'NUM': [26, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 15]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005   , -0.011667]), 'NUM': [1, 15]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 15]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 14]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3962\n",
            "[*] LOSS: 0.22248558700084686 / Q: {'Eval-Q': 2.274585, 'Target-Q': 2.196617}\n",
            "[*] LOSS: 0.4595838785171509 / Q: {'Eval-Q': 2.698094, 'Target-Q': 2.640492}\n",
            "[*] LOSS: 0.19370043277740479 / Q: {'Eval-Q': 2.050132, 'Target-Q': 2.092545}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.03343400866308081, 'total_reward': 294.39499859698117, 'kill': 67}\n",
            "\n",
            "\n",
            "[*] ROUND #1901, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.212826, 0.292206]), 'NUM': [23, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3404\n",
            "[*] LOSS: 0.28491735458374023 / Q: {'Eval-Q': 2.481566, 'Target-Q': 2.29158}\n",
            "[*] LOSS: 0.27313220500946045 / Q: {'Eval-Q': 2.322794, 'Target-Q': 2.287984}\n",
            "[*] LOSS: 0.19011369347572327 / Q: {'Eval-Q': 2.868698, 'Target-Q': 2.826312}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09019748019602854, 'total_reward': 250.18499872274697, 'kill': 54}\n",
            "\n",
            "\n",
            "[*] ROUND #1902, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.001825,  0.003333]), 'NUM': [63, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5437\n",
            "[*] LOSS: 0.49457842111587524 / Q: {'Eval-Q': 2.550343, 'Target-Q': 2.553319}\n",
            "[*] LOSS: 0.20754733681678772 / Q: {'Eval-Q': 2.579407, 'Target-Q': 2.495084}\n",
            "[*] LOSS: 0.243169367313385 / Q: {'Eval-Q': 2.745726, 'Target-Q': 2.477446}\n",
            "[*] LOSS: 0.32836630940437317 / Q: {'Eval-Q': 1.702036, 'Target-Q': 1.720975}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07428356915636779, 'total_reward': 387.51499781478196, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1903, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.006296, 0.152576]), 'NUM': [27, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3580\n",
            "[*] LOSS: 0.6329286694526672 / Q: {'Eval-Q': 3.146204, 'Target-Q': 2.850277}\n",
            "[*] LOSS: 0.4428638815879822 / Q: {'Eval-Q': 3.60293, 'Target-Q': 3.361016}\n",
            "[*] LOSS: 0.4506087303161621 / Q: {'Eval-Q': 2.769584, 'Target-Q': 2.506859}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1473756472836677, 'total_reward': 287.00499872583896, 'kill': 63}\n",
            "\n",
            "\n",
            "[*] ROUND #1904, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.022   , 0.228333]), 'NUM': [15, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005   , -0.020385]), 'NUM': [1, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3481\n",
            "[*] LOSS: 0.2245972603559494 / Q: {'Eval-Q': 2.91999, 'Target-Q': 2.950129}\n",
            "[*] LOSS: 0.2969084084033966 / Q: {'Eval-Q': 2.86557, 'Target-Q': 2.967194}\n",
            "[*] LOSS: 0.25057145953178406 / Q: {'Eval-Q': 2.482118, 'Target-Q': 2.51341}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13766391054852428, 'total_reward': 316.39999841619283, 'kill': 68}\n",
            "\n",
            "\n",
            "[*] ROUND #1905, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.227273, 0.34    ]), 'NUM': [22, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3516\n",
            "[*] LOSS: 0.2847488522529602 / Q: {'Eval-Q': 3.18861, 'Target-Q': 3.027346}\n",
            "[*] LOSS: 0.13262930512428284 / Q: {'Eval-Q': 2.480626, 'Target-Q': 2.395007}\n",
            "[*] LOSS: 0.3945711851119995 / Q: {'Eval-Q': 2.985769, 'Target-Q': 2.711554}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07583861361554961, 'total_reward': 260.7249988587573, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #1906, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.311094, 0.806667]), 'NUM': [32, 18]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.001154, -0.105   ]), 'NUM': [26, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4750\n",
            "[*] LOSS: 0.45298707485198975 / Q: {'Eval-Q': 2.687963, 'Target-Q': 2.738345}\n",
            "[*] LOSS: 0.4281286299228668 / Q: {'Eval-Q': 2.333643, 'Target-Q': 2.317667}\n",
            "[*] LOSS: 0.39216348528862 / Q: {'Eval-Q': 3.44869, 'Target-Q': 3.273341}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08434016671791666, 'total_reward': 379.3249982316047, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1907, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.256667, 0.224048]), 'NUM': [39, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4155\n",
            "[*] LOSS: 0.39959532022476196 / Q: {'Eval-Q': 2.633659, 'Target-Q': 2.561892}\n",
            "[*] LOSS: 0.6222189664840698 / Q: {'Eval-Q': 2.330817, 'Target-Q': 2.326091}\n",
            "[*] LOSS: 0.3341193199157715 / Q: {'Eval-Q': 3.374003, 'Target-Q': 3.432536}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11936611008420335, 'total_reward': 400.1599980164319, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1908, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.242619, -0.004474]), 'NUM': [42, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4959\n",
            "[*] LOSS: 0.33951976895332336 / Q: {'Eval-Q': 2.440886, 'Target-Q': 2.134411}\n",
            "[*] LOSS: 0.17573986947536469 / Q: {'Eval-Q': 2.89409, 'Target-Q': 2.70744}\n",
            "[*] LOSS: 0.18409700691699982 / Q: {'Eval-Q': 2.730398, 'Target-Q': 2.49555}\n",
            "[*] LOSS: 0.21529625356197357 / Q: {'Eval-Q': 2.508437, 'Target-Q': 2.496936}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0895133280015768, 'total_reward': 388.2049982128665, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1909, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.257432, 0.312   ]), 'NUM': [37, 30]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4380\n",
            "[*] LOSS: 0.4556422233581543 / Q: {'Eval-Q': 2.919636, 'Target-Q': 2.965757}\n",
            "[*] LOSS: 0.4939858913421631 / Q: {'Eval-Q': 3.451287, 'Target-Q': 3.32172}\n",
            "[*] LOSS: 0.8422881364822388 / Q: {'Eval-Q': 2.627366, 'Target-Q': 2.706502}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1094071665027136, 'total_reward': 383.4949980871752, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1910, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.489, -0.02 ]), 'NUM': [50, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4934\n",
            "[*] LOSS: 0.30787229537963867 / Q: {'Eval-Q': 3.198201, 'Target-Q': 3.171533}\n",
            "[*] LOSS: 0.4052892029285431 / Q: {'Eval-Q': 3.056322, 'Target-Q': 2.800388}\n",
            "[*] LOSS: 0.3195381462574005 / Q: {'Eval-Q': 3.145304, 'Target-Q': 3.122704}\n",
            "[*] LOSS: 0.36538830399513245 / Q: {'Eval-Q': 3.413881, 'Target-Q': 3.282847}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08934835888518801, 'total_reward': 392.89999815635383, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1911, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.310106, 0.403958]), 'NUM': [47, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5089\n",
            "[*] LOSS: 0.2930253744125366 / Q: {'Eval-Q': 2.850658, 'Target-Q': 2.840583}\n",
            "[*] LOSS: 0.315717488527298 / Q: {'Eval-Q': 2.937055, 'Target-Q': 2.90806}\n",
            "[*] LOSS: 0.5335527658462524 / Q: {'Eval-Q': 3.463224, 'Target-Q': 3.53691}\n",
            "[*] LOSS: 0.260837197303772 / Q: {'Eval-Q': 3.134786, 'Target-Q': 3.076526}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08245528851374435, 'total_reward': 392.36999820079654, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1912, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.395   , -0.014333]), 'NUM': [50, 30]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4885\n",
            "[*] LOSS: 0.33112832903862 / Q: {'Eval-Q': 2.982772, 'Target-Q': 2.808504}\n",
            "[*] LOSS: 0.2559660077095032 / Q: {'Eval-Q': 2.57719, 'Target-Q': 2.470652}\n",
            "[*] LOSS: 0.3061593770980835 / Q: {'Eval-Q': 2.896881, 'Target-Q': 3.024611}\n",
            "[*] LOSS: 0.21312157809734344 / Q: {'Eval-Q': 2.704725, 'Target-Q': 2.758446}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08997299541669453, 'total_reward': 386.1049981024116, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1913, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.012826, 0.366429]), 'NUM': [23, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0525  ,  0.440455]), 'NUM': [2, 11]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 11]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 11]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 11]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3814\n",
            "[*] LOSS: 0.266274631023407 / Q: {'Eval-Q': 3.320244, 'Target-Q': 3.3289}\n",
            "[*] LOSS: 0.5108786821365356 / Q: {'Eval-Q': 3.210583, 'Target-Q': 3.239681}\n",
            "[*] LOSS: 0.37190064787864685 / Q: {'Eval-Q': 2.799422, 'Target-Q': 2.544259}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0502309858894202, 'total_reward': 321.2349984915927, 'kill': 70}\n",
            "\n",
            "\n",
            "[*] ROUND #1914, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.616458, 0.4955  ]), 'NUM': [24, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3815\n",
            "[*] LOSS: 0.5623170137405396 / Q: {'Eval-Q': 2.848352, 'Target-Q': 2.976844}\n",
            "[*] LOSS: 0.2046659290790558 / Q: {'Eval-Q': 3.026126, 'Target-Q': 2.959157}\n",
            "[*] LOSS: 0.15747565031051636 / Q: {'Eval-Q': 2.517694, 'Target-Q': 2.524794}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01759274164032514, 'total_reward': 290.62499873153865, 'kill': 62}\n",
            "\n",
            "\n",
            "[*] ROUND #1915, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.226818, -0.00875 ]), 'NUM': [44, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4999\n",
            "[*] LOSS: 0.18240663409233093 / Q: {'Eval-Q': 2.93291, 'Target-Q': 2.876922}\n",
            "[*] LOSS: 0.17794476449489594 / Q: {'Eval-Q': 2.541001, 'Target-Q': 2.520628}\n",
            "[*] LOSS: 0.5424970388412476 / Q: {'Eval-Q': 2.852587, 'Target-Q': 2.872172}\n",
            "[*] LOSS: 0.8062650561332703 / Q: {'Eval-Q': 3.148865, 'Target-Q': 3.267722}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.0879119080324751, 'total_reward': 393.9049982354045, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1916, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.231591, 0.443261]), 'NUM': [44, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [35, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5719\n",
            "[*] LOSS: 0.32437679171562195 / Q: {'Eval-Q': 4.304794, 'Target-Q': 4.149143}\n",
            "[*] LOSS: 0.1987674981355667 / Q: {'Eval-Q': 2.996917, 'Target-Q': 2.980052}\n",
            "[*] LOSS: 0.7060757875442505 / Q: {'Eval-Q': 3.53576, 'Target-Q': 3.418015}\n",
            "[*] LOSS: 0.17189152538776398 / Q: {'Eval-Q': 3.044165, 'Target-Q': 2.984577}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06851115982005096, 'total_reward': 384.4349983083084, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1917, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.350357, -0.007424]), 'NUM': [56, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5292\n",
            "[*] LOSS: 0.4907569885253906 / Q: {'Eval-Q': 3.309408, 'Target-Q': 3.234459}\n",
            "[*] LOSS: 0.42477238178253174 / Q: {'Eval-Q': 3.261614, 'Target-Q': 3.324944}\n",
            "[*] LOSS: 0.7277984619140625 / Q: {'Eval-Q': 3.381762, 'Target-Q': 3.326972}\n",
            "[*] LOSS: 0.2854424715042114 / Q: {'Eval-Q': 3.130131, 'Target-Q': 2.769858}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08111054895864753, 'total_reward': 392.09499800764024, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1918, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.160625, 0.002069]), 'NUM': [32, 29]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.006111, -0.005   ]), 'NUM': [9, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4174\n",
            "[*] LOSS: 0.2934749722480774 / Q: {'Eval-Q': 3.188302, 'Target-Q': 3.173925}\n",
            "[*] LOSS: 0.36834850907325745 / Q: {'Eval-Q': 3.230182, 'Target-Q': 3.176997}\n",
            "[*] LOSS: 0.1631576120853424 / Q: {'Eval-Q': 3.016493, 'Target-Q': 2.860674}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13062821465657296, 'total_reward': 392.78999820444733, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1919, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.253659, 0.145294]), 'NUM': [41, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4291\n",
            "[*] LOSS: 0.25982409715652466 / Q: {'Eval-Q': 3.146335, 'Target-Q': 2.803722}\n",
            "[*] LOSS: 0.3448578715324402 / Q: {'Eval-Q': 3.086691, 'Target-Q': 3.10894}\n",
            "[*] LOSS: 0.3322954475879669 / Q: {'Eval-Q': 3.10107, 'Target-Q': 2.953394}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12008356028816411, 'total_reward': 398.9899980155751, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1920.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1920, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.212826, 0.361852]), 'NUM': [23, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.028333, -0.005   ]), 'NUM': [3, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3801\n",
            "[*] LOSS: 0.4091418385505676 / Q: {'Eval-Q': 3.944233, 'Target-Q': 3.814092}\n",
            "[*] LOSS: 0.671822726726532 / Q: {'Eval-Q': 3.469232, 'Target-Q': 3.491419}\n",
            "[*] LOSS: 0.25674957036972046 / Q: {'Eval-Q': 2.844129, 'Target-Q': 2.89723}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16356023380994886, 'total_reward': 335.69999830797315, 'kill': 75}\n",
            "\n",
            "\n",
            "[*] ROUND #1921, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.199184, 0.131389]), 'NUM': [49, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4728\n",
            "[*] LOSS: 0.2991742491722107 / Q: {'Eval-Q': 3.555319, 'Target-Q': 3.379203}\n",
            "[*] LOSS: 0.6998180150985718 / Q: {'Eval-Q': 3.32611, 'Target-Q': 3.111927}\n",
            "[*] LOSS: 0.7896638512611389 / Q: {'Eval-Q': 3.384696, 'Target-Q': 3.365463}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09950297043074324, 'total_reward': 389.8799979733303, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1922, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.251098, 0.008667]), 'NUM': [41, 30]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4523\n",
            "[*] LOSS: 0.6265583634376526 / Q: {'Eval-Q': 3.208237, 'Target-Q': 3.065549}\n",
            "[*] LOSS: 0.193333238363266 / Q: {'Eval-Q': 3.184643, 'Target-Q': 2.850228}\n",
            "[*] LOSS: 0.29355093836784363 / Q: {'Eval-Q': 3.302443, 'Target-Q': 3.304301}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10629997828666242, 'total_reward': 389.4149979026988, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1923, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.121829, -0.0075  ]), 'NUM': [41, 38]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4791\n",
            "[*] LOSS: 0.36144188046455383 / Q: {'Eval-Q': 2.408164, 'Target-Q': 2.451676}\n",
            "[*] LOSS: 0.18325598537921906 / Q: {'Eval-Q': 2.678824, 'Target-Q': 2.637802}\n",
            "[*] LOSS: 0.2961686849594116 / Q: {'Eval-Q': 2.66224, 'Target-Q': 2.593742}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10323810003446034, 'total_reward': 385.1299980627373, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1924, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.500513, 0.398378]), 'NUM': [39, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 9]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [13, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8239\n",
            "[*] LOSS: 0.459307998418808 / Q: {'Eval-Q': 2.609787, 'Target-Q': 2.436254}\n",
            "[*] LOSS: 0.460426926612854 / Q: {'Eval-Q': 2.651989, 'Target-Q': 2.678751}\n",
            "[*] LOSS: 0.6688311100006104 / Q: {'Eval-Q': 3.214723, 'Target-Q': 2.845331}\n",
            "[*] LOSS: 0.4628928303718567 / Q: {'Eval-Q': 2.918658, 'Target-Q': 2.803314}\n",
            "[*] LOSS: 0.34572216868400574 / Q: {'Eval-Q': 2.567572, 'Target-Q': 2.644342}\n",
            "[*] LOSS: 0.3263683021068573 / Q: {'Eval-Q': 2.938535, 'Target-Q': 2.847137}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.025602286338395494, 'total_reward': 350.64499866962433, 'kill': 78}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1925, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.140758, 0.34    ]), 'NUM': [33, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4092\n",
            "[*] LOSS: 0.21351556479930878 / Q: {'Eval-Q': 3.465693, 'Target-Q': 3.310814}\n",
            "[*] LOSS: 0.26977843046188354 / Q: {'Eval-Q': 2.701527, 'Target-Q': 2.533595}\n",
            "[*] LOSS: 0.39134180545806885 / Q: {'Eval-Q': 3.575332, 'Target-Q': 3.290536}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13213417233723274, 'total_reward': 380.8299980945885, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1926, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.125769, 0.002885]), 'NUM': [39, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4427\n",
            "[*] LOSS: 0.39746785163879395 / Q: {'Eval-Q': 2.731059, 'Target-Q': 2.54043}\n",
            "[*] LOSS: 0.2837585210800171 / Q: {'Eval-Q': 2.975157, 'Target-Q': 2.910543}\n",
            "[*] LOSS: 0.4045867919921875 / Q: {'Eval-Q': 2.796286, 'Target-Q': 2.684227}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11031511662932622, 'total_reward': 402.2099979631603, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1927, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.128462, 0.099659]), 'NUM': [39, 44]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4557\n",
            "[*] LOSS: 0.5330057740211487 / Q: {'Eval-Q': 3.116942, 'Target-Q': 2.961643}\n",
            "[*] LOSS: 0.3079375624656677 / Q: {'Eval-Q': 2.889922, 'Target-Q': 2.735458}\n",
            "[*] LOSS: 0.5692110061645508 / Q: {'Eval-Q': 3.146967, 'Target-Q': 3.208328}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11843836150769298, 'total_reward': 384.68499812763184, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1928, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.119667, 0.26    ]), 'NUM': [45, 37]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4440\n",
            "[*] LOSS: 0.3209158480167389 / Q: {'Eval-Q': 2.998774, 'Target-Q': 3.076906}\n",
            "[*] LOSS: 0.3333590626716614 / Q: {'Eval-Q': 3.396097, 'Target-Q': 3.224778}\n",
            "[*] LOSS: 0.30713436007499695 / Q: {'Eval-Q': 2.756312, 'Target-Q': 2.476836}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11459797825351027, 'total_reward': 383.76999810058624, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1929, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.204565, 0.749038]), 'NUM': [23, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3594\n",
            "[*] LOSS: 0.3878154754638672 / Q: {'Eval-Q': 3.078448, 'Target-Q': 3.093006}\n",
            "[*] LOSS: 0.5592406392097473 / Q: {'Eval-Q': 3.156022, 'Target-Q': 2.72301}\n",
            "[*] LOSS: 0.2966000437736511 / Q: {'Eval-Q': 3.246348, 'Target-Q': 2.927606}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1423576049579308, 'total_reward': 333.2349984701723, 'kill': 72}\n",
            "\n",
            "\n",
            "[*] ROUND #1930, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.295286, 0.40375 ]), 'NUM': [35, 24]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3857\n",
            "[*] LOSS: 0.28400060534477234 / Q: {'Eval-Q': 3.348932, 'Target-Q': 2.946822}\n",
            "[*] LOSS: 0.28050488233566284 / Q: {'Eval-Q': 2.573873, 'Target-Q': 2.383802}\n",
            "[*] LOSS: 0.18573155999183655 / Q: {'Eval-Q': 2.583554, 'Target-Q': 2.409156}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.14252253351494054, 'total_reward': 403.3749980693683, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1931, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.202778, 0.283529]), 'NUM': [27, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3651\n",
            "[*] LOSS: 0.26863935589790344 / Q: {'Eval-Q': 3.098625, 'Target-Q': 3.237862}\n",
            "[*] LOSS: 0.18030278384685516 / Q: {'Eval-Q': 3.700718, 'Target-Q': 3.559376}\n",
            "[*] LOSS: 0.6455578207969666 / Q: {'Eval-Q': 2.81751, 'Target-Q': 2.890226}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11961335091064058, 'total_reward': 280.4499987643212, 'kill': 62}\n",
            "\n",
            "\n",
            "[*] ROUND #1932, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.5792  , 0.129605]), 'NUM': [25, 38]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3522\n",
            "[*] LOSS: 0.2628459632396698 / Q: {'Eval-Q': 3.14798, 'Target-Q': 2.905537}\n",
            "[*] LOSS: 0.22683820128440857 / Q: {'Eval-Q': 3.368787, 'Target-Q': 3.0792}\n",
            "[*] LOSS: 0.5357357263565063 / Q: {'Eval-Q': 3.113518, 'Target-Q': 2.802446}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11085186980806858, 'total_reward': 252.59499880298972, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #1933, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.185179, 0.767031]), 'NUM': [28, 32]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3612\n",
            "[*] LOSS: 0.40862369537353516 / Q: {'Eval-Q': 3.167616, 'Target-Q': 2.928045}\n",
            "[*] LOSS: 0.3170587122440338 / Q: {'Eval-Q': 3.48155, 'Target-Q': 3.416938}\n",
            "[*] LOSS: 0.22490094602108002 / Q: {'Eval-Q': 3.009822, 'Target-Q': 2.887384}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16696072270649864, 'total_reward': 305.4449985427782, 'kill': 67}\n",
            "\n",
            "\n",
            "[*] ROUND #1934, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.335667, 0.224659]), 'NUM': [15, 44]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3261\n",
            "[*] LOSS: 0.2893806993961334 / Q: {'Eval-Q': 3.025503, 'Target-Q': 2.762342}\n",
            "[*] LOSS: 0.5959564447402954 / Q: {'Eval-Q': 3.389568, 'Target-Q': 3.234708}\n",
            "[*] LOSS: 0.33022284507751465 / Q: {'Eval-Q': 3.025113, 'Target-Q': 2.882632}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09825309074797732, 'total_reward': 164.19999914988875, 'kill': 43}\n",
            "\n",
            "\n",
            "[*] ROUND #1935, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.0034  , 0.281111]), 'NUM': [25, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3561\n",
            "[*] LOSS: 0.5043591856956482 / Q: {'Eval-Q': 3.764703, 'Target-Q': 3.487462}\n",
            "[*] LOSS: 0.3274563252925873 / Q: {'Eval-Q': 3.43901, 'Target-Q': 3.38806}\n",
            "[*] LOSS: 0.3685683310031891 / Q: {'Eval-Q': 3.711581, 'Target-Q': 3.402228}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12595670270747178, 'total_reward': 268.79999865498394, 'kill': 61}\n",
            "\n",
            "\n",
            "[*] ROUND #1936, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.564615, 0.241538]), 'NUM': [26, 39]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3554\n",
            "[*] LOSS: 0.5927559733390808 / Q: {'Eval-Q': 4.03968, 'Target-Q': 3.748532}\n",
            "[*] LOSS: 0.3992901146411896 / Q: {'Eval-Q': 3.42115, 'Target-Q': 3.174358}\n",
            "[*] LOSS: 0.2124301642179489 / Q: {'Eval-Q': 2.936662, 'Target-Q': 2.964218}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12956644672906098, 'total_reward': 278.73499868717045, 'kill': 59}\n",
            "\n",
            "\n",
            "[*] ROUND #1937, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.012941, 0.13    ]), 'NUM': [17, 40]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3260\n",
            "[*] LOSS: 0.16164837777614594 / Q: {'Eval-Q': 2.985398, 'Target-Q': 2.636745}\n",
            "[*] LOSS: 0.35670939087867737 / Q: {'Eval-Q': 3.594096, 'Target-Q': 3.462866}\n",
            "[*] LOSS: 0.5166246294975281 / Q: {'Eval-Q': 3.339696, 'Target-Q': 3.503571}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11409131193415652, 'total_reward': 217.40499900933355, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #1938, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.012083, 0.442059]), 'NUM': [36, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3871\n",
            "[*] LOSS: 0.24155443906784058 / Q: {'Eval-Q': 2.884803, 'Target-Q': 2.775158}\n",
            "[*] LOSS: 0.30211910605430603 / Q: {'Eval-Q': 3.542585, 'Target-Q': 3.304966}\n",
            "[*] LOSS: 0.2976972758769989 / Q: {'Eval-Q': 3.080125, 'Target-Q': 2.766333}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16597080835494152, 'total_reward': 303.7499985266477, 'kill': 67}\n",
            "\n",
            "\n",
            "[*] ROUND #1939, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.004565, 0.627581]), 'NUM': [23, 31]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3497\n",
            "[*] LOSS: 0.36836954951286316 / Q: {'Eval-Q': 3.972984, 'Target-Q': 3.973902}\n",
            "[*] LOSS: 0.21395836770534515 / Q: {'Eval-Q': 3.471206, 'Target-Q': 3.423936}\n",
            "[*] LOSS: 0.42405804991722107 / Q: {'Eval-Q': 3.579018, 'Target-Q': 3.224948}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12910694583351368, 'total_reward': 291.4199986308813, 'kill': 64}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1940.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1940, EPS: 0.12 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.015   , 0.014231]), 'NUM': [35, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4108\n",
            "[*] LOSS: 0.3304558992385864 / Q: {'Eval-Q': 3.667184, 'Target-Q': 3.576016}\n",
            "[*] LOSS: 0.432740718126297 / Q: {'Eval-Q': 4.247776, 'Target-Q': 3.815299}\n",
            "[*] LOSS: 0.288643479347229 / Q: {'Eval-Q': 3.990101, 'Target-Q': 3.859093}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1292274832763422, 'total_reward': 392.5649981740862, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1941, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.505   , -0.001071]), 'NUM': [20, 28]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 13]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3568\n",
            "[*] LOSS: 0.7505351305007935 / Q: {'Eval-Q': 3.846617, 'Target-Q': 3.701602}\n",
            "[*] LOSS: 0.26628023386001587 / Q: {'Eval-Q': 3.057764, 'Target-Q': 3.043804}\n",
            "[*] LOSS: 0.5887613296508789 / Q: {'Eval-Q': 4.293924, 'Target-Q': 4.056576}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.19479520991177154, 'total_reward': 335.86499840114266, 'kill': 69}\n",
            "\n",
            "\n",
            "[*] ROUND #1942, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.0152  , 0.226818]), 'NUM': [25, 22]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4054\n",
            "[*] LOSS: 0.5261124968528748 / Q: {'Eval-Q': 3.751162, 'Target-Q': 3.34093}\n",
            "[*] LOSS: 0.541190505027771 / Q: {'Eval-Q': 3.650676, 'Target-Q': 3.469694}\n",
            "[*] LOSS: 0.5085457563400269 / Q: {'Eval-Q': 3.907856, 'Target-Q': 3.642524}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12536110541894285, 'total_reward': 376.7599981734529, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1943, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.498621, 0.007115]), 'NUM': [29, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3853\n",
            "[*] LOSS: 0.4079711437225342 / Q: {'Eval-Q': 3.609148, 'Target-Q': 3.521588}\n",
            "[*] LOSS: 0.5363937616348267 / Q: {'Eval-Q': 3.851262, 'Target-Q': 3.551151}\n",
            "[*] LOSS: 0.5189389586448669 / Q: {'Eval-Q': 4.222776, 'Target-Q': 3.991288}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12981797689659436, 'total_reward': 322.1399985095486, 'kill': 72}\n",
            "\n",
            "\n",
            "[*] ROUND #1944, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.012414, 0.143485]), 'NUM': [29, 33]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.105   ,  0.000556]), 'NUM': [1, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4114\n",
            "[*] LOSS: 0.4611009359359741 / Q: {'Eval-Q': 3.728365, 'Target-Q': 3.661562}\n",
            "[*] LOSS: 1.114212155342102 / Q: {'Eval-Q': 4.417846, 'Target-Q': 4.340282}\n",
            "[*] LOSS: 0.3750975728034973 / Q: {'Eval-Q': 3.793716, 'Target-Q': 3.601032}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08197788916320849, 'total_reward': 295.03499876894057, 'kill': 63}\n",
            "\n",
            "\n",
            "[*] ROUND #1945, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.174483, 0.145139]), 'NUM': [29, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3722\n",
            "[*] LOSS: 0.2203335165977478 / Q: {'Eval-Q': 4.145913, 'Target-Q': 3.643252}\n",
            "[*] LOSS: 0.5635071992874146 / Q: {'Eval-Q': 3.449178, 'Target-Q': 3.55917}\n",
            "[*] LOSS: 0.18164671957492828 / Q: {'Eval-Q': 3.888869, 'Target-Q': 3.529335}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10244232064386129, 'total_reward': 253.99499881081283, 'kill': 58}\n",
            "\n",
            "\n",
            "[*] ROUND #1946, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.036111, 0.307121]), 'NUM': [27, 33]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005,  0.015]), 'NUM': [9, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4311\n",
            "[*] LOSS: 0.3954366147518158 / Q: {'Eval-Q': 4.290173, 'Target-Q': 4.143844}\n",
            "[*] LOSS: 0.35661575198173523 / Q: {'Eval-Q': 4.615856, 'Target-Q': 4.028954}\n",
            "[*] LOSS: 0.4294354319572449 / Q: {'Eval-Q': 3.986023, 'Target-Q': 3.978676}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11714052009180848, 'total_reward': 373.5199980735779, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1947, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.191833, 0.138   ]), 'NUM': [30, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([0.095   , 0.000556]), 'NUM': [1, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3890\n",
            "[*] LOSS: 0.44008898735046387 / Q: {'Eval-Q': 3.541988, 'Target-Q': 3.5835}\n",
            "[*] LOSS: 0.6540587544441223 / Q: {'Eval-Q': 4.171618, 'Target-Q': 3.822595}\n",
            "[*] LOSS: 0.5359256863594055 / Q: {'Eval-Q': 3.836145, 'Target-Q': 3.523192}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13263168550090804, 'total_reward': 280.8549985466525, 'kill': 63}\n",
            "\n",
            "\n",
            "[*] ROUND #1948, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.038667, 0.286667]), 'NUM': [30, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 7]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4195\n",
            "[*] LOSS: 0.3971771001815796 / Q: {'Eval-Q': 4.207381, 'Target-Q': 3.782498}\n",
            "[*] LOSS: 0.5927232503890991 / Q: {'Eval-Q': 3.521596, 'Target-Q': 3.44947}\n",
            "[*] LOSS: 0.5591779947280884 / Q: {'Eval-Q': 5.158268, 'Target-Q': 4.784654}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.032229089118441606, 'total_reward': 333.22499831113964, 'kill': 74}\n",
            "\n",
            "\n",
            "[*] ROUND #1949, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.149857, 0.559028]), 'NUM': [35, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3811\n",
            "[*] LOSS: 0.33294543623924255 / Q: {'Eval-Q': 4.319848, 'Target-Q': 4.112972}\n",
            "[*] LOSS: 0.35273459553718567 / Q: {'Eval-Q': 3.887334, 'Target-Q': 3.742279}\n",
            "[*] LOSS: 0.9081018567085266 / Q: {'Eval-Q': 4.046938, 'Target-Q': 3.865123}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1114991737100632, 'total_reward': 274.1499987812713, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #1950, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.012241, 0.010152]), 'NUM': [29, 33]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3722\n",
            "[*] LOSS: 0.302058607339859 / Q: {'Eval-Q': 4.160738, 'Target-Q': 3.78409}\n",
            "[*] LOSS: 0.4973936080932617 / Q: {'Eval-Q': 4.74406, 'Target-Q': 4.31888}\n",
            "[*] LOSS: 0.22680334746837616 / Q: {'Eval-Q': 3.69427, 'Target-Q': 3.529114}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09827071988116023, 'total_reward': 254.29499882925302, 'kill': 59}\n",
            "\n",
            "\n",
            "[*] ROUND #1951, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.189327, 0.11775 ]), 'NUM': [52, 40]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5434\n",
            "[*] LOSS: 0.2827582359313965 / Q: {'Eval-Q': 3.584558, 'Target-Q': 3.537749}\n",
            "[*] LOSS: 0.3932092785835266 / Q: {'Eval-Q': 3.907357, 'Target-Q': 3.701812}\n",
            "[*] LOSS: 0.5586686730384827 / Q: {'Eval-Q': 4.980854, 'Target-Q': 4.547537}\n",
            "[*] LOSS: 0.27044615149497986 / Q: {'Eval-Q': 4.231354, 'Target-Q': 4.209655}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08399501451869716, 'total_reward': 398.909998123534, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1952, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.364048, 0.012439]), 'NUM': [42, 41]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.008704, -0.005   ]), 'NUM': [27, 5]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 5]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 13141\n",
            "[*] LOSS: 0.3955215811729431 / Q: {'Eval-Q': 3.645488, 'Target-Q': 3.546924}\n",
            "[*] LOSS: 0.3192211985588074 / Q: {'Eval-Q': 4.224408, 'Target-Q': 4.281492}\n",
            "[*] LOSS: 0.44793564081192017 / Q: {'Eval-Q': 3.736345, 'Target-Q': 3.422901}\n",
            "[*] LOSS: 0.4005400240421295 / Q: {'Eval-Q': 4.251923, 'Target-Q': 3.864682}\n",
            "[*] LOSS: 0.21852393448352814 / Q: {'Eval-Q': 3.226277, 'Target-Q': 3.079744}\n",
            "[*] LOSS: 0.3657413125038147 / Q: {'Eval-Q': 4.339533, 'Target-Q': 4.312286}\n",
            "[*] LOSS: 0.39375507831573486 / Q: {'Eval-Q': 3.760292, 'Target-Q': 3.552872}\n",
            "[*] LOSS: 0.43161094188690186 / Q: {'Eval-Q': 3.686964, 'Target-Q': 3.680802}\n",
            "[*] LOSS: 0.6263178586959839 / Q: {'Eval-Q': 3.89962, 'Target-Q': 3.486832}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.017477327201751564, 'total_reward': 322.7649991279468, 'kill': 76}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1953, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.015125, 0.131842]), 'NUM': [40, 38]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4036\n",
            "[*] LOSS: 0.2412189394235611 / Q: {'Eval-Q': 3.093861, 'Target-Q': 3.120607}\n",
            "[*] LOSS: 0.4401799738407135 / Q: {'Eval-Q': 3.882214, 'Target-Q': 3.619222}\n",
            "[*] LOSS: 0.509730339050293 / Q: {'Eval-Q': 4.015656, 'Target-Q': 3.723548}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.09679998799104271, 'total_reward': 298.52499861922115, 'kill': 65}\n",
            "\n",
            "\n",
            "[*] ROUND #1954, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.155429, 0.359405]), 'NUM': [35, 42]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3844\n",
            "[*] LOSS: 0.22154812514781952 / Q: {'Eval-Q': 3.58562, 'Target-Q': 3.502282}\n",
            "[*] LOSS: 0.3197450041770935 / Q: {'Eval-Q': 3.572063, 'Target-Q': 3.507412}\n",
            "[*] LOSS: 0.3143881559371948 / Q: {'Eval-Q': 3.569154, 'Target-Q': 3.549227}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08504280985083357, 'total_reward': 252.0849988302216, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #1955, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.0034, 0.295 ]), 'NUM': [25, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3572\n",
            "[*] LOSS: 0.34927046298980713 / Q: {'Eval-Q': 3.647262, 'Target-Q': 3.459976}\n",
            "[*] LOSS: 0.5997920036315918 / Q: {'Eval-Q': 3.670521, 'Target-Q': 3.374885}\n",
            "[*] LOSS: 0.6637442708015442 / Q: {'Eval-Q': 4.262567, 'Target-Q': 4.238046}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11664559820628959, 'total_reward': 249.74499887041748, 'kill': 55}\n",
            "\n",
            "\n",
            "[*] ROUND #1956, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.005667, 0.512949]), 'NUM': [30, 39]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3631\n",
            "[*] LOSS: 0.4803501069545746 / Q: {'Eval-Q': 3.205973, 'Target-Q': 3.095311}\n",
            "[*] LOSS: 0.44594645500183105 / Q: {'Eval-Q': 3.232528, 'Target-Q': 3.194438}\n",
            "[*] LOSS: 0.3490368723869324 / Q: {'Eval-Q': 4.041062, 'Target-Q': 4.038401}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12405070547917021, 'total_reward': 247.74999889358878, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #1957, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.1375  , 0.472581]), 'NUM': [38, 31]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4249\n",
            "[*] LOSS: 0.31284162402153015 / Q: {'Eval-Q': 3.563729, 'Target-Q': 3.325924}\n",
            "[*] LOSS: 0.7468555569648743 / Q: {'Eval-Q': 4.179614, 'Target-Q': 4.042374}\n",
            "[*] LOSS: 0.4639766812324524 / Q: {'Eval-Q': 3.673983, 'Target-Q': 3.760492}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13047846730067345, 'total_reward': 388.6949982671067, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1958, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.420278, 0.271081]), 'NUM': [36, 37]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3945\n",
            "[*] LOSS: 0.4302216172218323 / Q: {'Eval-Q': 3.865372, 'Target-Q': 3.691895}\n",
            "[*] LOSS: 0.7341684103012085 / Q: {'Eval-Q': 3.330979, 'Target-Q': 3.305456}\n",
            "[*] LOSS: 0.18067339062690735 / Q: {'Eval-Q': 3.328734, 'Target-Q': 3.165908}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.16680400055302713, 'total_reward': 328.0799983693287, 'kill': 70}\n",
            "\n",
            "\n",
            "[*] ROUND #1959, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.2114  , 0.164138]), 'NUM': [25, 29]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 17]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4257\n",
            "[*] LOSS: 0.33528628945350647 / Q: {'Eval-Q': 2.722812, 'Target-Q': 2.729314}\n",
            "[*] LOSS: 0.2620992362499237 / Q: {'Eval-Q': 3.829304, 'Target-Q': 3.403844}\n",
            "[*] LOSS: 0.3103431463241577 / Q: {'Eval-Q': 3.67972, 'Target-Q': 3.740775}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.019409608728967364, 'total_reward': 297.10999872069806, 'kill': 64}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1960.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1960, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.384868, 0.239268]), 'NUM': [38, 41]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4171\n",
            "[*] LOSS: 0.22735995054244995 / Q: {'Eval-Q': 3.836634, 'Target-Q': 3.484519}\n",
            "[*] LOSS: 0.31330740451812744 / Q: {'Eval-Q': 3.16911, 'Target-Q': 3.188736}\n",
            "[*] LOSS: 0.3801926374435425 / Q: {'Eval-Q': 3.462451, 'Target-Q': 3.21127}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.136653874390165, 'total_reward': 376.2499981718138, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1961, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.473226, 0.66027 ]), 'NUM': [31, 37]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3677\n",
            "[*] LOSS: 0.2294456660747528 / Q: {'Eval-Q': 3.229185, 'Target-Q': 2.762471}\n",
            "[*] LOSS: 0.35197216272354126 / Q: {'Eval-Q': 4.014464, 'Target-Q': 3.831148}\n",
            "[*] LOSS: 0.4541867673397064 / Q: {'Eval-Q': 3.361222, 'Target-Q': 3.27341}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1480250867975455, 'total_reward': 273.7199986744672, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #1962, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.382179, 0.002679]), 'NUM': [39, 28]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4428\n",
            "[*] LOSS: 0.703285813331604 / Q: {'Eval-Q': 3.987272, 'Target-Q': 3.827377}\n",
            "[*] LOSS: 0.31587859988212585 / Q: {'Eval-Q': 3.455405, 'Target-Q': 3.569885}\n",
            "[*] LOSS: 0.3449519872665405 / Q: {'Eval-Q': 3.993834, 'Target-Q': 3.897651}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11170878427443559, 'total_reward': 396.59999817889184, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1963, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.166029, 0.353793]), 'NUM': [34, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4039\n",
            "[*] LOSS: 0.22196094691753387 / Q: {'Eval-Q': 3.59929, 'Target-Q': 3.404115}\n",
            "[*] LOSS: 0.3420102000236511 / Q: {'Eval-Q': 3.874248, 'Target-Q': 3.85879}\n",
            "[*] LOSS: 0.3411804437637329 / Q: {'Eval-Q': 3.676428, 'Target-Q': 3.588659}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.13188709315226946, 'total_reward': 383.9499982269481, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1964, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.419324, 0.014839]), 'NUM': [37, 31]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4238\n",
            "[*] LOSS: 0.4445548355579376 / Q: {'Eval-Q': 2.908628, 'Target-Q': 2.903557}\n",
            "[*] LOSS: 0.3497956395149231 / Q: {'Eval-Q': 2.704014, 'Target-Q': 2.662341}\n",
            "[*] LOSS: 0.3895302414894104 / Q: {'Eval-Q': 3.791613, 'Target-Q': 3.821716}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11705667121732886, 'total_reward': 372.29999823495746, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1965, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.4425  , 0.341977]), 'NUM': [34, 43]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [9, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5489\n",
            "[*] LOSS: 0.29062405228614807 / Q: {'Eval-Q': 2.727652, 'Target-Q': 2.638032}\n",
            "[*] LOSS: 0.1712517887353897 / Q: {'Eval-Q': 2.943473, 'Target-Q': 2.836166}\n",
            "[*] LOSS: 0.5810496211051941 / Q: {'Eval-Q': 3.83215, 'Target-Q': 3.641041}\n",
            "[*] LOSS: 0.2975548207759857 / Q: {'Eval-Q': 3.059506, 'Target-Q': 2.780126}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.059577391907083437, 'total_reward': 369.61499832198024, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1966, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.172419, 0.001833]), 'NUM': [31, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [6, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5813\n",
            "[*] LOSS: 0.3886004686355591 / Q: {'Eval-Q': 3.685314, 'Target-Q': 3.369914}\n",
            "[*] LOSS: 0.2559913098812103 / Q: {'Eval-Q': 3.373098, 'Target-Q': 3.072009}\n",
            "[*] LOSS: 0.39468735456466675 / Q: {'Eval-Q': 3.015022, 'Target-Q': 3.072701}\n",
            "[*] LOSS: 0.06409040093421936 / Q: {'Eval-Q': 2.882768, 'Target-Q': 2.850711}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02898523163697785, 'total_reward': 349.10999858472496, 'kill': 77}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1967, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.132973, 0.139868]), 'NUM': [37, 38]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.011667, -0.005   ]), 'NUM': [15, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.001667, -0.005   ]), 'NUM': [15, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.011667, -0.005   ]), 'NUM': [15, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [15, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 8919\n",
            "[*] LOSS: 0.33098429441452026 / Q: {'Eval-Q': 3.936331, 'Target-Q': 3.8726}\n",
            "[*] LOSS: 0.2769525349140167 / Q: {'Eval-Q': 2.993658, 'Target-Q': 2.867644}\n",
            "[*] LOSS: 0.5616747140884399 / Q: {'Eval-Q': 3.778018, 'Target-Q': 3.670069}\n",
            "[*] LOSS: 0.2861814796924591 / Q: {'Eval-Q': 3.663421, 'Target-Q': 3.522233}\n",
            "[*] LOSS: 0.33444705605506897 / Q: {'Eval-Q': 2.576429, 'Target-Q': 2.509011}\n",
            "[*] LOSS: 0.603271484375 / Q: {'Eval-Q': 3.108046, 'Target-Q': 3.044587}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024511052387266884, 'total_reward': 346.1349983802065, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1968, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.080794, 0.092935]), 'NUM': [63, 46]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9836\n",
            "[*] LOSS: 0.28553643822669983 / Q: {'Eval-Q': 3.179842, 'Target-Q': 3.148676}\n",
            "[*] LOSS: 0.39074939489364624 / Q: {'Eval-Q': 3.53052, 'Target-Q': 3.453132}\n",
            "[*] LOSS: 0.10683148354291916 / Q: {'Eval-Q': 2.954888, 'Target-Q': 2.776054}\n",
            "[*] LOSS: 0.2855890989303589 / Q: {'Eval-Q': 4.324835, 'Target-Q': 3.949482}\n",
            "[*] LOSS: 0.3520453870296478 / Q: {'Eval-Q': 3.33401, 'Target-Q': 3.518801}\n",
            "[*] LOSS: 0.5581928491592407 / Q: {'Eval-Q': 3.551946, 'Target-Q': 3.358246}\n",
            "[*] LOSS: 0.6283801794052124 / Q: {'Eval-Q': 3.810321, 'Target-Q': 3.858649}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.035663784922390335, 'total_reward': 363.97499873675406, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1969, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.105306, 0.165167]), 'NUM': [49, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [41, 4]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 6815\n",
            "[*] LOSS: 0.2367616444826126 / Q: {'Eval-Q': 2.971429, 'Target-Q': 2.677226}\n",
            "[*] LOSS: 0.43290361762046814 / Q: {'Eval-Q': 3.057084, 'Target-Q': 3.063486}\n",
            "[*] LOSS: 0.20850025117397308 / Q: {'Eval-Q': 3.37585, 'Target-Q': 3.364091}\n",
            "[*] LOSS: 0.44891664385795593 / Q: {'Eval-Q': 3.141752, 'Target-Q': 2.960791}\n",
            "[*] LOSS: 0.3413119614124298 / Q: {'Eval-Q': 3.070117, 'Target-Q': 2.88618}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.05519455624480698, 'total_reward': 375.9249980514869, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1970, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.016875, 0.015833]), 'NUM': [32, 24]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.000238, -0.005   ]), 'NUM': [21, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [21, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 10783\n",
            "[*] LOSS: 0.22939133644104004 / Q: {'Eval-Q': 3.663581, 'Target-Q': 3.685263}\n",
            "[*] LOSS: 0.2794548273086548 / Q: {'Eval-Q': 2.41017, 'Target-Q': 2.369258}\n",
            "[*] LOSS: 0.3537558317184448 / Q: {'Eval-Q': 2.7018, 'Target-Q': 2.826697}\n",
            "[*] LOSS: 0.6947567462921143 / Q: {'Eval-Q': 3.299432, 'Target-Q': 3.040876}\n",
            "[*] LOSS: 0.556310772895813 / Q: {'Eval-Q': 3.642952, 'Target-Q': 3.30999}\n",
            "[*] LOSS: 0.284138560295105 / Q: {'Eval-Q': 2.87981, 'Target-Q': 2.582641}\n",
            "[*] LOSS: 0.39154481887817383 / Q: {'Eval-Q': 2.696883, 'Target-Q': 2.744816}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.02169013245008479, 'total_reward': 347.18499885033816, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1971, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.008919, 0.406111]), 'NUM': [37, 36]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [26, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 12527\n",
            "[*] LOSS: 0.19924554228782654 / Q: {'Eval-Q': 2.305143, 'Target-Q': 2.294496}\n",
            "[*] LOSS: 0.22130359709262848 / Q: {'Eval-Q': 2.246511, 'Target-Q': 2.268818}\n",
            "[*] LOSS: 0.0953965112566948 / Q: {'Eval-Q': 2.549116, 'Target-Q': 2.447206}\n",
            "[*] LOSS: 0.2689720392227173 / Q: {'Eval-Q': 1.989304, 'Target-Q': 2.076744}\n",
            "[*] LOSS: 0.47640460729599 / Q: {'Eval-Q': 2.295311, 'Target-Q': 2.328192}\n",
            "[*] LOSS: 0.5846071243286133 / Q: {'Eval-Q': 2.547152, 'Target-Q': 2.545222}\n",
            "[*] LOSS: 0.4240809381008148 / Q: {'Eval-Q': 3.013757, 'Target-Q': 3.158149}\n",
            "[*] LOSS: 0.2289414256811142 / Q: {'Eval-Q': 2.176931, 'Target-Q': 1.912747}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.021007148557764657, 'total_reward': 342.0399989904836, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1972, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.002885,  0.308667]), 'NUM': [52, 30]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4767\n",
            "[*] LOSS: 0.169520303606987 / Q: {'Eval-Q': 2.34568, 'Target-Q': 2.41712}\n",
            "[*] LOSS: 0.3698970377445221 / Q: {'Eval-Q': 3.476619, 'Target-Q': 3.398215}\n",
            "[*] LOSS: 0.41053539514541626 / Q: {'Eval-Q': 2.931652, 'Target-Q': 2.992344}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08786929850052481, 'total_reward': 369.7899982566014, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1973, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.168529, 0.013333]), 'NUM': [34, 33]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 4]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [17, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 9455\n",
            "[*] LOSS: 0.2048533707857132 / Q: {'Eval-Q': 2.114842, 'Target-Q': 1.831738}\n",
            "[*] LOSS: 0.15798646211624146 / Q: {'Eval-Q': 2.234728, 'Target-Q': 2.261484}\n",
            "[*] LOSS: 0.5443781614303589 / Q: {'Eval-Q': 2.330695, 'Target-Q': 2.305948}\n",
            "[*] LOSS: 0.14654438197612762 / Q: {'Eval-Q': 2.382638, 'Target-Q': 2.440539}\n",
            "[*] LOSS: 0.5226026773452759 / Q: {'Eval-Q': 2.954826, 'Target-Q': 2.889971}\n",
            "[*] LOSS: 0.37169820070266724 / Q: {'Eval-Q': 2.197039, 'Target-Q': 2.20249}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.025805705049160573, 'total_reward': 358.3449986372143, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1974, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.185566, -0.00225 ]), 'NUM': [53, 40]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5120\n",
            "[*] LOSS: 0.16667518019676208 / Q: {'Eval-Q': 2.027144, 'Target-Q': 1.828777}\n",
            "[*] LOSS: 0.4298110008239746 / Q: {'Eval-Q': 2.197271, 'Target-Q': 2.094901}\n",
            "[*] LOSS: 0.10445182025432587 / Q: {'Eval-Q': 2.078072, 'Target-Q': 2.037351}\n",
            "[*] LOSS: 0.26523321866989136 / Q: {'Eval-Q': 2.300872, 'Target-Q': 2.360398}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08278513954252269, 'total_reward': 373.1849982906133, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1975, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.113295, 0.13027 ]), 'NUM': [44, 37]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [36, 3]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 16125\n",
            "[*] LOSS: 0.08923976123332977 / Q: {'Eval-Q': 1.771176, 'Target-Q': 1.829075}\n",
            "[*] LOSS: 0.17428697645664215 / Q: {'Eval-Q': 2.566764, 'Target-Q': 2.155842}\n",
            "[*] LOSS: 0.14196157455444336 / Q: {'Eval-Q': 1.89809, 'Target-Q': 1.906436}\n",
            "[*] LOSS: 0.2670399844646454 / Q: {'Eval-Q': 2.202233, 'Target-Q': 2.18172}\n",
            "[*] LOSS: 0.1468350887298584 / Q: {'Eval-Q': 2.261534, 'Target-Q': 2.230434}\n",
            "[*] LOSS: 0.34118038415908813 / Q: {'Eval-Q': 1.989073, 'Target-Q': 1.893663}\n",
            "[*] LOSS: 0.12466932833194733 / Q: {'Eval-Q': 1.728986, 'Target-Q': 1.718186}\n",
            "[*] LOSS: 0.202292799949646 / Q: {'Eval-Q': 1.772252, 'Target-Q': 1.806039}\n",
            "[*] LOSS: 0.20725731551647186 / Q: {'Eval-Q': 1.887866, 'Target-Q': 1.911005}\n",
            "[*] LOSS: 0.15165850520133972 / Q: {'Eval-Q': 2.028598, 'Target-Q': 2.102264}\n",
            "[*] LOSS: 0.2832333743572235 / Q: {'Eval-Q': 1.846543, 'Target-Q': 1.89442}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01645991068938205, 'total_reward': 321.59999951999635, 'kill': 78}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1976, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.088103, -0.004853]), 'NUM': [58, 34]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 5]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [50, 5]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.0069,  0.995 ]), 'NUM': [50, 5]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [49, 4]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [49, 3]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [49, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [49, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 21020\n",
            "[*] LOSS: 0.19162242114543915 / Q: {'Eval-Q': 1.279298, 'Target-Q': 1.36461}\n",
            "[*] LOSS: 0.25025302171707153 / Q: {'Eval-Q': 2.166924, 'Target-Q': 2.175026}\n",
            "[*] LOSS: 0.15524724125862122 / Q: {'Eval-Q': 1.773249, 'Target-Q': 1.662533}\n",
            "[*] LOSS: 0.1109890565276146 / Q: {'Eval-Q': 1.964086, 'Target-Q': 1.76147}\n",
            "[*] LOSS: 0.13918815553188324 / Q: {'Eval-Q': 1.677472, 'Target-Q': 1.652486}\n",
            "[*] LOSS: 0.19418618083000183 / Q: {'Eval-Q': 1.525451, 'Target-Q': 1.573102}\n",
            "[*] LOSS: 0.10000374168157578 / Q: {'Eval-Q': 1.607349, 'Target-Q': 1.591034}\n",
            "[*] LOSS: 0.1140354797244072 / Q: {'Eval-Q': 1.433078, 'Target-Q': 1.39863}\n",
            "[*] LOSS: 0.0938296914100647 / Q: {'Eval-Q': 1.457385, 'Target-Q': 1.470047}\n",
            "[*] LOSS: 0.08762241154909134 / Q: {'Eval-Q': 1.475564, 'Target-Q': 1.507583}\n",
            "[*] LOSS: 0.3004133701324463 / Q: {'Eval-Q': 1.661184, 'Target-Q': 1.664176}\n",
            "[*] LOSS: 0.10945916920900345 / Q: {'Eval-Q': 1.74559, 'Target-Q': 1.696784}\n",
            "[*] LOSS: 0.15371565520763397 / Q: {'Eval-Q': 1.689662, 'Target-Q': 1.506796}\n",
            "[*] LOSS: 0.060038864612579346 / Q: {'Eval-Q': 1.675081, 'Target-Q': 1.643251}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.01195669698541204, 'total_reward': 303.6600000131875, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1977, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.121429, 0.361852]), 'NUM': [42, 27]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [27, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5474\n",
            "[*] LOSS: 0.17546571791172028 / Q: {'Eval-Q': 2.596688, 'Target-Q': 2.483192}\n",
            "[*] LOSS: 0.2301415503025055 / Q: {'Eval-Q': 2.112472, 'Target-Q': 1.558691}\n",
            "[*] LOSS: 0.3903214931488037 / Q: {'Eval-Q': 1.706166, 'Target-Q': 1.759924}\n",
            "[*] LOSS: 0.0612371489405632 / Q: {'Eval-Q': 1.398756, 'Target-Q': 1.419302}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06935870357154521, 'total_reward': 371.1999983517453, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1978, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.301471, 0.165167]), 'NUM': [17, 30]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 20]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3265\n",
            "[*] LOSS: 0.2482275366783142 / Q: {'Eval-Q': 2.113783, 'Target-Q': 2.241796}\n",
            "[*] LOSS: 0.27344879508018494 / Q: {'Eval-Q': 2.069851, 'Target-Q': 2.161529}\n",
            "[*] LOSS: 0.11677215993404388 / Q: {'Eval-Q': 2.528488, 'Target-Q': 2.443412}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11459745839587908, 'total_reward': 293.57999849691987, 'kill': 61}\n",
            "\n",
            "\n",
            "[*] ROUND #1979, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.396154, 0.72525 ]), 'NUM': [13, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3171\n",
            "[*] LOSS: 0.3546494245529175 / Q: {'Eval-Q': 1.987726, 'Target-Q': 1.994455}\n",
            "[*] LOSS: 0.10719998180866241 / Q: {'Eval-Q': 1.750378, 'Target-Q': 1.529522}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1473014488948804, 'total_reward': 314.4499984718859, 'kill': 68}\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_1980.gif with imageio.\n",
            "[*] Saved Render\n",
            "\n",
            "\n",
            "[*] ROUND #1980, EPS: 0.11 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.307647, 0.595192]), 'NUM': [17, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3111\n",
            "[*] LOSS: 0.29514697194099426 / Q: {'Eval-Q': 2.739533, 'Target-Q': 2.548481}\n",
            "[*] LOSS: 0.25875580310821533 / Q: {'Eval-Q': 1.869372, 'Target-Q': 1.90909}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1295437019370179, 'total_reward': 253.849998828955, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #1981, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.033462,  1.080185]), 'NUM': [13, 27]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3122\n",
            "[*] LOSS: 0.7863714694976807 / Q: {'Eval-Q': 1.988085, 'Target-Q': 2.053229}\n",
            "[*] LOSS: 0.21023835241794586 / Q: {'Eval-Q': 2.584196, 'Target-Q': 2.571147}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12933559618401477, 'total_reward': 266.7949988115579, 'kill': 57}\n",
            "\n",
            "\n",
            "[*] ROUND #1982, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.031591, 0.221087]), 'NUM': [22, 23]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 7]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 4]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 2]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 2]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 2]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 2]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [2, 2]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4225\n",
            "[*] LOSS: 0.152383491396904 / Q: {'Eval-Q': 2.412833, 'Target-Q': 2.135356}\n",
            "[*] LOSS: 0.2996715307235718 / Q: {'Eval-Q': 2.049613, 'Target-Q': 1.981262}\n",
            "[*] LOSS: 0.5728280544281006 / Q: {'Eval-Q': 2.498756, 'Target-Q': 2.565504}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.06292435887393374, 'total_reward': 372.6699982928112, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1983, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.366607, 0.238333]), 'NUM': [28, 21]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4711\n",
            "[*] LOSS: 0.2177763730287552 / Q: {'Eval-Q': 1.92807, 'Target-Q': 1.858546}\n",
            "[*] LOSS: 0.0754682794213295 / Q: {'Eval-Q': 2.350803, 'Target-Q': 2.434651}\n",
            "[*] LOSS: 0.18150568008422852 / Q: {'Eval-Q': 2.03515, 'Target-Q': 2.062647}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07969907197372864, 'total_reward': 390.86999829113483, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1984, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.152576, -0.004722]), 'NUM': [33, 18]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3830\n",
            "[*] LOSS: 0.3675391674041748 / Q: {'Eval-Q': 2.577263, 'Target-Q': 2.667282}\n",
            "[*] LOSS: 0.21644046902656555 / Q: {'Eval-Q': 2.161091, 'Target-Q': 2.254086}\n",
            "[*] LOSS: 0.21236032247543335 / Q: {'Eval-Q': 2.719208, 'Target-Q': 2.739586}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.12114688068528896, 'total_reward': 367.0149983437732, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1985, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.145   , 0.016316]), 'NUM': [38, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3949\n",
            "[*] LOSS: 0.5401849746704102 / Q: {'Eval-Q': 3.024923, 'Target-Q': 3.188463}\n",
            "[*] LOSS: 0.21311800181865692 / Q: {'Eval-Q': 2.731238, 'Target-Q': 2.081202}\n",
            "[*] LOSS: 0.14048440754413605 / Q: {'Eval-Q': 2.112532, 'Target-Q': 2.108341}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11364186197798205, 'total_reward': 380.21499822475016, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1986, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.093208, 0.195192]), 'NUM': [53, 26]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 16975\n",
            "[*] LOSS: 0.15446478128433228 / Q: {'Eval-Q': 1.915159, 'Target-Q': 1.846365}\n",
            "[*] LOSS: 0.5716301202774048 / Q: {'Eval-Q': 2.462971, 'Target-Q': 2.530731}\n",
            "[*] LOSS: 0.2916850745677948 / Q: {'Eval-Q': 2.629724, 'Target-Q': 2.378869}\n",
            "[*] LOSS: 0.13797754049301147 / Q: {'Eval-Q': 2.099425, 'Target-Q': 2.063389}\n",
            "[*] LOSS: 0.5880837440490723 / Q: {'Eval-Q': 2.628316, 'Target-Q': 2.51683}\n",
            "[*] LOSS: 0.15914461016654968 / Q: {'Eval-Q': 2.333588, 'Target-Q': 2.124982}\n",
            "[*] LOSS: 0.2959713935852051 / Q: {'Eval-Q': 2.424378, 'Target-Q': 2.487684}\n",
            "[*] LOSS: 0.24740129709243774 / Q: {'Eval-Q': 2.020465, 'Target-Q': 1.94739}\n",
            "[*] LOSS: 0.2713600993156433 / Q: {'Eval-Q': 2.43941, 'Target-Q': 2.107804}\n",
            "[*] LOSS: 0.5754028558731079 / Q: {'Eval-Q': 2.576622, 'Target-Q': 2.490096}\n",
            "[*] LOSS: 0.16018322110176086 / Q: {'Eval-Q': 1.435586, 'Target-Q': 1.51602}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.013328348568916674, 'total_reward': 319.7399996621534, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1987, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.223182, 0.560857]), 'NUM': [22, 35]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 25]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3368\n",
            "[*] LOSS: 0.19162660837173462 / Q: {'Eval-Q': 2.569102, 'Target-Q': 2.517086}\n",
            "[*] LOSS: 0.2547355890274048 / Q: {'Eval-Q': 2.83917, 'Target-Q': 2.63342}\n",
            "[*] LOSS: 0.5287045836448669 / Q: {'Eval-Q': 2.363407, 'Target-Q': 2.444196}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.061638345761525576, 'total_reward': 270.4649988198653, 'kill': 56}\n",
            "\n",
            "\n",
            "[*] ROUND #1988, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.007903,  0.230714]), 'NUM': [31, 42]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [3, 34]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4813\n",
            "[*] LOSS: 0.10856796056032181 / Q: {'Eval-Q': 2.156406, 'Target-Q': 2.075317}\n",
            "[*] LOSS: 0.4195496737957001 / Q: {'Eval-Q': 2.173874, 'Target-Q': 2.086597}\n",
            "[*] LOSS: 0.30230072140693665 / Q: {'Eval-Q': 2.399859, 'Target-Q': 2.207013}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.007615654673779246, 'total_reward': 195.82499922066927, 'kill': 47}\n",
            "\n",
            "\n",
            "[*] ROUND #1989, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.002069, 0.153065]), 'NUM': [29, 31]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [1, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 3876\n",
            "[*] LOSS: 0.40339192748069763 / Q: {'Eval-Q': 2.427684, 'Target-Q': 2.481424}\n",
            "[*] LOSS: 0.15106385946273804 / Q: {'Eval-Q': 1.896733, 'Target-Q': 1.724887}\n",
            "[*] LOSS: 0.6432085633277893 / Q: {'Eval-Q': 2.722808, 'Target-Q': 2.594952}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.014305289432794481, 'total_reward': 269.8199989106506, 'kill': 60}\n",
            "\n",
            "\n",
            "[*] ROUND #1990, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.008103,  0.2475  ]), 'NUM': [29, 40]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0145  ,  0.133889]), 'NUM': [10, 36]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4220\n",
            "[*] LOSS: 0.4212895333766937 / Q: {'Eval-Q': 2.783562, 'Target-Q': 2.852419}\n",
            "[*] LOSS: 0.3118939995765686 / Q: {'Eval-Q': 2.57606, 'Target-Q': 2.529439}\n",
            "[*] LOSS: 0.29152244329452515 / Q: {'Eval-Q': 2.539309, 'Target-Q': 2.582631}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.024540183033242322, 'total_reward': 197.30499911494553, 'kill': 46}\n",
            "\n",
            "\n",
            "[*] ROUND #1991, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.278472, -0.014048]), 'NUM': [36, 21]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [34, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [34, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7801\n",
            "[*] LOSS: 0.4794657826423645 / Q: {'Eval-Q': 2.917682, 'Target-Q': 3.030394}\n",
            "[*] LOSS: 0.2067619413137436 / Q: {'Eval-Q': 3.34762, 'Target-Q': 2.895563}\n",
            "[*] LOSS: 0.4981692433357239 / Q: {'Eval-Q': 3.026748, 'Target-Q': 2.944692}\n",
            "[*] LOSS: 0.2058536559343338 / Q: {'Eval-Q': 3.254797, 'Target-Q': 2.941592}\n",
            "[*] LOSS: 0.28638213872909546 / Q: {'Eval-Q': 3.394461, 'Target-Q': 3.088786}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.040845086550939776, 'total_reward': 360.7299984320998, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1992, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.365732, -0.012115]), 'NUM': [41, 26]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4371\n",
            "[*] LOSS: 0.2832348346710205 / Q: {'Eval-Q': 2.690032, 'Target-Q': 2.606983}\n",
            "[*] LOSS: 0.2300005406141281 / Q: {'Eval-Q': 2.848705, 'Target-Q': 2.880969}\n",
            "[*] LOSS: 0.3302321434020996 / Q: {'Eval-Q': 2.772026, 'Target-Q': 2.270974}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.10408753367910563, 'total_reward': 382.08499822579324, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1993, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.2051, 0.2455]), 'NUM': [50, 20]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 5322\n",
            "[*] LOSS: 0.4609302580356598 / Q: {'Eval-Q': 3.21606, 'Target-Q': 3.006399}\n",
            "[*] LOSS: 0.557106077671051 / Q: {'Eval-Q': 3.908207, 'Target-Q': 4.002205}\n",
            "[*] LOSS: 0.3676340579986572 / Q: {'Eval-Q': 3.549194, 'Target-Q': 3.027392}\n",
            "[*] LOSS: 0.26295119524002075 / Q: {'Eval-Q': 3.264296, 'Target-Q': 3.069733}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.07492995987468593, 'total_reward': 390.07499839831144, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1994, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.127674, 0.242857]), 'NUM': [43, 21]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4118\n",
            "[*] LOSS: 0.3745916783809662 / Q: {'Eval-Q': 2.967086, 'Target-Q': 2.661511}\n",
            "[*] LOSS: 0.4569362998008728 / Q: {'Eval-Q': 2.727893, 'Target-Q': 2.874088}\n",
            "[*] LOSS: 0.20642894506454468 / Q: {'Eval-Q': 3.157071, 'Target-Q': 3.01675}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.1167731803802208, 'total_reward': 407.92999810073525, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1995, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.571744, -0.019474]), 'NUM': [43, 19]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [38, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 7878\n",
            "[*] LOSS: 0.18304584920406342 / Q: {'Eval-Q': 2.354744, 'Target-Q': 2.312067}\n",
            "[*] LOSS: 0.3011855483055115 / Q: {'Eval-Q': 2.798966, 'Target-Q': 2.621774}\n",
            "[*] LOSS: 0.23499618470668793 / Q: {'Eval-Q': 2.650489, 'Target-Q': 2.690488}\n",
            "[*] LOSS: 0.3454146683216095 / Q: {'Eval-Q': 2.607528, 'Target-Q': 2.57264}\n",
            "[*] LOSS: 0.3217090964317322 / Q: {'Eval-Q': 2.911556, 'Target-Q': 2.605957}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.04098697217827385, 'total_reward': 374.32499856036156, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1996, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-1.19000e-04,  1.63966e-01]), 'NUM': [42, 29]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4467\n",
            "[*] LOSS: 0.5873795747756958 / Q: {'Eval-Q': 2.715607, 'Target-Q': 2.477567}\n",
            "[*] LOSS: 0.43554022908210754 / Q: {'Eval-Q': 2.624168, 'Target-Q': 2.434169}\n",
            "[*] LOSS: 0.1420697122812271 / Q: {'Eval-Q': 2.179265, 'Target-Q': 1.988632}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11164651193932043, 'total_reward': 388.55499830748886, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1997, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.189444, -0.008333]), 'NUM': [54, 27]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4769\n",
            "[*] LOSS: 0.9151539206504822 / Q: {'Eval-Q': 2.302193, 'Target-Q': 2.449391}\n",
            "[*] LOSS: 0.3034331798553467 / Q: {'Eval-Q': 2.605052, 'Target-Q': 2.587679}\n",
            "[*] LOSS: 0.19568856060504913 / Q: {'Eval-Q': 2.715175, 'Target-Q': 2.774118}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.08714365144160705, 'total_reward': 377.01999810151756, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1998, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.12119 , 0.011053]), 'NUM': [42, 19]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 4121\n",
            "[*] LOSS: 0.2424848973751068 / Q: {'Eval-Q': 3.254373, 'Target-Q': 3.284798}\n",
            "[*] LOSS: 0.34663933515548706 / Q: {'Eval-Q': 2.961168, 'Target-Q': 2.865926}\n",
            "[*] LOSS: 0.24162130057811737 / Q: {'Eval-Q': 2.575705, 'Target-Q': 2.459407}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.11370061864921094, 'total_reward': 394.52499821409583, 'kill': 79}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "\n",
            "\n",
            "[*] ROUND #1999, EPS: 0.10 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([ 0.182719, -0.004767]), 'NUM': [57, 43]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.007326, -0.005   ]), 'NUM': [43, 1]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [43, 1]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [43, 1]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [43, 1]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007326, -0.005   ]), 'NUM': [43, 1]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.007326, -0.005   ]), 'NUM': [43, 1]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.007326, -0.005   ]), 'NUM': [43, 1]}\n",
            "\n",
            "[INFO] Length of buffer and new add: 80000 18867\n",
            "[*] LOSS: 0.23363865911960602 / Q: {'Eval-Q': 3.19786, 'Target-Q': 3.014724}\n",
            "[*] LOSS: 0.1796334981918335 / Q: {'Eval-Q': 2.112727, 'Target-Q': 2.085246}\n",
            "[*] LOSS: 0.25634995102882385 / Q: {'Eval-Q': 2.427721, 'Target-Q': 2.34302}\n",
            "[*] LOSS: 0.2473367601633072 / Q: {'Eval-Q': 3.004608, 'Target-Q': 2.896324}\n",
            "[*] LOSS: 0.38265833258628845 / Q: {'Eval-Q': 2.799746, 'Target-Q': 2.849431}\n",
            "[*] LOSS: 0.1166672483086586 / Q: {'Eval-Q': 1.944466, 'Target-Q': 1.900801}\n",
            "[*] LOSS: 0.3453211784362793 / Q: {'Eval-Q': 1.921888, 'Target-Q': 1.957967}\n",
            "[*] LOSS: 0.10158195346593857 / Q: {'Eval-Q': 1.328827, 'Target-Q': 1.331255}\n",
            "[*] LOSS: 0.5009223818778992 / Q: {'Eval-Q': 3.13759, 'Target-Q': 2.939861}\n",
            "[*] LOSS: 0.2900385856628418 / Q: {'Eval-Q': 2.36962, 'Target-Q': 2.367347}\n",
            "[*] LOSS: 0.27436164021492004 / Q: {'Eval-Q': 2.409469, 'Target-Q': 2.342044}\n",
            "[*] LOSS: 0.10293202847242355 / Q: {'Eval-Q': 2.83372, 'Target-Q': 2.55231}\n",
            "\n",
            "[INFO] {'ave_agent_reward': 0.012290035103845716, 'total_reward': 300.1549994163215, 'kill': 80}\n",
            "\u001b[1;34m\n",
            "[INFO] Begin self-play Update ...\u001b[0m\n",
            "\u001b[1;34m[INFO] Self-play Updated!\n",
            "\u001b[0m\n",
            "\u001b[1;34m[INFO] Saving model ...\u001b[0m\n",
            "[*] Model saved\n",
            "[*] Model saved\n",
            "[*] Saving Render\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/render/iql/replay_2000.gif with imageio.\n",
            "[*] Saved Render\n"
          ]
        }
      ],
      "source": [
        "!python train_battle.py --algo iql --cuda True --render --max_steps 400 --n_round 2000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN BATTLE VS RANDOM\n",
        "In this part, we set the blue is my IQL model (blue team) and the opponent is random (red team)."
      ],
      "metadata": {
        "id": "b5TebAjKqbAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python battle_vs_random.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D9FtOLoqkp7",
        "outputId": "ea0b606f-27f5-4c58-c142-3b30afcbc2cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #0, EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.045, -0.005]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.048421, -0.005   ]), 'NUM': [76, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.037877, -0.005   ]), 'NUM': [73, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.044394, -0.003765]), 'NUM': [66, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.035645, -0.003765]), 'NUM': [62, 81]}\n",
            "[*] Saving render to /content/Magent2-RL-main/data/battle_vs_random.gif...\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/battle_vs_random.gif with imageio.\n",
            "[*] Render saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN EVAL VS RANDOM\n",
        "We evaluate our model (IQL) (blue team) by fighting against random opponent (red team)\n",
        "We use two main metrics to evaluate the results:\n",
        "\n",
        "- **Win Rate**: The win rate of the team.\n",
        "- **Mean Reward**: The average reward of each team during the matches.\n",
        "\n",
        "The **Win Rate** (win_rate) is determined by the following rule:\n",
        "- The **red team** is considered the winner if the number of opponents they eliminate (`n_kill[\"red\"]`) is greater than or equal to the number of opponents eliminated by the **blue team** (5 agents).\n",
        "- The **blue team** is considered the winner if the number of opponents they eliminate is greater than or equal to the number of opponents eliminated by the **red team** (5 agents).\n",
        "- In all other cases, the result is considered a draw.\n",
        "\n",
        "This rule is applied for matches with a maximum duration of 300 steps per round. We perform evaluations over 30 rounds to ensure accuracy and reliability."
      ],
      "metadata": {
        "id": "92IRuuaXqzdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_random.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb1AhZyiq6LL",
        "outputId": "df0a509f-89e2-4ce0-cd27-d415ee71b7bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[0], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.043272, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.039615, -0.005   ]), 'NUM': [78, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.035556, -0.005   ]), 'NUM': [72, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.039328, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.052761, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.040385, -0.005   ]), 'NUM': [65, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[1], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.034114, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.037, -0.005]), 'NUM': [75, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.043356, -0.005   ]), 'NUM': [73, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.052761, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.045323, -0.005   ]), 'NUM': [62, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[2], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.045741, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.05057, -0.005  ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.042838, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.040135, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.041765, -0.005   ]), 'NUM': [68, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.034412, -0.005   ]), 'NUM': [68, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[3], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.038333, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0525, -0.005 ]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.030676, -0.001296]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.042313, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.050455, -0.005   ]), 'NUM': [66, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.048077, -0.003765]), 'NUM': [65, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[4], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.037099, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.042975, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.051753, -0.007469]), 'NUM': [77, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.033986, -0.005   ]), 'NUM': [69, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.035769, -0.005   ]), 'NUM': [65, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.046935, -0.005   ]), 'NUM': [62, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[5], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.038333, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.0375  , -0.003765]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.050833, -0.005   ]), 'NUM': [72, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.050588, -0.005   ]), 'NUM': [68, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.033358, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.050313, -0.003765]), 'NUM': [64, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[6], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.050679, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.042179, -0.005   ]), 'NUM': [78, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.038333, -0.006235]), 'NUM': [75, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.04162 , -0.006235]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.041765, -0.006235]), 'NUM': [68, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.049444, -0.005   ]), 'NUM': [63, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[7], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.045, -0.005]), 'NUM': [80, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.045506, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.040616, -0.006235]), 'NUM': [73, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.051479, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.050313, -0.005   ]), 'NUM': [64, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.037258, -0.005   ]), 'NUM': [62, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[8], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.042037, -0.003765]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.05057, -0.005  ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.053649, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.047466, -0.005   ]), 'NUM': [73, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.053611, -0.005   ]), 'NUM': [72, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.042143, -0.005   ]), 'NUM': [70, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[9], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.04821, -0.005  ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.041709, -0.003765]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.036081, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.044437, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.044706, -0.005   ]), 'NUM': [68, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.040385, -0.005   ]), 'NUM': [65, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[10], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.045741, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.041709, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.041364, -0.003765]), 'NUM': [77, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.043356, -0.005   ]), 'NUM': [73, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.039783, -0.006235]), 'NUM': [69, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.044063, -0.005   ]), 'NUM': [64, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[11], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.04821 , -0.003765]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.042975, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.036944, -0.002531]), 'NUM': [72, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.043806, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.035159, -0.002531]), 'NUM': [63, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.049068, -0.005   ]), 'NUM': [59, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[12], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.051914, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.04625 , -0.002531]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.040526, -0.006235]), 'NUM': [76, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.041986, -0.006235]), 'NUM': [73, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.040211, -0.003765]), 'NUM': [71, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.039783, -0.005   ]), 'NUM': [69, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[13], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.03375, -0.005  ]), 'NUM': [80, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.038333, -0.005   ]), 'NUM': [78, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.037877, -0.006235]), 'NUM': [73, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.045909, -0.003765]), 'NUM': [66, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.043095, -0.005   ]), 'NUM': [63, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.046667, -0.005   ]), 'NUM': [60, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[14], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.042037, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.035, -0.005]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.042333, -0.005   ]), 'NUM': [75, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.053571, -0.005   ]), 'NUM': [70, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.033571, -0.006235]), 'NUM': [63, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.028729, -0.005   ]), 'NUM': [59, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[15], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.029691, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.05625, -0.005  ]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.038784, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.044437, -0.006235]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.036818, -0.005   ]), 'NUM': [66, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.035079,  0.054259]), 'NUM': [63, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[16], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.030926, -0.002531]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.046267,  0.055494]), 'NUM': [75, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.048662, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.04162 , -0.006235]), 'NUM': [71, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.041765, -0.003765]), 'NUM': [68, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[17], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.03875, -0.005  ]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.039667, -0.005   ]), 'NUM': [75, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.042838, -0.006235]), 'NUM': [74, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.043028, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.049444, -0.005   ]), 'NUM': [63, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[18], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.037099, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.046772, -0.002531]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.032027, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.047857, -0.005   ]), 'NUM': [70, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.036818, -0.006235]), 'NUM': [66, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.046935, -0.005   ]), 'NUM': [62, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[19], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.042037, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.035, -0.005]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.045541, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.045299, -0.003765]), 'NUM': [67, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.052619, -0.005   ]), 'NUM': [63, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.033333, -0.005   ]), 'NUM': [60, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[20], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.038333, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.03125 , -0.006235]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.040443, -0.006235]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.048056, -0.005   ]), 'NUM': [72, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.039214,  0.055494]), 'NUM': [70, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.044063, -0.005   ]), 'NUM': [64, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[21], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.033395, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.037911, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.051667, -0.005   ]), 'NUM': [75, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.038333, -0.005   ]), 'NUM': [75, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.045208,  0.055494]), 'NUM': [72, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.039848, -0.003765]), 'NUM': [66, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[22], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.03463, -0.005  ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.04125 , -0.002531]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.050205, -0.005   ]), 'NUM': [73, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.038803, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.042143, -0.005   ]), 'NUM': [70, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.036343, -0.005   ]), 'NUM': [67, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[23], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.050679, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.045   , -0.002531]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.041842, -0.003765]), 'NUM': [76, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.033378, -0.005   ]), 'NUM': [74, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.047647, -0.005   ]), 'NUM': [68, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.040821, -0.006235]), 'NUM': [67, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[24], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.04821, -0.005  ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.040065, -0.005   ]), 'NUM': [77, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.045845, -0.003765]), 'NUM': [71, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.040294, -0.005   ]), 'NUM': [68, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.04371 , -0.006235]), 'NUM': [62, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.050763, -0.003765]), 'NUM': [59, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[25], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.039568, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.03538 , -0.003765]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.043889, -0.005   ]), 'NUM': [72, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.043028, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.049118, -0.003765]), 'NUM': [68, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.047424, -0.005   ]), 'NUM': [66, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[26], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.051914, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.044744, -0.002531]), 'NUM': [78, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.046892, -0.003765]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.052887, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.043806, -0.002531]), 'NUM': [67, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.049444, -0.005   ]), 'NUM': [63, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[27], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.05125, -0.005  ]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.043889, -0.005   ]), 'NUM': [72, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.042681, -0.005   ]), 'NUM': [69, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.036746, -0.005   ]), 'NUM': [63, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.031667, -0.005   ]), 'NUM': [60, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[28], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.045506, -0.003765]), 'NUM': [79, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.045541, -0.003765]), 'NUM': [74, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.043028, -0.005   ]), 'NUM': [71, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.040821, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.044394, -0.005   ]), 'NUM': [66, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[29], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.040802, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.04125, -0.005  ]), 'NUM': [80, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.043356, -0.005   ]), 'NUM': [73, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.041111, -0.006235]), 'NUM': [72, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.048284, -0.005   ]), 'NUM': [67, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.040385, -0.005   ]), 'NUM': [65, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[30], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.038333, -0.003765]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.036579, -0.006235]), 'NUM': [76, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.041918,  0.055494]), 'NUM': [73, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.047424, -0.005   ]), 'NUM': [66, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.04627, -0.005  ]), 'NUM': [63, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.038871, -0.003765]), 'NUM': [62, 81]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.041207, -0.005   ]), 'NUM': [58, 81]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.041121,  0.056728]), 'NUM': [58, 81]}\n",
            "> step #450, info: {'Ave-Reward': array([-0.043182, -0.003765]), 'NUM': [55, 81]}\n",
            "> step #500, info: {'Ave-Reward': array([-0.048396, -0.005   ]), 'NUM': [53, 81]}\n",
            "> step #550, info: {'Ave-Reward': array([-0.046176, -0.002531]), 'NUM': [51, 81]}\n",
            "> step #600, info: {'Ave-Reward': array([-0.04117, -0.005  ]), 'NUM': [47, 81]}\n",
            "> step #650, info: {'Ave-Reward': array([-0.034787, -0.005   ]), 'NUM': [47, 81]}\n",
            "> step #700, info: {'Ave-Reward': array([-0.050652, -0.005   ]), 'NUM': [46, 81]}\n",
            "> step #750, info: {'Ave-Reward': array([-0.058333, -0.005   ]), 'NUM': [45, 81]}\n",
            "> step #800, info: {'Ave-Reward': array([-0.042778, -0.005   ]), 'NUM': [45, 81]}\n",
            "> step #850, info: {'Ave-Reward': array([-0.045, -0.005]), 'NUM': [45, 81]}\n",
            "> step #900, info: {'Ave-Reward': array([-0.042778, -0.005   ]), 'NUM': [45, 81]}\n",
            "> step #950, info: {'Ave-Reward': array([-0.039091, -0.005   ]), 'NUM': [44, 81]}\n",
            "> step #1000, info: {'Ave-Reward': array([-0.02593, -0.005  ]), 'NUM': [43, 81]}\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/iql_vs_random.gif with imageio.\n",
            "[*] Render saved to /content/Magent2-RL-main/data/iql_vs_random.gif\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Total Episodes: 30\n",
            "Blue Wins: 30 (100.00%)\n",
            "Red Wins: 0 (0.00%)\n",
            "Draws: 0 (0.00%)\n",
            "\n",
            "Reward Statistics:\n",
            "Blue Average Reward: -0.0014 ± 0.0006\n",
            "Red Average Reward: -0.0428 ± 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN BATTLE VS DQN\n",
        "In this part, we set the blue is my IQL model (blue team) and the opponent is DQN pretrained red.pt (red team)."
      ],
      "metadata": {
        "id": "3PL9ECKoh3KP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ7O1P0FgLRP",
        "outputId": "2aa0cd14-684a-4d98-c43d-8355a430f85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #0, EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [47, 66]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 66]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 66]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [46, 66]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.01587, -0.005  ]), 'NUM': [46, 66]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.01587, -0.005  ]), 'NUM': [46, 66]}\n",
            "[*] Saving render to /content/Magent2-RL-main/data/battle_vs_dqn.gif...\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/battle_vs_dqn.gif with imageio.\n",
            "[*] Render saved!\n"
          ]
        }
      ],
      "source": [
        "!python battle_vs_dqn.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN EVAL VS DQN\n",
        "We evaluate our model (IQL) (blue team) by fighting against DQN (red.pt)\n",
        "We use two main metrics to evaluate the results:\n",
        "\n",
        "- **Win Rate**: The win rate of the team.\n",
        "- **Mean Reward**: The average reward of each team during the matches.\n",
        "\n",
        "The **Win Rate** (win_rate) is determined by the following rule:\n",
        "- The **red team** is considered the winner if the number of opponents they eliminate (`n_kill[\"red\"]`) is greater than or equal to the number of opponents eliminated by the **blue team** (5 agents).\n",
        "- The **blue team** is considered the winner if the number of opponents they eliminate is greater than or equal to the number of opponents eliminated by the **red team** (5 agents).\n",
        "- In all other cases, the result is considered a draw.\n",
        "\n",
        "This rule is applied for matches with a maximum duration of 300 steps per round. We perform evaluations over 30 rounds to ensure accuracy and reliability."
      ],
      "metadata": {
        "id": "S4rqVwr7jiFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_dqn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJufBLGIk5uq",
        "outputId": "caef09e2-fb69-4c1d-a027-3a0e4a8c497b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[0], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[1], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[2], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[3], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[4], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[5], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[6], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[7], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[8], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[9], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[10], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[11], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[12], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[13], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[14], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[15], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[16], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[17], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[18], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[19], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[20], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[21], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[22], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[23], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[24], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[25], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[26], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[27], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[28], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[29], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[30], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([0.10875 , 0.070887]), 'NUM': [44, 62]}\n",
            "> step #100, info: {'Ave-Reward': array([ 0.172778, -0.011413]), 'NUM': [27, 46]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [18, 36]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [16, 34]}\n",
            "> step #250, info: {'Ave-Reward': array([ 0.002143, -0.005   ]), 'NUM': [14, 31]}\n",
            "> step #300, info: {'Ave-Reward': array([0.0055  , 0.159286]), 'NUM': [10, 28]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 22]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [5, 22]}\n",
            "> step #450, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #500, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #550, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #600, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #650, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #700, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #750, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #800, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #850, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #900, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #950, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "> step #1000, info: {'Ave-Reward': array([-0.005, -0.005]), 'NUM': [4, 21]}\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/iql_vs_dqn.gif with imageio.\n",
            "[*] Render saved to /content/Magent2-RL-main/data/iql_vs_dqn.gif\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Total Episodes: 30\n",
            "Blue Wins: 30 (100.00%)\n",
            "Red Wins: 0 (0.00%)\n",
            "Draws: 0 (0.00%)\n",
            "\n",
            "Reward Statistics:\n",
            "Blue Average Reward: 0.0170 ± 0.0000\n",
            "Red Average Reward: 0.0247 ± 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN BATTLE VS FINAL MODEL\n",
        "In this part, we set the blue is my IQL model (blue team) and the opponent is DQN pretrained red_final.pt (red team)."
      ],
      "metadata": {
        "id": "GZ9xngl1k-4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python battle_vs_final.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k416qYANlPqj",
        "outputId": "6f9fcb8f-39ca-4ccf-bd1c-672369b95000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #0, EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Saving render to /content/Magent2-RL-main/data/battle_vs_final.gif...\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/battle_vs_final.gif with imageio.\n",
            "[*] Render saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN EVAL VS FINAL MODEL\n",
        "We evaluate our model (IQL) (blue team) by fighting against DQN (red_final.pt)\n",
        "We use two main metrics to evaluate the results:\n",
        "\n",
        "- **Win Rate**: The win rate of the team.\n",
        "- **Mean Reward**: The average reward of each team during the matches.\n",
        "\n",
        "The **Win Rate** (win_rate) is determined by the following rule:\n",
        "- The **red team** is considered the winner if the number of opponents they eliminate (`n_kill[\"red\"]`) is greater than or equal to the number of opponents eliminated by the **blue team** (5 agents).\n",
        "- The **blue team** is considered the winner if the number of opponents they eliminate is greater than or equal to the number of opponents eliminated by the **red team** (5 agents).\n",
        "- In all other cases, the result is considered a draw.\n",
        "\n",
        "This rule is applied for matches with a maximum duration of 300 steps per round. We perform evaluations over 30 rounds to ensure accuracy and reliability."
      ],
      "metadata": {
        "id": "9usmThWolawN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_final.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY0kqvsRlm4h",
        "outputId": "9ccef3db-ff1a-4960-ba69-e69e23208c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: XDG_RUNTIME_DIR not set in the environment.\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[0], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[1], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[2], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[3], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[4], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[5], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[6], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[7], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[8], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[9], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[10], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[11], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[12], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[13], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[14], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[15], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[16], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[17], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[18], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[19], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[20], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[21], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[22], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[23], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[24], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[25], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[26], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[27], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[28], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[29], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "[*] Loaded model\n",
            "\n",
            "\n",
            "[*] ROUND #[30], EPS: 1.00 NUMBER: [81, 81]\n",
            "> step #50, info: {'Ave-Reward': array([-0.011173, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #100, info: {'Ave-Reward': array([-0.013642, -0.005   ]), 'NUM': [81, 81]}\n",
            "> step #150, info: {'Ave-Reward': array([-0.006266, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #200, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #250, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #300, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #350, info: {'Ave-Reward': array([-0.008797, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #400, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #450, info: {'Ave-Reward': array([-0.008797, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #500, info: {'Ave-Reward': array([-0.008797, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #550, info: {'Ave-Reward': array([-0.008797, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #600, info: {'Ave-Reward': array([-0.015127, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #650, info: {'Ave-Reward': array([-0.010063, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #700, info: {'Ave-Reward': array([-0.012595, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #750, info: {'Ave-Reward': array([-0.016392, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #800, info: {'Ave-Reward': array([-0.007532, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #850, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #900, info: {'Ave-Reward': array([-0.010063, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #950, info: {'Ave-Reward': array([-0.011329, -0.005   ]), 'NUM': [79, 81]}\n",
            "> step #1000, info: {'Ave-Reward': array([-0.008797, -0.005   ]), 'NUM': [79, 81]}\n",
            "MoviePy - Building file /content/Magent2-RL-main/data/battle.gif with imageio.\n",
            "[*] Render saved to /content/Magent2-RL-main/data/battle.gif\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Total Episodes: 30\n",
            "Blue Wins: 0 (0.00%)\n",
            "Red Wins: 0 (0.00%)\n",
            "Draws: 30 (100.00%)\n",
            "\n",
            "Reward Statistics:\n",
            "Blue Average Reward: -0.0046 ± 0.0000\n",
            "Red Average Reward: -0.0106 ± 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOAD MODEL\n",
        "You don't need to concern about this part, I just download the model during training."
      ],
      "metadata": {
        "id": "oKMkHSU7luCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3ZIfRTJOmcxh",
        "outputId": "4a2ceb34-d812-4123-fd42-474691bae933"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4688f645-491b-462e-b8f0-5a38d6ba5209\", \"dqn_eval_460\", 2893958)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf0ca54e-bbc7-4fcd-a661-86b6fbeb5024\", \"dqn_target_460\", 2893994)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('data/models/iql-0/dqn_eval_1999')\n",
        "files.download('data/models/iql-0/dqn_target_1999')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}